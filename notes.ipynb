{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "KNN is an effective classification and regression algorithm that uses nearby points in order to generate a prediction.\n",
    "\n",
    "1. Choose a point \n",
    "2. Find the K-nearest points\n",
    "    1. K is a predefined user constant such as 1, 3, 5, or 11 \n",
    "3. Predict a label for the current point:\n",
    "    1. Classification - Take the most common class of the k neighbors\n",
    "    2. Regression - Take the average target metric of the k neighbors\n",
    "    3. Both classification or regression can also be modified to use weighted averages based on the distance of the neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of Distance Based Classifiers\n",
    "distance helps us quantify similarity\n",
    "\n",
    "### Manhattan distance\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/manhattan_fs.png' width=\"300\">\n",
    "\n",
    "$$ \\large d(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "manhattan_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the absolute difference and add it\n",
    "    manhattan_distance += abs(A[i] - B[i])\n",
    "\n",
    "manhattan_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/euclidean_fs.png' width = \"200\">\n",
    "\n",
    "$a^2 + b^2 = c^2$, or the **Pythagorean theorem**!\n",
    "\n",
    "$$ \\large d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.49324200890693"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "euclidean_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the difference, square, and add it\n",
    "    euclidean_distance += (A[i] - B[i]) ** 2\n",
    "\n",
    "# Square root of the final result\n",
    "euclidean_distance = sqrt(euclidean_distance)\n",
    "\n",
    "euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski distance\n",
    "\n",
    "A Normed Vector Space is just a fancy way of saying a collection of space where each point has been run through a function. It can be any function, as long it meets two criteria: \n",
    "1. the zero vector (just a vector filled with zeros) will output a length of 0, and \n",
    "2. every other vector must have a positive length \n",
    "\n",
    "Both the Manhattan and Euclidean distances are actually _special cases of Minkowski distance_. Take a look: \n",
    "\n",
    "$$\\large d(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Distance\n",
    "Hamming distance can even be used to compare strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How adjusting K works\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/fit_fs.png\" width = \"700\">\n",
    "\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/best_k_fs.png' width = \"550\">\n",
    "\n",
    "### Big O is Exponential for KNN\n",
    "Note that KNN isn't the best choice for extremely large datasets, and/or models with high dimensionality. This is because the time complexity (what computer scientists call \"Big O\", which you saw briefly earlier) of this algorithm is exponential.\n",
    "\n",
    "### Best value for K \n",
    "arrived at through testing on data set and trying diff values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecutre on KNN\n",
    "* Pick K for low bias low variance\n",
    "* Fitting doesn't train, it just stores the locations in the feature space.  What's the distance, get the closest distance.\n",
    "* Hyper tuning the number of neighbors we have\n",
    "* Low K = overfit, High K = underfit\n",
    "* Must scale the features!\n",
    "* Kfolds, GridSearchCV etc standardize after splitting\n",
    "* next(fold_index) will show the iteration of indexes in cross validation\n",
    "* cross validation finding the best score\n",
    "* lower k that predicts better is usually better\n",
    "* weighted averages: multiply support by\n",
    "* hidden dimensions latatent space\n",
    "* predicting about generalizing well\n",
    "* KNN is a lazy algorithm it works well with smaller data sets\n",
    "    * over 100K it starts to be too big\n",
    "    * columns matter too\n",
    "* Alternative to OHE? Encode one column with all the values\n",
    "* More features = more dimensions = more sparsity\n",
    "    * makes it harder to train or predict and can overfit\n",
    "    * volume scales exponentially\n",
    "    * affects all algorithms\n",
    "    * more columns can capture variance but you can over do it\n",
    "* Feature spaces\n",
    "    * cosine used for recommendations\n",
    "    * hamming mlp, distance between words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lloyd's vs Fair Lloyds\n",
    "\n",
    "K clustering fair lloyd's attempts to make cost between clusters fair by defining demographics groups where costs should be compared and altering clustering based on that, small increase in cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "Cross validation and hyper parameter tuning all in one\n",
    "It's exhaustive and how good it is depends on what params you feed it, it can waste a lot of time for no gain if not done thoughtfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle\n",
    "serialize state and read or write it to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines\n",
    "helps avoid data leakage and lets you make a workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                 ('tree', DecisionTreeClassifier(random_state=123))])\n",
    "\n",
    "# Create the grid parameter\n",
    "grid = [{'tree__max_depth': [None, 2, 6, 10], \n",
    "         'tree__min_samples_split': [5, 10]}]\n",
    "\n",
    "\n",
    "# Create the grid, with \"pipe\" as the estimator\n",
    "gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "\n",
    "# Fit using grid search\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the test score\n",
    "gridsearch.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used with other libraries\n",
    "Cross validate accepts a param for a pipeline and possibly others so it's well integrated with some libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture on Pipelines and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameters exist for both parametric and non parametric models\n",
    "* Pipeline solves\n",
    "    * K Fold cross validation takes loops and can get unwieldly\n",
    "    * crossval for each fold\n",
    "    * streamline this preprocessing\n",
    "    * do things in parallel \n",
    "* Pipeline takes\n",
    "    * constructor takes in a list of tuples as steps\n",
    "        * user label and transformer/estimator\n",
    "    * pipeline.fit\n",
    "    * pipeline.transform\n",
    "* GridSearchCV\n",
    "    * pipelinename__hyperparameter\n",
    "    * .best_estimator_\n",
    "    * refit on entire train after for better predictions\n",
    "    * ending in _ means it was filled after the fitting\n",
    "* (add the rest of the lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "Model that uses more than one model to make a prediction.  They often aggregate results.  Usually used in supervised learning.\n",
    "\n",
    "They are resilient to variance, think a group of specialists all weighing in on something to come up with wisdom of the crowd.\n",
    "\n",
    "Over and under estimates cancel out which is called smoothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregation\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png' alt=\"flowchart of input sample being split into several bootstrap samples, then building several decision trees, then aggregation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Bagging_**, which is short for **_Bootstrap Aggregation_** is two ideas bootstrap resampling and aggregation.\n",
    "\n",
    "**Bootstrap resampling** is a statistical method used to estimate the distribution of a statistic (e.g., mean, variance) by sampling with replacement from the original dataset.\n",
    "\n",
    "**Sampling with Replacement** Sampling with replacement means that when selecting elements from a dataset, each element is returned to the dataset after being selected. This allows the same element to be chosen multiple times in the sampling process.\n",
    "\n",
    "**Aggregation** is combining.  In this case it is combining the bootstrap samples.\n",
    "\n",
    "The process for training an ensemble through bootstrap aggregation is as follows:\n",
    "\n",
    "1. Grab a sizable sample from your dataset, with replacement \n",
    "2. Train a classifier on this sample  \n",
    "3. Repeat until all classifiers have been trained on their own sample from the dataset  \n",
    "4. When making a prediction, have each classifier in the ensemble make a prediction \n",
    "5. Aggregate all predictions from all classifiers into a single prediction, using the method of your choice  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are often used because they are sensitive to variance but they don't have to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Ensemble of decision trees, but decision trees use a greedy algorithm that maximizes information gain at each step.  We need each tree to be different.  **Bagging** and **subspace sampling** let the trees have more variance.\n",
    "\n",
    "\n",
    "For each tree in the dataset:\n",
    "\n",
    "1. Bag 2/3 of the overall data -- in our example, 2000 rows \n",
    "2. Randomly select a set number of features to use for training each node within this -- in this example, 6 features  \n",
    "3. Train the tree on the modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns  \n",
    "4. Drop the unused columns from step 3 from the out-of-bag rows that weren't bagged in step 1, and then use this as an internal testing set to calculate the out-of-bag error for this particular tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging for Random Forest\n",
    "1. obtain a portion of the data with replacement\n",
    "2. use this data to build a tree\n",
    "3. remaining data is **Out-of-Bag Data** or **OOB**.  \n",
    "4. OOB is used as test set to calculate the **Out-Of-Bag Error** to estimate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspace Sampling for Random Forest\n",
    "Further increases variability between trees by using a subset of features for each tree.\n",
    "\n",
    "## Random Foreset Visual of Algorithm\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_rf-diagram.png' width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient to overfitting\n",
    "due to the number of trees and their variance it is resilient to overfitting.  Finds signal in the noise.\n",
    "\n",
    "Each tree \"votes\" on the overall outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits\n",
    "**Strong Performance** - it is an ensemble method so and it tends to outperform many models.\n",
    "\n",
    "**Interpretability** - it is called a **glass box model** because it is transparent and easy to see how it arrived at a solution.  \n",
    "\n",
    "## Drawbacks\n",
    "\n",
    "**Computational Cost** - It can be slow to train on large data sets.\n",
    "\n",
    "**Memory Footprint** - It has to store all the data for each tree which can end up being hundreds of MBs.  Logistic regression only needs to store the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Paper and Website\n",
    "\n",
    "- [Random forests paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n",
    "\n",
    "- [Random forests website](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting and Weak Learners\n",
    "\n",
    "## Weak Learners\n",
    "A model that is only good at predicting slightly better than random chance\n",
    "\n",
    "1. Train a single weak learner  \n",
    "2. Figure out which examples the weak learner got wrong  \n",
    "3. Build another weak learner that focuses on the areas the first weak learner got wrong  \n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting vs Random Forest\n",
    "Very similar to random forests: ensembles of high variance models that aggregate to make a prediction.  Both often use tree models, boosting can use other models though.\n",
    "|Boosting|Random Forest|\n",
    "|--------|-------------|\n",
    "|Iterate|Parallel|\n",
    "|Corrects on Prior Trees|Trees don't know of each other|\n",
    "|Ensembel of Weak Learners|Ensemble of Strong Learners|\n",
    "|Very Resistant To Overfitting|Resistant to Overfitting|\n",
    "|Weighted Votes|Simple Votes|\n",
    "|Weight on Trees That Solve Harder Problems|All Even Weights|\n",
    "|Aggregate Solves Easy Problems|No Interaction Like this|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "* One of the first boosting algorithms\n",
    "* Uses weights on the sampling to increase weights on samples that the learner gets wrong, these weights increasing means the sample is more likely to end up in the bag\n",
    "* Ensemble can guess easy on easy problems so they are given less weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees\n",
    "* Makes use of Gradient Descent\n",
    "* Uses weak learners\n",
    "* This is where it diverges from AdaBoost: It calculated the residuals next to see how far it is off\n",
    "* Residuals are combined with a loss function\n",
    "* Loss function is differentiable\n",
    "* Loss function is inflated more where the model is more wrong, thus it will be pushed towards making a model focusing on these harder problems\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_gradient-boosting.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "$\\gamma$ -- this is the greek letter, **_gamma_** which is for learning rate\n",
    "\n",
    "Remember that too high of a learning rate is good to quickly train but won't find the best setting, and can lead to bouncing.\n",
    "\n",
    "A small learning rate will take longer to train and can get stuck in local minimums easier but will find a better value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Extreme Gradient Boosting\n",
    "* Handles missing values for you\n",
    "* Runs on multiple cpu cores in parallel\n",
    "* Distributes training across computer clusters\n",
    "* Go-to competition Algorithm\n",
    "* Always use multiple algorithms but it's a top dog right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems\n",
    "* Allows predicting the future preference list\n",
    "\n",
    "## Matrix Factorization\n",
    "* Singular Value Decomposition (SVD) and Alternating Least Squares (ALS)\n",
    "\n",
    "## Surprise Library\n",
    "* Used to create recommendation systems and runs really optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Expose People to What They Like\n",
    "* Predicts the future preference of a set of items or user\n",
    "* Taps into the \"long tail\", there's very common items everyone buys but the long tail specific items, like a certain genre of music or special toy are long tail\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/LongTailConcept.png\" alt=\"graph showing products on the x-axis and popularity on the y-axis. a few products are very popular, labeled Head. many other products are not very popular, labeled Long Tail\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal Definition\n",
    "***Recommendation Systems are software agents that elicit the interests and preferences of individual consumers […] and make recommendations accordingly. They have the potential to support and improve the quality of the\n",
    "decisions consumers make while searching for and selecting products online.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applications of Recommendation Systems\n",
    "* Suggest items to a customer\n",
    "* Estimate profit & loss of many competing items and make recommendations to the customer (e.g. buying and selling stocks)\n",
    "* Recommend a product or service based on experience of the custoemr\n",
    "* Show offers appealing to a customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Recommendation Systems\n",
    "* Unpersonalized and Personalized\n",
    "\n",
    "### Unpersonalized\n",
    "* EX: Youtube recommending the most viewed videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalized\n",
    "__Given__: \n",
    "The profile of the \"active\" user and possibly some situational context, i.e. user browsing a product or making a purchase etc. \n",
    "\n",
    "__Required__:\n",
    "Creating a set of items, and a score for each recommendable item in that set\n",
    "\n",
    "__Profile__:\n",
    "\n",
    "User profile may contain past purchases, ratings in either implicit or explicit form, demographics and interest scores for item features \n",
    "\n",
    "> There are two ways to gather such data. The first method is to ask for explicit ratings from a user, typically on a concrete rating scale (such as rating a movie from one to five stars). The second is to gather data implicitly as the user is in the domain of the system - that is, to log the actions of a user on the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these techniques make use of different similarity metrics to determine how \"similar\" items are to one another. \n",
    "* [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n",
    "* [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "* [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "* [Jaccard index (useful with binary data)](https://en.wikipedia.org/wiki/Jaccard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Recommenders \n",
    "\n",
    "> __Main Idea__: If you like an item, you will also like \"similar\" items.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/content_based.png\" alt=\"content based filtering. user watches movies, then similar movies are recommended to the user\" width=\"500\">\n",
    "\n",
    "* These systems are based on the characteristics of the items themselves. \"Try other items like this\"\n",
    "* Gives the user a bit more information on why they are seeing the recommendation\n",
    "* Require manual or semi-manual tagging of products\n",
    "* advanced systems can average all items a user liked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering Systems\n",
    "\n",
    "\n",
    "> __Main Idea__: If user A likes items 5, 6, 7, and 8 and user B likes items 5, 6, and 7, then it is highly likely that user B will also like item 8.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/collaborative_filtering.png\" alt=\"collaborative filtering: movies watched by both users indicate that the users are similar, then movies are recommended by one user to another user\" width=\"450\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
