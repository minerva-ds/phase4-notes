{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "KNN is an effective classification and regression algorithm that uses nearby points in order to generate a prediction.\n",
    "\n",
    "1. Choose a point \n",
    "2. Find the K-nearest points\n",
    "    1. K is a predefined user constant such as 1, 3, 5, or 11 \n",
    "3. Predict a label for the current point:\n",
    "    1. Classification - Take the most common class of the k neighbors\n",
    "    2. Regression - Take the average target metric of the k neighbors\n",
    "    3. Both classification or regression can also be modified to use weighted averages based on the distance of the neighbors \n",
    "4. Don't technically train or fit\n",
    "5. Efficient on small-mid size data not good for large data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of Distance Based Classifiers\n",
    "distance helps us quantify similarity\n",
    "\n",
    "### Manhattan distance\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/manhattan_fs.png' width=\"300\">\n",
    "\n",
    "$$ \\large d(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "manhattan_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the absolute difference and add it\n",
    "    manhattan_distance += abs(A[i] - B[i])\n",
    "\n",
    "manhattan_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/euclidean_fs.png' width = \"200\">\n",
    "\n",
    "$a^2 + b^2 = c^2$, or the **Pythagorean theorem**!\n",
    "\n",
    "$$ \\large d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.49324200890693"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "euclidean_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the difference, square, and add it\n",
    "    euclidean_distance += (A[i] - B[i]) ** 2\n",
    "\n",
    "# Square root of the final result\n",
    "euclidean_distance = sqrt(euclidean_distance)\n",
    "\n",
    "euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski distance\n",
    "\n",
    "A Normed Vector Space is just a fancy way of saying a collection of space where each point has been run through a function. It can be any function, as long it meets two criteria: \n",
    "1. the zero vector (just a vector filled with zeros) will output a length of 0, and \n",
    "2. every other vector must have a positive length \n",
    "\n",
    "Both the Manhattan and Euclidean distances are actually _special cases of Minkowski distance_. Take a look: \n",
    "\n",
    "$$\\large d(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Distance\n",
    "Hamming distance can even be used to compare strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How adjusting K works\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/fit_fs.png\" width = \"700\">\n",
    "\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/best_k_fs.png' width = \"550\">\n",
    "\n",
    "### Big O is Exponential for KNN\n",
    "Note that KNN isn't the best choice for extremely large datasets, and/or models with high dimensionality. This is because the time complexity (what computer scientists call \"Big O\", which you saw briefly earlier) of this algorithm is exponential.\n",
    "\n",
    "### Best value for K \n",
    "arrived at through testing on data set and trying diff values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecutre on KNN\n",
    "* Pick K for low bias low variance\n",
    "* Fitting doesn't train, it just stores the locations in the feature space.  What's the distance, get the closest distance.\n",
    "* Hyper tuning the number of neighbors we have\n",
    "* Low K = overfit, High K = underfit\n",
    "* Must scale the features!\n",
    "* Kfolds, GridSearchCV etc standardize after splitting\n",
    "* next(fold_index) will show the iteration of indexes in cross validation\n",
    "* cross validation finding the best score\n",
    "* lower k that predicts better is usually better\n",
    "* weighted averages: multiply support by\n",
    "* hidden dimensions latatent space\n",
    "* predicting about generalizing well\n",
    "* KNN is a lazy algorithm it works well with smaller data sets\n",
    "    * over 100K it starts to be too big\n",
    "    * columns matter too\n",
    "* Alternative to OHE? Encode one column with all the values\n",
    "* More features = more dimensions = more sparsity\n",
    "    * makes it harder to train or predict and can overfit\n",
    "    * volume scales exponentially\n",
    "    * affects all algorithms\n",
    "    * more columns can capture variance but you can over do it\n",
    "* Feature spaces\n",
    "    * cosine used for recommendations\n",
    "    * hamming mlp, distance between words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src = \"images/nonnormal.png\" /></center>\n",
    "<center>Unscaled</center>\n",
    "\n",
    "<center><img src = \"images/normalized.png\" /></center>\n",
    "<center>Scaled</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_transform = LabelEncoder()\n",
    "iris_df['Species'] = target_transform.fit_transform(iris_df['Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoder - takes categorical data like dog, cat, fish etc and turns them into numerical values like 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = iris_df[['SepalWidthCm', 'PetalWidthCm']]\n",
    "y = iris_df['Species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.15, random_state = 42)\n",
    "fold_index = KFold(n_splits = 5).split(X_train)\n",
    "\n",
    "next(fold_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Distance Metrics\n",
    "> The \"closeness\" of data points â†’ proxy for similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lloyd's vs Fair Lloyds\n",
    "\n",
    "K clustering fair lloyd's attempts to make cost between clusters fair by defining demographics groups where costs should be compared and altering clustering based on that, small increase in cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "Cross validation and hyper parameter tuning all in one\n",
    "It's exhaustive and how good it is depends on what params you feed it, it can waste a lot of time for no gain if not done thoughtfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle\n",
    "serialize state and read or write it to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines\n",
    "helps avoid data leakage and lets you make a workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                 ('tree', DecisionTreeClassifier(random_state=123))])\n",
    "\n",
    "# Create the grid parameter\n",
    "grid = [{'tree__max_depth': [None, 2, 6, 10], \n",
    "         'tree__min_samples_split': [5, 10]}]\n",
    "\n",
    "\n",
    "# Create the grid, with \"pipe\" as the estimator\n",
    "gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "\n",
    "# Fit using grid search\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the test score\n",
    "gridsearch.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used with other libraries\n",
    "Cross validate accepts a param for a pipeline and possibly others so it's well integrated with some libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture on Pipelines and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameters exist for both parametric and non parametric models\n",
    "* Pipeline solves\n",
    "    * K Fold cross validation takes loops and can get unwieldly\n",
    "    * crossval for each fold\n",
    "    * streamline this preprocessing\n",
    "    * do things in parallel \n",
    "* Pipeline takes\n",
    "    * constructor takes in a list of tuples as steps\n",
    "        * user label and transformer/estimator\n",
    "    * pipeline.fit\n",
    "    * pipeline.transform\n",
    "* GridSearchCV\n",
    "    * pipelinename__hyperparameter\n",
    "    * .best_estimator_\n",
    "    * refit on entire train after for better predictions\n",
    "    * ending in _ means it was filled after the fitting\n",
    "* (add the rest of the lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "Model that uses more than one model to make a prediction.  They often aggregate results.  Usually used in supervised learning.\n",
    "\n",
    "They are resilient to variance, think a group of specialists all weighing in on something to come up with wisdom of the crowd.\n",
    "\n",
    "Over and under estimates cancel out which is called smoothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregation\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png' alt=\"flowchart of input sample being split into several bootstrap samples, then building several decision trees, then aggregation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Bagging_**, which is short for **_Bootstrap Aggregation_** is two ideas bootstrap resampling and aggregation.\n",
    "\n",
    "**Bootstrap resampling** is a statistical method used to estimate the distribution of a statistic (e.g., mean, variance) by sampling with replacement from the original dataset.\n",
    "\n",
    "**Sampling with Replacement** Sampling with replacement means that when selecting elements from a dataset, each element is returned to the dataset after being selected. This allows the same element to be chosen multiple times in the sampling process.\n",
    "\n",
    "**Aggregation** is combining.  In this case it is combining the bootstrap samples.\n",
    "\n",
    "The process for training an ensemble through bootstrap aggregation is as follows:\n",
    "\n",
    "1. Grab a sizable sample from your dataset, with replacement \n",
    "2. Train a classifier on this sample  \n",
    "3. Repeat until all classifiers have been trained on their own sample from the dataset  \n",
    "4. When making a prediction, have each classifier in the ensemble make a prediction \n",
    "5. Aggregate all predictions from all classifiers into a single prediction, using the method of your choice  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are often used because they are sensitive to variance but they don't have to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Ensemble of decision trees, but decision trees use a greedy algorithm that maximizes information gain at each step.  We need each tree to be different.  **Bagging** and **subspace sampling** let the trees have more variance.\n",
    "\n",
    "\n",
    "For each tree in the dataset:\n",
    "\n",
    "1. Bag 2/3 of the overall data -- in our example, 2000 rows \n",
    "2. Randomly select a set number of features to use for training each node within this -- in this example, 6 features  \n",
    "3. Train the tree on the modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns  \n",
    "4. Drop the unused columns from step 3 from the out-of-bag rows that weren't bagged in step 1, and then use this as an internal testing set to calculate the out-of-bag error for this particular tree \n",
    "\n",
    "* Great for large complex datasets\n",
    "* Not prone to overfitting\n",
    "* Data doesn't need to be standardized \n",
    "* Uses bootstrap sampling to randomly select different samples\n",
    "* Uses random feature selections\n",
    "* Can fail to capture linear relationships\n",
    "* Smooths out classes so it's not as subject to influence by single points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging for Random Forest\n",
    "1. obtain a portion of the data with replacement\n",
    "2. use this data to build a tree\n",
    "3. remaining data is **Out-of-Bag Data** or **OOB**.  \n",
    "4. OOB is used as test set to calculate the **Out-Of-Bag Error** to estimate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspace Sampling for Random Forest\n",
    "Further increases variability between trees by using a subset of features for each tree.\n",
    "\n",
    "## Random Foreset Visual of Algorithm\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_rf-diagram.png' width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient to overfitting\n",
    "due to the number of trees and their variance it is resilient to overfitting.  Finds signal in the noise.\n",
    "\n",
    "Each tree \"votes\" on the overall outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits\n",
    "**Strong Performance** - it is an ensemble method so and it tends to outperform many models.\n",
    "\n",
    "**Interpretability** - it is called a **glass box model** because it is transparent and easy to see how it arrived at a solution.  \n",
    "\n",
    "## Drawbacks\n",
    "\n",
    "**Computational Cost** - It can be slow to train on large data sets.\n",
    "\n",
    "**Memory Footprint** - It has to store all the data for each tree which can end up being hundreds of MBs.  Logistic regression only needs to store the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Paper and Website\n",
    "\n",
    "- [Random forests paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n",
    "\n",
    "- [Random forests website](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting and Weak Learners\n",
    "\n",
    "## Weak Learners\n",
    "A model that is only good at predicting slightly better than random chance\n",
    "\n",
    "1. Train a single weak learner  \n",
    "2. Figure out which examples the weak learner got wrong  \n",
    "3. Build another weak learner that focuses on the areas the first weak learner got wrong  \n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting vs Random Forest\n",
    "Very similar to random forests: ensembles of high variance models that aggregate to make a prediction.  Both often use tree models, boosting can use other models though.\n",
    "|Boosting|Random Forest|\n",
    "|--------|-------------|\n",
    "|Iterate|Parallel|\n",
    "|Corrects on Prior Trees|Trees don't know of each other|\n",
    "|Ensemble of Weak Learners|Ensemble of Strong Learners|\n",
    "|Very Resistant To Overfitting|Resistant to Overfitting|\n",
    "|Weighted Votes|Simple Votes|\n",
    "|Weight on Trees That Solve Harder Problems|All Even Weights|\n",
    "|Aggregate Solves Easy Problems|No Interaction Like this|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "* One of the first boosting algorithms\n",
    "* Uses weights on the sampling to increase weights on samples that the learner gets wrong, these weights increasing means the sample is more likely to end up in the bag\n",
    "* Ensemble can guess easy on easy problems so they are given less weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees\n",
    "* Makes use of Gradient Descent\n",
    "* Uses weak learners\n",
    "* This is where it diverges from AdaBoost: It calculated the residuals next to see how far it is off\n",
    "* Residuals are combined with a loss function\n",
    "* Loss function is differentiable\n",
    "* Loss function is inflated more where the model is more wrong, thus it will be pushed towards making a model focusing on these harder problems\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_gradient-boosting.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ How does gradient boosting work for a classification problem? How do we even make sense of the notion of a gradient in that context? The short answer is that we appeal to the probabilities associated with the predictions for the various classes. See more on this topic [here](https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/). <br/> $\\rightarrow$ Why is this called \"_gradient_ boosting\"? Because using a model's residuals to build a new model is using information about the derivative of that model's loss function. See more on this topic [here](https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "$\\gamma$ -- this is the greek letter, **_gamma_** which is for learning rate\n",
    "\n",
    "Remember that too high of a learning rate is good to quickly train but won't find the best setting, and can lead to bouncing.\n",
    "\n",
    "A small learning rate will take longer to train and can get stuck in local minimums easier but will find a better value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mean squared error (MSE) and want to minimize that <-- done by gradient descent\n",
    "\n",
    "Use the residuals (pattern in the residuals) to create an even better model\n",
    "\n",
    "1. Fit a model to the data, $F_1(x) = y$\n",
    "2. Fit a model to the residuals, $h_1(x) = y - F_1(x)$\n",
    "3. Create a new model, $F_2(x) = F_1(x) + h_1(x)$\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Iterative Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Parts adapted from https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Extreme Gradient Boosting\n",
    "* Handles missing values for you\n",
    "* Runs on multiple cpu cores in parallel\n",
    "* Distributes training across computer clusters\n",
    "* Go-to competition Algorithm\n",
    "* Always use multiple algorithms but it's a top dog right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems\n",
    "* Allows predicting the future preference list\n",
    "\n",
    "## Matrix Factorization\n",
    "* Singular Value Decomposition (SVD) and Alternating Least Squares (ALS)\n",
    "\n",
    "## Surprise Library\n",
    "* Used to create recommendation systems and runs really optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Expose People to What They Like\n",
    "* Predicts the future preference of a set of items or user\n",
    "* Taps into the \"long tail\", there's very common items everyone buys but the long tail specific items, like a certain genre of music or special toy are long tail\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/LongTailConcept.png\" alt=\"graph showing products on the x-axis and popularity on the y-axis. a few products are very popular, labeled Head. many other products are not very popular, labeled Long Tail\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal Definition\n",
    "***Recommendation Systems are software agents that elicit the interests and preferences of individual consumers [â€¦] and make recommendations accordingly. They have the potential to support and improve the quality of the\n",
    "decisions consumers make while searching for and selecting products online.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applications of Recommendation Systems\n",
    "* Suggest items to a customer\n",
    "* Estimate profit & loss of many competing items and make recommendations to the customer (e.g. buying and selling stocks)\n",
    "* Recommend a product or service based on experience of the custoemr\n",
    "* Show offers appealing to a customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Recommendation Systems\n",
    "* Unpersonalized and Personalized\n",
    "\n",
    "### Unpersonalized\n",
    "* EX: Youtube recommending the most viewed videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalized\n",
    "__Given__: \n",
    "The profile of the \"active\" user and possibly some situational context, i.e. user browsing a product or making a purchase etc. \n",
    "\n",
    "__Required__:\n",
    "Creating a set of items, and a score for each recommendable item in that set\n",
    "\n",
    "__Profile__:\n",
    "\n",
    "User profile may contain past purchases, ratings in either implicit or explicit form, demographics and interest scores for item features \n",
    "\n",
    "> There are two ways to gather such data. The first method is to ask for explicit ratings from a user, typically on a concrete rating scale (such as rating a movie from one to five stars). The second is to gather data implicitly as the user is in the domain of the system - that is, to log the actions of a user on the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these techniques make use of different similarity metrics to determine how \"similar\" items are to one another. \n",
    "* [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n",
    "* [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "* [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "* [Jaccard index (useful with binary data)](https://en.wikipedia.org/wiki/Jaccard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Recommenders \n",
    "\n",
    "> __Main Idea__: If you like an item, you will also like \"similar\" items.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/content_based.png\" alt=\"content based filtering. user watches movies, then similar movies are recommended to the user\" width=\"500\">\n",
    "\n",
    "* These systems are based on the characteristics of the items themselves. \"Try other items like this\"\n",
    "* Gives the user a bit more information on why they are seeing the recommendation\n",
    "* Require manual or semi-manual tagging of products\n",
    "* advanced systems can average all items a user liked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering Systems\n",
    "\n",
    "\n",
    "> __Main Idea__: If user A likes items 5, 6, 7, and 8 and user B likes items 5, 6, and 7, then it is highly likely that user B will also like item 8.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/collaborative_filtering.png\" alt=\"collaborative filtering: movies watched by both users indicate that the users are similar, then movies are recommended by one user to another user\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The key idea behind collaborative filtering is that similar users share similar interests and that users tend to like items that are similar to one another.__\n",
    "\n",
    "* Often based off user reviews\n",
    "* Have a cold start problem on how to recommend things to new users that have no activity yet.\n",
    "\n",
    "## Utility Matrix reprsents the associated opinion that a user holds.\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           | 2          |                | 5         |\n",
    "| Lore   | 2         |            | 4              |           |\n",
    "| Mike   |           | 5          | 3              | 2         |\n",
    "| Forest | 5         |            | 1              |           |\n",
    "| Taylor | 1         | 5          |                | 2         |\n",
    "\n",
    "$r_{\\text{Mike},\\text{Little Mermaid}} = 3$.\n",
    "\n",
    "A recommendation system tries to fill in the blanks.  Most of the time these values are largely empty.\n",
    "The matrix above is what  is known as an explicit rating.  Each person has rated what they've seen.  However we can infer or use judgement to determine how to use data for a recommendation system.\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           |  1         |                | 1         |\n",
    "| Lore   | 1         |            | 1              |           |\n",
    "| Mike   |           | 1          | 1              | 1         |\n",
    "| Forest | 1         |            | 1              |           |\n",
    "| Taylor | 1         | 1          |                | 1         |\n",
    "\n",
    "These are __implicit__ ratings because we are assuming that because a person has bought something, they would like to buy other items like it. Of course, this is not necessarily true, but it's better than nothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Create  clusters that have high similarity between the data belonging to one cluster while aiming for minimal similarity between clusters\n",
    "\n",
    "## K-Means Clustering\n",
    "K determines the number of clusters and the algorithm optimizes around that\n",
    "\n",
    "## Hierarchial Agglomerative Clustering\n",
    "You start with $n$ clusters equal the number of data points and at each step you join two clusters.  You stop joining when a certain criterion is reached.\n",
    "\n",
    "## Semi-Supervised Learning\n",
    "Combine both concepts of supervised and unsupervised learning.  Increasingly popular.\n",
    "\n",
    "## Market Segmentation with Clustering\n",
    "Common and useful, we'll practice with a market segmentation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering\n",
    "The most popular and widely used clustering algorithm, and clustering are one of the most popular unsupervised machine learning algorithms. \n",
    "\n",
    "## Goal\n",
    "> **Intra**-class similarity is high\n",
    "\n",
    "> **Inter**-class similarity is low\n",
    "\n",
    "Similarity is determined by distance.  Closer is more similar.\n",
    "* **Agglomerative hierarchical** algorithm starts with n clusters\n",
    "* **Non-heirarchical** chooses k initial clusters\n",
    "\n",
    "Unsupervised and you do not know how many clusters you are looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Hierarchical Clustering with K-Means Clustering\n",
    "\n",
    "### Process\n",
    "1. Select $k$ initial seeds \n",
    "2. Assign each observation to the cluster to which it is \"closest\"\n",
    "3. Loop\n",
    "    - Cluster center is the mean of all points in the cluster, recalculated each iteration.\n",
    "    - Each iteration reassign points to be part of the closest cluster center.\n",
    "    - Stop if there is no reallocation \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/good-centroid-start.gif' alt=\"k-means clustering animation\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set number of clusters at initialization time\n",
    "k_means = KMeans(n_clusters=3) \n",
    "\n",
    "# Run the clustering algorithm\n",
    "k_means.fit(some_df) \n",
    "\n",
    "# Generate cluster index values for each row\n",
    "cluster_assignments = k_means.predict(some_df) \n",
    "\n",
    "# Cluster predictions for each point are also stored in k_means.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Variance Ratio\n",
    "* Accepted metric in wide use is **_Variance Ratio_** aka [**_Calinski Harabasz Score_**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)\n",
    "    * The variance of the points within a cluster to the variance of a point to points in other clusters.\n",
    "    * We want intra-cluster variance to be low suggesting the clusters are tightly knit.\n",
    "    * We want inter-cluster variance to be high suggesting that there is little to no ambiguity about which cluster a point belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Variance Ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code builds on the previous example\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Note that we could also pass in k_means.labels_ instead of cluster_assignments\n",
    "print(calinski_harabasz_score(some_df, cluster_assignments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics\n",
    "* [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
    "* No metric is best, each have diff strengths weaknesses based on given goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal K Value\n",
    "1. Fit different K-means clustering objects for every $k$ we want to try then compare the variance ratio scores of each.\n",
    "2. Visualize results with an **_Elbow Plot_** - plots that we can easily see wehre we hit a point of diminishign returns.  They are used with more than just variance ratios, one example is distortion another clustering metric.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_elbow-method.png' alt=\"Calinski Harabaz scores for different values of k\" width='500'>\n",
    "\n",
    "#### Understanding the Elbow\n",
    "\n",
    "A note on elbow plots: higher scores aren't always better. Higher values of $k$ mean introducing more overall complexity -- we will sometimes see elbow plots that look like this:\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_dim_returns.png' alt=\"plot with the number of clusters on the x-axis and the sum of squared distances to cluster center on the y-axis\" width=\"500\">\n",
    "\n",
    "$k$ = 20 is technically better as a score but $k$ = 4 is better because it balances model complexity with score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Agglomerative Clustering\n",
    "* K-means uses Expectation-Maximization after we tell it to give us $k$ clusters, however it can not have subgroups within subgroups\n",
    "* Agglomerative Clustering to the rescue!  It can have subgroups within subgroups\n",
    "* It starts with $n$ clusters with $n$ = the number of data points then merges until some stopping criterion\n",
    "\n",
    "## Linking Clusters Together\n",
    "\n",
    "* **ward** - merges two cluster on the least variance between them.  Leads to more equally sized clusters\n",
    "* **average** - merges the two clusters that have the smallest average distance between all points\n",
    "* **complete** - merges the two clusters that have the smallest maximum distance between their points\n",
    "\n",
    "Can affect the performance, which to use is based on the data and goals.\n",
    "\n",
    "The following diagram demonstrates the clusters created at each step for a dataset of 16 points. Take a look at the diagram and see if you can figure out what the algorithm is doing at each step as it merges clusters together:\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/hac_iterative.png' alt=\"initialization through step 14 of HAC algorithm\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it takes the closest clusters and merges them into a single cluster.  Below shows as the dots disappear the visualization is repalcing them with the newly calculated center.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/dendrogram_gif.gif' alt=\"animation of clusters shown in x-y space on the left and a dendrogram on the right, showing which clusters correspond to which parts of the dendrogram\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms and Clustergrams\n",
    "* Easily visualize the results at any given step \n",
    "* The image to the right above in the gif is a Dendrogram\n",
    "    * shows the hierarchical realtionship between the various clusters that are computed throughout each step.  \n",
    "* The image below is a Clustergram\n",
    "    * Visualize the same information by drawing lines representing each cluster at the each step\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/new_clustergram.png' alt=\"another view of clusters on the left and dendrogram on the right\" width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases\n",
    "* market segmentation\n",
    "    * things like market segmentation\n",
    "* gain a deeper understanding of a dataset through cluster analysis\n",
    "* photo sorting on smartphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Problems with Clustering Algorithms\n",
    "* No way of verifying the results are correct or not\n",
    "    * Never treat results of a cluster as ground-truth\n",
    "## Advantages and Disadvantages of K-Means Clustering\n",
    "### Advantages\n",
    "* Easy to implement\n",
    "* Usually faster than HAC with reasonably small $k$ and many features\n",
    "* Objects can shift clusters\n",
    "* Tighter clusters than HAC\n",
    "\n",
    "### Disadvantages\n",
    "* Need the right value for $k$\n",
    "* Scaling completely changes the results\n",
    "* Starting points have a strong impact on final results, as seen below.  Bad init is less likely than good init and you can run it multiple times.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/bad-centroid-start.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages of HAC\n",
    "\n",
    "### Advantages\n",
    "* ordered relationship between clusters, which can be useful when visualized\n",
    "* smaller clusters which allows more granular understanding\n",
    "\n",
    "### Disadvantages\n",
    "* Results depend on distance metric used\n",
    "* Objects can be grouped badly early on and no way to move them\n",
    "* We can't check visuals on more than 3 dimensions so it's hard to know when the algorithm was correct\n",
    "* Clustergram below\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_bad-hac.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning and Look-Alike Models\n",
    "* Combining both to solve real world problems\n",
    "\n",
    "## Case 1: Look-Alike Models\n",
    "* Find a similar audience\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_look-alike-model.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identify more customers/market segments that we can plausibly assume are equally valuable due to their similarity with valuable custmoers or market segement we already identified.\n",
    "* Divide into two groups: the ones we know are valuable and everyone else\n",
    "* Uses distance metric of choice to rate similarity of the unknown customers with the ones we have identified\n",
    "* Once we know they are somewhat similar to the valuable group we can spend resources to capture them\n",
    "* Likely see customers that are only somewhat similar to our valuable group\n",
    "* Customer sthat look nothing like our known valuable customer segment\n",
    "* It is a lot like clustering\n",
    "* referred to as prospecting.\n",
    "* choose resources to market to the customers that look like our valuabel customers to increase our top-of-funnel, meaning an increase to the number of potential customers that haven't shown interest in our product or cmpany yet but are likely to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Semi-Supervised Learning\n",
    "* Known as weakly supervised learning too\n",
    "* Generate Pseudo-labels that are possibly correct.\n",
    "    * doesn't use clustering, it uses supervised learning algorithms in an unsupervised way.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_semi-supervised.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. **Train your model on your labeled training data**\n",
    "2. **Use your trained model to generate pseudo-labels for unlabeled data**\n",
    "3. **Combine the pseudo-labels with your actual data**\n",
    "4. **Retrain your model on the new data set**\n",
    "#### Benefits\n",
    "* It is risky\n",
    "* When done well it can increase overall model performance by opening up access to much more data\n",
    "* Saves a ton of money on labeling costs!\n",
    "\n",
    "#### Downsides\n",
    "* When data is really noisy incorrect lables will skew the model\n",
    "* Feedback Loops\n",
    "* More complicated problems tend to work less\n",
    "\n",
    "#### Use a Holdout Set to Test\n",
    "* As usual but even more important in this case, make sure to use ground-truth or non pseudo-code labels to test with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Coefficient\n",
    "\n",
    "The Silhouette Coefficient is a measure used to evaluate the quality of clusters created by a clustering algorithm. It takes into account both the cohesion within clusters and the separation between clusters.\n",
    "\n",
    "## Definition\n",
    "\n",
    "For a given data point $i$, the Silhouette Coefficient $s(i)$ is defined as:\n",
    "\n",
    "$$ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
    "\n",
    "where:\n",
    "- $a(i)$ is the mean distance between $i$ and all other points in the same cluster.\n",
    "- $b(i)$ is the mean distance between $i$ and all points in the nearest cluster (the cluster with the smallest mean distance to $i$).\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "- $s(i)$ ranges from -1 to 1.\n",
    "  - $s(i) \\approx 1$: The data point is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "  - $s(i) \\approx 0$: The data point is on or very close to the decision boundary between two neighboring clusters.\n",
    "  - $s(i) \\approx -1$: The data point is poorly matched to its own cluster and well-matched to a neighboring cluster.\n",
    "\n",
    "## Overall Silhouette Score\n",
    "\n",
    "The overall Silhouette Score for a clustering is the mean Silhouette Coefficient of all data points:\n",
    "\n",
    "$$ S = \\frac{1}{N} \\sum_{i=1}^{N} s(i) $$\n",
    "\n",
    "where $N$ is the total number of data points.\n",
    "\n",
    "## Usage\n",
    "\n",
    "The Silhouette Coefficient can be used to:\n",
    "- Determine the optimal number of clusters by comparing the average silhouette scores for different numbers of clusters.\n",
    "- Evaluate the quality of clustering algorithms, with higher scores indicating better-defined clusters.\n",
    "\n",
    "## Example\n",
    "\n",
    "To compute the Silhouette Coefficient in Python, you can use the `silhouette_score` function from the `sklearn.metrics` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# X is your data and labels are the cluster labels\n",
    "score = silhouette_score(X, labels)\n",
    "print(f'Silhouette Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Principal Component Analysis in scikit-learn\n",
    "* Reduces dimensions while trying to capture as much info from the dataset as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms dataset along principal axes.  The first axes tries to capture the maximum variance within the data.  From here additional axes are constructed which are orthogonal to the previous axes and continue to account for as much of the remaining variance as possible.\n",
    "\n",
    "Transforms this:\n",
    "\n",
    "<img src=\"images/pca-data1.png\">\n",
    "\n",
    "Into this:\n",
    "\n",
    "<img src=\"images/pca-data2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/inhouse_pca.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99760273e-01, 2.39727247e-04])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99976027, 1.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize the first PCA component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pca-data3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Performing PCA\n",
    "\n",
    "The theory behind PCA rests upon many foundational concepts of linear algebra. After all, PCA is re-encoding a dataset into an alternative basis (the axes). Here are the exact steps:\n",
    "\n",
    "1. Recenter each feature of the dataset by subtracting that feature's mean from the feature vector\n",
    "2. Calculate the covariance matrix for your centered dataset\n",
    "3. Calculate the eigenvectors of the covariance matrix\n",
    "    1. You'll further investigate the concept of eigenvectors in the upcoming lesson\n",
    "4. Project the dataset into the new feature space: Multiply the eigenvectors by the mean-centered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Segmentation with Clustering\n",
    "* one of the most popular use cases for clustering\n",
    "\n",
    "# What is Market Segmentation?\n",
    "* **Cluster Analysis** to segment a customer base into different _market segments_ using the clustering techniques we've learned\n",
    "* Ex: decide marketing budget allocation in order to attract more customers\n",
    "    * Create personalized regression models for each group\n",
    "* Know who your customer is.  Identify sements in the customer data we can look for trends\n",
    "    * Ex decide the station to run commercials on\n",
    "* Find the segments with clustering\n",
    "    * find them based on behavior\n",
    "\n",
    "## Targeting\n",
    "Segmentation is just the first step\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_marketing-strategy.png' width='700'>\n",
    "\n",
    "* Build individualized strategies\n",
    "    * which market segment is most valuable to us? Use resarch and data analysis\n",
    "    * how do we allocate the advertising budget?  determine where to spend money best to reach the group\n",
    "* Figure out how to position our product to make it both desirable and stand out from competitors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Natural Language Tool Kit (NLTK)\n",
    "Popular NLP library in Python\n",
    "\n",
    "## Regular Expressions\n",
    "regex\n",
    "\n",
    "## Feature Engineering for Text Data\n",
    "Text data has a lot of ambiguity and feature engineering for NLP is specific.  \n",
    "* How to remvoe stop words\n",
    "* create frequency distributions\n",
    "* representing histograms\n",
    "* stemming\n",
    "* lemmatization\n",
    "* bigrams which shows how often two words occur together\n",
    "\n",
    "## Context-Free Grammars and Part-of-Speech (POS) Tagging\n",
    "* Context Free Grammar and Part of Speech tagging\n",
    "* POS tagging helps a computer understand how to interpret a sentence\n",
    "* Context free grammars (CFG) defines the rules of how sentences can exist.\n",
    "\n",
    "## Text Classification\n",
    "Will be gone over\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "The study of how computers can interact with humans through natural language\n",
    "\n",
    "* intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*\n",
    "* History\n",
    "    * Used to be rules based with rules borrowed from Linguistics\n",
    "    * Around the 1980s machine learning and AI started to show potential\n",
    "    * Now it is used around the globe every data by data scientists everywhere\n",
    "\n",
    "## NLP and Bayesian Statistics\n",
    "**_Naive Bayesian Classification_** is what keeps spam email at bay.\n",
    "\n",
    "## Working With Text Data\n",
    "Can require more cleaning and preprocessing than many other data types\n",
    "\n",
    "## Creating a Bag of Words\n",
    "* **Corpus**: a large structured text set used for NLP tasks\n",
    "* **Bag of Words**: vectorize data by capturing the unique words in a work, in any order.  A common way to do that is have usage counts of all the unique words.\n",
    "\n",
    "## Basic Cleaning and Tokenization\n",
    "* Often lowercase everything and remove punctuation\n",
    "* Decisions have to be made on how to tokenize and what variations of words to count as the same or different.  Ex. run and runs, Apple and Apple's\n",
    "\n",
    "### Stemming, Lemmatization, and Stop Words\n",
    "* **Stemming** reduces words to their root, in a crude way.  For example runs and running would become run but ponies would become poni.\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-nlp-and-word-vectorization/master/images/new_stemming.png' alt=\"stemming rules and examples\" width=\"400\">\n",
    "* **Lemmatization** uses **morphology** to reduce words to their basic forms called **lemma**\n",
    "|   Word   |  Stem | Lemma |\n",
    "|:--------:|:-----:|:-----:|\n",
    "|  Studies | Studi | Study |\n",
    "| Studying | Study | Study |\n",
    "* **Stop Words** have little to no information.  examples are \"of\" and \"the\".  Stop words are often removed from models to cut down on dimensionality.\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "### Count \n",
    "Count the number of times a word appears in a corpus.\n",
    "\n",
    "| Document | Aardvark | Apple | ... | Zebra |\n",
    "|:--------:|:--------:|:-----:|-----|-------|\n",
    "|     1    |     0    |   3   | ... | 1     |\n",
    "|     2    |     1    |   2   | ... | 0     |\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF) Vectorization\n",
    "Idea that rare words contain more information about a document than words used all the time in all documents.\n",
    "\n",
    "\n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large Term\\ Frequency(t) = \\frac{number\\ of\\ times\\ t\\ appears\\ in\\ a\\ document} {total\\ number\\ of\\ terms\\ in\\ the\\ document} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large IDF(t) = log_e(\\frac{Total\\ Number\\ of\\ Documents}{Number\\ of\\ Documents\\ with\\ t\\ in\\ it})$$\n",
    "\n",
    "The **_TF-IDF_** value is the product of both. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with NLTK\n",
    "**_Natural Language Tool Kit_**: A popular library for python for Natural Language Processing\n",
    "* Contains sample corpus like presidential speeches and project gutenberg transcripts.\n",
    "* Contains its own Bayesian classifiers for quick testing\n",
    "* Relies heavily on Linguistics but the tools are easy for a non linguist to use, ex making a parse tree\n",
    "<center> <img src='images/new_parse_tree.png'  width=\"750\"> </center>\n",
    "\n",
    "## Working with Text\n",
    "* **Stop Word Removal**\n",
    "* **Filtering and Cleaning**\n",
    "* **Feature Selection and Feature Engineering** - Libraries like Penn Tree Bank, part of speech tags, sentence polarity and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions (regex)\n",
    "* powerful for NLP\n",
    "* word_tokenize() splits a word that contains an apostrephe into 3 tokens.  they're becomes [\"they\",\"'\",\"re\"].  \n",
    "* we can use small regex patterns to capture they're as a word\n",
    "## Patterns\n",
    "* Regex is as good as the patterns we create\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = 'he said that she said \"hello\".'\n",
    "pattern = 'he'\n",
    "p = re.compile(pattern)\n",
    "p.findall(sentence) # Output will be ['he', 'he, 'he']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**anchors** can be used to define word boundaries\n",
    "\n",
    "## Ranges, Groups, Quantifiers\n",
    "* Range: `[A-Za-z0-9]` would match most alphanumeric chars in the English alphabet\n",
    "* Character Class Examples\n",
    "    * `\\d` is `[0-9]`\n",
    "    * `\\w` is any word\n",
    "    * `\\D` anything that isn't a digit\n",
    "    * `\\W` anything that isn't a word\n",
    "* Quantifiers - matches preceding ex. a* or (cat)* \n",
    "    * `*` 0 or more times\n",
    "    * `+` 1 or more times\n",
    "    * `?` 0 or 1 times\n",
    "    * `{n}` match exactly n times, ex `{3}` matches 3 times\n",
    "    * `{n,k}` match between n and k times, ex `{3,5}` matches 3-5 times\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/master/images/regex_cheat_sheet.png' alt=\"regex cheat sheet\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Text Data\n",
    "* Text data has a lot of ambiguity\n",
    "\n",
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(some_text_data)\n",
    "\n",
    "# It is usually a good idea to lowercase all tokens during this step, as well\n",
    "stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import FreqDist\n",
    "freqdist = FreqDist(tokens)\n",
    "\n",
    "# get the 200 most common words \n",
    "most_common = freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Often utilizes wordnet lexical database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('feet') # foot\n",
    "lemmatizer.lemmatize('running') # run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams and Mutual Information Score\n",
    "* Pair adjacent words and treat them as one token, special case of n-grams where n is 2\n",
    "    * n-grams can be created at the character level, and commonly done so\n",
    "    * useful because you can apply a freuency filter.  Common values are at least 5.\n",
    "    * **Pointwise Mutual Information Score** - from information theory a measure of mutual dependence between two words. ex San Franscisco would appear together a lot meaning it has a higher information score.\n",
    "    * NLTK can compute this too\n",
    "* \"the dog played outside\" becomes `('the', 'dog'), ('dog', 'played'), ('played', 'outside')`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
