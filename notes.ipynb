{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "KNN is an effective classification and regression algorithm that uses nearby points in order to generate a prediction.\n",
    "\n",
    "1. Choose a point \n",
    "2. Find the K-nearest points\n",
    "    1. K is a predefined user constant such as 1, 3, 5, or 11 \n",
    "3. Predict a label for the current point:\n",
    "    1. Classification - Take the most common class of the k neighbors\n",
    "    2. Regression - Take the average target metric of the k neighbors\n",
    "    3. Both classification or regression can also be modified to use weighted averages based on the distance of the neighbors \n",
    "4. Don't technically train or fit\n",
    "5. Efficient on small-mid size data not good for large data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of Distance Based Classifiers\n",
    "distance helps us quantify similarity\n",
    "\n",
    "### Manhattan distance\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/manhattan_fs.png' width=\"300\">\n",
    "\n",
    "$$ \\large d(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "manhattan_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the absolute difference and add it\n",
    "    manhattan_distance += abs(A[i] - B[i])\n",
    "\n",
    "manhattan_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/euclidean_fs.png' width = \"200\">\n",
    "\n",
    "$a^2 + b^2 = c^2$, or the **Pythagorean theorem**!\n",
    "\n",
    "$$ \\large d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.49324200890693"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "euclidean_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the difference, square, and add it\n",
    "    euclidean_distance += (A[i] - B[i]) ** 2\n",
    "\n",
    "# Square root of the final result\n",
    "euclidean_distance = sqrt(euclidean_distance)\n",
    "\n",
    "euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski distance\n",
    "\n",
    "A Normed Vector Space is just a fancy way of saying a collection of space where each point has been run through a function. It can be any function, as long it meets two criteria: \n",
    "1. the zero vector (just a vector filled with zeros) will output a length of 0, and \n",
    "2. every other vector must have a positive length \n",
    "\n",
    "Both the Manhattan and Euclidean distances are actually _special cases of Minkowski distance_. Take a look: \n",
    "\n",
    "$$\\large d(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Distance\n",
    "Hamming distance can even be used to compare strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How adjusting K works\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/fit_fs.png\" width = \"700\">\n",
    "\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/best_k_fs.png' width = \"550\">\n",
    "\n",
    "### Big O is Exponential for KNN\n",
    "Note that KNN isn't the best choice for extremely large datasets, and/or models with high dimensionality. This is because the time complexity (what computer scientists call \"Big O\", which you saw briefly earlier) of this algorithm is exponential.\n",
    "\n",
    "### Best value for K \n",
    "arrived at through testing on data set and trying diff values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecutre on KNN\n",
    "* Pick K for low bias low variance\n",
    "* Fitting doesn't train, it just stores the locations in the feature space.  What's the distance, get the closest distance.\n",
    "* Hyper tuning the number of neighbors we have\n",
    "* Low K = overfit, High K = underfit\n",
    "* Must scale the features!\n",
    "* Kfolds, GridSearchCV etc standardize after splitting\n",
    "* next(fold_index) will show the iteration of indexes in cross validation\n",
    "* cross validation finding the best score\n",
    "* lower k that predicts better is usually better\n",
    "* weighted averages: multiply support by\n",
    "* hidden dimensions latatent space\n",
    "* predicting about generalizing well\n",
    "* KNN is a lazy algorithm it works well with smaller data sets\n",
    "    * over 100K it starts to be too big\n",
    "    * columns matter too\n",
    "* Alternative to OHE? Encode one column with all the values\n",
    "* More features = more dimensions = more sparsity\n",
    "    * makes it harder to train or predict and can overfit\n",
    "    * volume scales exponentially\n",
    "    * affects all algorithms\n",
    "    * more columns can capture variance but you can over do it\n",
    "* Feature spaces\n",
    "    * cosine used for recommendations\n",
    "    * hamming mlp, distance between words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src = \"images/nonnormal.png\" /></center>\n",
    "<center>Unscaled</center>\n",
    "\n",
    "<center><img src = \"images/normalized.png\" /></center>\n",
    "<center>Scaled</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_transform = LabelEncoder()\n",
    "iris_df['Species'] = target_transform.fit_transform(iris_df['Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoder - takes categorical data like dog, cat, fish etc and turns them into numerical values like 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = iris_df[['SepalWidthCm', 'PetalWidthCm']]\n",
    "y = iris_df['Species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.15, random_state = 42)\n",
    "fold_index = KFold(n_splits = 5).split(X_train)\n",
    "\n",
    "next(fold_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1: K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Introduction to Distance-Based Algorithms\n",
    "\n",
    "#### K-Nearest Neighbors (KNN)\n",
    "- KNN is a distance-based supervised learning algorithm.\n",
    "- It should not be confused with K-means clustering, which is an unsupervised technique.\n",
    "\n",
    "#### Key Concepts\n",
    "- **Distance Metrics:** Different types of distance metrics are used in KNN, similar to those in L1 and L2 regularizations.\n",
    "  - Euclidean Distance (L2)\n",
    "  - Manhattan Distance (L1)\n",
    "  - Minkowski Distance (a combination of L1 and L2)\n",
    "- **Hyperparameter Tuning:** Tuning the value of K is essential for achieving low bias and low variance.\n",
    "- **Labels:** KNN uses labeled data for its predictions.\n",
    "\n",
    "#### KNN in Practice\n",
    "1. **Choosing the Value of K:**\n",
    "   - K represents the number of nearest neighbors.\n",
    "   - It is a hyperparameter that needs to be tuned for optimal performance.\n",
    "   - Common values are 1, 3, 5, 7, etc., typically chosen to avoid ties.\n",
    "\n",
    "2. **Distance Metrics:**\n",
    "   - Euclidean Distance: Suitable for continuous features.\n",
    "   - Manhattan Distance: Suitable for categorical features.\n",
    "   - Minkowski Distance: Generalization that includes both Euclidean and Manhattan distances.\n",
    "\n",
    "3. **Algorithm Steps:**\n",
    "   - Take a test data point.\n",
    "   - Calculate the distance to all training data points.\n",
    "   - Select the K-nearest neighbors based on the chosen distance metric.\n",
    "   - For classification, use majority voting among the neighbors.\n",
    "   - For regression, take the average of the neighbors' target values.\n",
    "\n",
    "4. **No Training Phase:**\n",
    "   - KNN does not have a traditional training phase involving model fitting.\n",
    "   - Instead, it stores the training data and calculates distances during prediction.\n",
    "\n",
    "#### Tuning K and Avoiding Overfitting/Underfitting\n",
    "- **Low K:** Leads to high variance and overfitting.\n",
    "- **High K:** Leads to high bias and underfitting.\n",
    "- **Optimal K:** Achieved through cross-validation and testing on the validation set.\n",
    "\n",
    "#### Example Scenario\n",
    "- Predict the class of a new data point based on its K-nearest neighbors.\n",
    "- Different values of K can lead to different classifications.\n",
    "- Evaluate the model using accuracy or other appropriate metrics.\n",
    "\n",
    "#### Important Considerations\n",
    "- **Scaling Features:** Features must be standardized to ensure fair distance calculations.\n",
    "- **Odd vs. Even K:** Odd values are preferred to avoid ties in majority voting.\n",
    "\n",
    "#### Computational Complexity\n",
    "- KNN has exponential time complexity, making it less suitable for very large datasets or high-dimensional data.\n",
    "- Dimensionality reduction techniques like PCA can help mitigate these issues.\n",
    "\n",
    "#### Practical Applications\n",
    "- KNN is straightforward and interpretable, making it useful for small to mid-sized datasets.\n",
    "- It is not ideal for datasets with over 100,000 samples or very high-dimensional data.\n",
    "\n",
    "### Summary\n",
    "- KNN is a versatile and easy-to-understand algorithm.\n",
    "- Proper selection of K and distance metrics is crucial.\n",
    "- Scaling features and dimensionality reduction can enhance performance.\n",
    "- KNN works well for both classification and regression tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### Additional Concepts from the Lecture\n",
    "- **Lazy Algorithm:** KNN is considered a lazy algorithm because it does not involve explicit training.\n",
    "- **Public Opinion Analogy:** The concept of KNN can be compared to public opinion, where the majority vote influences the outcome.\n",
    "- **Cosine Similarity:** Often used in recommendation systems to find the similarity between vectors.\n",
    "- **Hamming Distance:** Used in natural language processing to measure the distance between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Distance Metrics\n",
    "> The \"closeness\" of data points → proxy for similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lloyd's vs Fair Lloyds\n",
    "\n",
    "K clustering fair lloyd's attempts to make cost between clusters fair by defining demographics groups where costs should be compared and altering clustering based on that, small increase in cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "Cross validation and hyper parameter tuning all in one\n",
    "It's exhaustive and how good it is depends on what params you feed it, it can waste a lot of time for no gain if not done thoughtfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle\n",
    "serialize state and read or write it to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines\n",
    "helps avoid data leakage and lets you make a workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                 ('tree', DecisionTreeClassifier(random_state=123))])\n",
    "\n",
    "# Create the grid parameter\n",
    "grid = [{'tree__max_depth': [None, 2, 6, 10], \n",
    "         'tree__min_samples_split': [5, 10]}]\n",
    "\n",
    "\n",
    "# Create the grid, with \"pipe\" as the estimator\n",
    "gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "\n",
    "# Fit using grid search\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the test score\n",
    "gridsearch.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used with other libraries\n",
    "Cross validate accepts a param for a pipeline and possibly others so it's well integrated with some libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture on Pipelines and GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameters exist for both parametric and non parametric models\n",
    "* Pipeline solves\n",
    "    * K Fold cross validation takes loops and can get unwieldly\n",
    "    * crossval for each fold\n",
    "    * streamline this preprocessing\n",
    "    * do things in parallel \n",
    "* Pipeline takes\n",
    "    * constructor takes in a list of tuples as steps\n",
    "        * user label and transformer/estimator\n",
    "    * pipeline.fit\n",
    "    * pipeline.transform\n",
    "* GridSearchCV\n",
    "    * pipelinename__hyperparameter\n",
    "    * .best_estimator_\n",
    "    * refit on entire train after for better predictions\n",
    "    * ending in _ means it was filled after the fitting\n",
    "* (add the rest of the lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Pipelines\n",
    "\n",
    "## Questions to Consider\n",
    "1. **Describe KNN. How does it make predictions for regression or classification tasks?**\n",
    "   - KNN makes predictions by storing the locations of data points. For classification, it assigns the majority class of the K nearest neighbors to the new data point. For regression, it predicts the average of the target values of the K nearest neighbors.\n",
    "\n",
    "2. **How does KNN make predictions?**\n",
    "   - KNN predicts based on the K nearest neighbors using a distance metric (e.g., Euclidean, Manhattan). It doesn't involve training in the traditional sense but stores the data points for distance computation during prediction.\n",
    "\n",
    "3. **Impact of scaling on KNN:**\n",
    "   - Scaling normalizes the distances between data points, ensuring fair comparisons. It standardizes the feature space so that no single feature disproportionately influences the distance calculation.\n",
    "\n",
    "## Important Concepts\n",
    "- **Nonparametric and Lazy Model:** KNN doesn't derive coefficients or optimization functions. It simply stores data points and computes distances during prediction.\n",
    "- **Choosing K:** Small K values can overfit (high variance), while large K values can underfit (high bias). Optimal K balances this trade-off.\n",
    "- **Distance Metrics:** Euclidean for continuous features, Manhattan for categorical features, and Minkowski as a generalization.\n",
    "\n",
    "## Scaling Techniques\n",
    "- **Standard Scaler:** Centers the data around the mean with unit variance.\n",
    "- **Normalizer:** Scales individual samples to unit norm. More useful for clustering and classification when the direction is more important than the magnitude.\n",
    "\n",
    "## Pipelines\n",
    "- **Purpose:** Streamlines preprocessing and model fitting into a single workflow.\n",
    "- **Steps:** \n",
    "  - Define preprocessing steps (e.g., imputation, scaling).\n",
    "  - Define the model.\n",
    "  - Fit and transform data within the pipeline.\n",
    "  - Predict using the same pipeline to ensure consistent preprocessing.\n",
    "\n",
    "### Pipeline Components\n",
    "1. **Imputation:** Filling missing values.\n",
    "2. **Scaling:** Standardizing or normalizing features.\n",
    "3. **Model Fitting:** Training the model with the preprocessed data.\n",
    "\n",
    "### Example: Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformer\n",
    "**Purpose**: Apply different preprocessing steps to different subsets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_features = ['age', 'hours_per_week']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['occupation', 'sex']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "**GridSearchCV**: Exhaustively searches through a specified parameter grid to find the best combination.\n",
    "\n",
    "**RandomizedSearchCV**: Randomly samples a specified number of parameter settings from the grid, useful for large datasets or high-dimensional parameter spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE and Pipeline\n",
    "**Purpose**: Handle class imbalance by oversampling the minority class using synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', smote),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union\n",
    "**Purpose**: Apply multiple transformations to the same features and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "\n",
    "def sin_transform(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def cos_transform(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "feature_union = FeatureUnion([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('sin', FunctionTransformer(sin_transform)),\n",
    "    ('cos', FunctionTransformer(cos_transform))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', feature_union),\n",
    "    ('classifier', Ridge())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- **Pipelines** simplify the process of applying multiple transformations and fitting models, ensuring consistency and reducing code duplication.\n",
    "- **ColumnTransformer** and **FeatureUnion** offer flexibility in preprocessing different types of features.\n",
    "- **GridSearchCV** and **RandomizedSearchCV** help in tuning hyperparameters efficiently.\n",
    "- **SMOTE** and **ImbPipeline** address class imbalance issues effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "Model that uses more than one model to make a prediction.  They often aggregate results.  Usually used in supervised learning.\n",
    "\n",
    "They are resilient to variance, think a group of specialists all weighing in on something to come up with wisdom of the crowd.\n",
    "\n",
    "Over and under estimates cancel out which is called smoothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregation\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png' alt=\"flowchart of input sample being split into several bootstrap samples, then building several decision trees, then aggregation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Bagging_**, which is short for **_Bootstrap Aggregation_** is two ideas bootstrap resampling and aggregation.\n",
    "\n",
    "**Bootstrap resampling** is a statistical method used to estimate the distribution of a statistic (e.g., mean, variance) by sampling with replacement from the original dataset.\n",
    "\n",
    "**Sampling with Replacement** Sampling with replacement means that when selecting elements from a dataset, each element is returned to the dataset after being selected. This allows the same element to be chosen multiple times in the sampling process.\n",
    "\n",
    "**Aggregation** is combining.  In this case it is combining the bootstrap samples.\n",
    "\n",
    "The process for training an ensemble through bootstrap aggregation is as follows:\n",
    "\n",
    "1. Grab a sizable sample from your dataset, with replacement \n",
    "2. Train a classifier on this sample  \n",
    "3. Repeat until all classifiers have been trained on their own sample from the dataset  \n",
    "4. When making a prediction, have each classifier in the ensemble make a prediction \n",
    "5. Aggregate all predictions from all classifiers into a single prediction, using the method of your choice  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are often used because they are sensitive to variance but they don't have to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Randome Forests and Bagging and Boosting Techniques\n",
    "\n",
    "## Topics Covered\n",
    "- Bagging and boosting techniques\n",
    "- Random forest\n",
    "- Averaging models\n",
    "- Weighted averages of models\n",
    "- Boosting algorithms such as gradient boosting and AdaBoost\n",
    "\n",
    "## Questions\n",
    "\n",
    "### 1. Explain the difference between Feature Union and Column Transformer. When would you use Feature Union and when would you use Column Transformer?\n",
    "\n",
    "**Answers:**\n",
    "- Column Transformer allows you to apply different transformations to different columns in parallel.\n",
    "- Feature Union applies transformations to all columns and concatenates the results.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "- Bagging is short for bootstrap aggregation.\n",
    "- Bagging classifier/regressor can be used with different models.\n",
    "- Random Forest involves two levels of randomness: bootstrap sampling and random feature selection.\n",
    "- Extra Trees involves three levels of randomness.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "- Handles big data efficiently.\n",
    "- Adds randomization by sampling with replacement and selecting random subsets of features at each node.\n",
    "- Helps in reducing overfitting and increasing model robustness.\n",
    "\n",
    "## Extra Trees\n",
    "\n",
    "- Adds an extra layer of randomization by randomly selecting the feature space inside each node.\n",
    "- Useful when Random Forest still shows overfitting.\n",
    "\n",
    "## Combining Models\n",
    "\n",
    "### 1. Averaging and Weighted Averaging\n",
    "\n",
    "- Combine multiple models by averaging their predictions.\n",
    "- Weighted averaging assigns different weights to each model's predictions based on their performance.\n",
    "\n",
    "### 2. Stacking\n",
    "\n",
    "- Uses the output of individual estimators as input for a final estimator.\n",
    "- Can combine different types of models (e.g., logistic regression, KNN, decision tree) to improve overall performance.\n",
    "\n",
    "## Bagging Classifier Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the base model\n",
    "base_estimator = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Define the bagging classifier\n",
    "bagging_clf = BaggingClassifier(base_estimator=base_estimator, n_estimators=150, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bagging_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Define the extra trees classifier\n",
    "et_clf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt', bootstrap=True, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "et_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = et_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define base models\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# Define the stacking classifier\n",
    "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "# Fit the model\n",
    "stack_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = stack_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Pipelines simplify the process of applying multiple transformations and fitting models.\n",
    "- ColumnTransformer and FeatureUnion offer flexibility in preprocessing different types of features.\n",
    "- GridSearchCV and RandomizedSearchCV help in tuning hyperparameters efficiently.\n",
    "- SMOTE and ImbPipeline address class imbalance issues effectively.\n",
    "- Bagging, random forests, extra trees, averaging, and stacking are powerful techniques for building robust models.\n",
    "- Hyperparameter tuning and model stacking can significantly improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Ensemble of decision trees, but decision trees use a greedy algorithm that maximizes information gain at each step.  We need each tree to be different.  **Bagging** and **subspace sampling** let the trees have more variance.\n",
    "\n",
    "\n",
    "For each tree in the dataset:\n",
    "\n",
    "1. Bag 2/3 of the overall data -- in our example, 2000 rows \n",
    "2. Randomly select a set number of features to use for training each node within this -- in this example, 6 features  \n",
    "3. Train the tree on the modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns  \n",
    "4. Drop the unused columns from step 3 from the out-of-bag rows that weren't bagged in step 1, and then use this as an internal testing set to calculate the out-of-bag error for this particular tree \n",
    "\n",
    "* Great for large complex datasets\n",
    "* Not prone to overfitting\n",
    "* Data doesn't need to be standardized \n",
    "* Uses bootstrap sampling to randomly select different samples\n",
    "* Uses random feature selections\n",
    "* Can fail to capture linear relationships\n",
    "* Smooths out classes so it's not as subject to influence by single points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging for Random Forest\n",
    "1. obtain a portion of the data with replacement\n",
    "2. use this data to build a tree\n",
    "3. remaining data is **Out-of-Bag Data** or **OOB**.  \n",
    "4. OOB is used as test set to calculate the **Out-Of-Bag Error** to estimate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspace Sampling for Random Forest\n",
    "Further increases variability between trees by using a subset of features for each tree.\n",
    "\n",
    "## Random Foreset Visual of Algorithm\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_rf-diagram.png' width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient to overfitting\n",
    "due to the number of trees and their variance it is resilient to overfitting.  Finds signal in the noise.\n",
    "\n",
    "Each tree \"votes\" on the overall outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits\n",
    "**Strong Performance** - it is an ensemble method so and it tends to outperform many models.\n",
    "\n",
    "**Interpretability** - it is called a **glass box model** because it is transparent and easy to see how it arrived at a solution.  \n",
    "\n",
    "## Drawbacks\n",
    "\n",
    "**Computational Cost** - It can be slow to train on large data sets.\n",
    "\n",
    "**Memory Footprint** - It has to store all the data for each tree which can end up being hundreds of MBs.  Logistic regression only needs to store the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Paper and Website\n",
    "\n",
    "- [Random forests paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n",
    "\n",
    "- [Random forests website](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.2: Bagging and Boosting Techniques\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Pipelines simplify the process of applying multiple transformations and fitting models.\n",
    "- ColumnTransformer and FeatureUnion offer flexibility in preprocessing different types of features.\n",
    "- GridSearchCV and RandomizedSearchCV help in tuning hyperparameters efficiently.\n",
    "- SMOTE and ImbPipeline address class imbalance issues effectively.\n",
    "- Bagging, random forests, extra trees, averaging, and stacking are powerful techniques for building robust models.\n",
    "- Hyperparameter tuning and model stacking can significantly improve model performance.\n",
    "### Introduction\n",
    "- **Bagging and Boosting Techniques**\n",
    "  - Random Forest\n",
    "  - Averaging Models\n",
    "  - Weighted Averages of Models\n",
    "  - Boosting Algorithms (Gradient Boost, AdaBoost)\n",
    "\n",
    "### Random Forests\n",
    "- **Advantages**\n",
    "  - Great for complex data sets.\n",
    "  - Not prone to overfitting compared to decision trees.\n",
    "  - Can handle large data sets.\n",
    "  - Capable of modeling complex relationships.\n",
    "  - Does not require data standardization.\n",
    "- **Disadvantages**\n",
    "  - Less interpretable as complexity increases.\n",
    "  - Requires careful tuning of hyperparameters.\n",
    "\n",
    "### K Nearest Neighbors (KNN)\n",
    "- **Advantages**\n",
    "  - Simple to understand and implement.\n",
    "  - No training phase; just stores the training data.\n",
    "  - Handles multi-class classification.\n",
    "- **Disadvantages**\n",
    "  - Computationally expensive during prediction.\n",
    "  - Requires large memory.\n",
    "  - Sensitive to redundant features and the curse of dimensionality.\n",
    "\n",
    "### Adaptive Boosting (AdaBoost)\n",
    "- Uses weak learners (e.g., decision tree stumps).\n",
    "- Attaches weights to data points, increasing weights for misclassified points.\n",
    "- Sequentially trains models, focusing more on difficult-to-classify data points.\n",
    "- **Implementation Example**\n",
    "  - Instantiate with `AdaBoostClassifier` or `AdaBoostRegressor`.\n",
    "  - Fit the model and predict.\n",
    "\n",
    "### Gradient Boosting\n",
    "- Similar to AdaBoost but uses residuals instead of weights.\n",
    "- Sequentially fits models to correct the residuals of previous models.\n",
    "- **Implementation Example**\n",
    "  - Instantiate with `GradientBoostingClassifier` or `GradientBoostingRegressor`.\n",
    "  - Fit the model and predict.\n",
    "\n",
    "### XGBoost\n",
    "- An advanced implementation of gradient boosting.\n",
    "- Handles missing values.\n",
    "- Allows custom loss functions.\n",
    "- **Implementation Example**\n",
    "  - Instantiate with `XGBClassifier` or `XGBRegressor`.\n",
    "  - Fit the model and predict.\n",
    "\n",
    "### Model Stacking\n",
    "- Combines different models' predictions by feeding them into a meta-model.\n",
    "- Useful for leveraging the strengths of various models.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- Essential for improving model performance.\n",
    "- Common tools: `GridSearchCV`, `RandomizedSearchCV`.\n",
    "- **Example Parameters for Tuning**\n",
    "  - Number of estimators.\n",
    "  - Learning rate.\n",
    "  - Max depth.\n",
    "\n",
    "### Conclusion\n",
    "- Understanding and applying ensemble techniques like bagging, boosting, and stacking can significantly enhance model performance.\n",
    "- Proper hyperparameter tuning is crucial for maximizing the potential of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting and Weak Learners\n",
    "\n",
    "## Weak Learners\n",
    "A model that is only good at predicting slightly better than random chance\n",
    "\n",
    "1. Train a single weak learner  \n",
    "2. Figure out which examples the weak learner got wrong  \n",
    "3. Build another weak learner that focuses on the areas the first weak learner got wrong  \n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting vs Random Forest\n",
    "Very similar to random forests: ensembles of high variance models that aggregate to make a prediction.  Both often use tree models, boosting can use other models though.\n",
    "|Boosting|Random Forest|\n",
    "|--------|-------------|\n",
    "|Iterate|Parallel|\n",
    "|Corrects on Prior Trees|Trees don't know of each other|\n",
    "|Ensemble of Weak Learners|Ensemble of Strong Learners|\n",
    "|Very Resistant To Overfitting|Resistant to Overfitting|\n",
    "|Weighted Votes|Simple Votes|\n",
    "|Weight on Trees That Solve Harder Problems|All Even Weights|\n",
    "|Aggregate Solves Easy Problems|No Interaction Like this|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "* One of the first boosting algorithms\n",
    "* Uses weights on the sampling to increase weights on samples that the learner gets wrong, these weights increasing means the sample is more likely to end up in the bag\n",
    "* Ensemble can guess easy on easy problems so they are given less weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees\n",
    "* Makes use of Gradient Descent\n",
    "* Uses weak learners\n",
    "* This is where it diverges from AdaBoost: It calculated the residuals next to see how far it is off\n",
    "* Residuals are combined with a loss function\n",
    "* Loss function is differentiable\n",
    "* Loss function is inflated more where the model is more wrong, thus it will be pushed towards making a model focusing on these harder problems\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_gradient-boosting.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ How does gradient boosting work for a classification problem? How do we even make sense of the notion of a gradient in that context? The short answer is that we appeal to the probabilities associated with the predictions for the various classes. See more on this topic [here](https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/). <br/> $\\rightarrow$ Why is this called \"_gradient_ boosting\"? Because using a model's residuals to build a new model is using information about the derivative of that model's loss function. See more on this topic [here](https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "$\\gamma$ -- this is the greek letter, **_gamma_** which is for learning rate\n",
    "\n",
    "Remember that too high of a learning rate is good to quickly train but won't find the best setting, and can lead to bouncing.\n",
    "\n",
    "A small learning rate will take longer to train and can get stuck in local minimums easier but will find a better value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mean squared error (MSE) and want to minimize that <-- done by gradient descent\n",
    "\n",
    "Use the residuals (pattern in the residuals) to create an even better model\n",
    "\n",
    "1. Fit a model to the data, $F_1(x) = y$\n",
    "2. Fit a model to the residuals, $h_1(x) = y - F_1(x)$\n",
    "3. Create a new model, $F_2(x) = F_1(x) + h_1(x)$\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Iterative Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Parts adapted from https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Extreme Gradient Boosting\n",
    "* Handles missing values for you\n",
    "* Runs on multiple cpu cores in parallel\n",
    "* Distributes training across computer clusters\n",
    "* Go-to competition Algorithm\n",
    "* Always use multiple algorithms but it's a top dog right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Recommendation Systems\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Pipelines simplify the process of applying multiple transformations and fitting models.\n",
    "- ColumnTransformer and FeatureUnion offer flexibility in preprocessing different types of features.\n",
    "- GridSearchCV and RandomizedSearchCV help in tuning hyperparameters efficiently.\n",
    "- SMOTE and ImbPipeline address class imbalance issues effectively.\n",
    "- Bagging, random forests, extra trees, averaging, and stacking are powerful techniques for building robust models.\n",
    "- Hyperparameter tuning and model stacking can significantly improve model performance.\n",
    "\n",
    "### Introduction\n",
    "- **Boosting Techniques Overview**\n",
    "  - Adaptive Boosting (AdaBoost)\n",
    "  - Gradient Boosting (GB)\n",
    "  - Difference: AdaBoost uses weights on misclassified data points; GB uses residuals to improve model accuracy.\n",
    "\n",
    "### Key Concepts in Recommendation Systems\n",
    "- **Cold Start Problem**: Difficulty in recommending items to new users without historical data.\n",
    "- **Implicit vs. Explicit Data**\n",
    "  - **Implicit Data**: User behaviors (e.g., click history, time spent on page).\n",
    "  - **Explicit Data**: Direct feedback (e.g., ratings).\n",
    "\n",
    "### Types of Filtering\n",
    "- **Content-Based Filtering**\n",
    "  - Recommends items similar to those the user liked in the past.\n",
    "  - Based on item attributes and user preferences.\n",
    "  - **Example**: Pandora recommends music with similar properties.\n",
    "\n",
    "- **Collaborative Filtering**\n",
    "  - Recommends items based on the preferences of similar users.\n",
    "  - **User-User Collaborative Filtering**: Finds users similar to the target user and recommends items they liked.\n",
    "  - **Item-Item Collaborative Filtering**: Recommends items that are similar to items the user liked.\n",
    "  - **Example**: Netflix recommends shows watched by users with similar viewing histories.\n",
    "\n",
    "### Memory-Based vs. Model-Based Systems\n",
    "- **Memory-Based Systems**\n",
    "  - Use the entire user-item dataset.\n",
    "  - Compute similarity scores between users/items.\n",
    "  - Metrics: Cosine Similarity, Pearson Correlation.\n",
    "\n",
    "- **Model-Based Systems**\n",
    "  - Use machine learning models to make recommendations.\n",
    "  - Example Techniques: Matrix Factorization, Alternating Least Squares (ALS).\n",
    "\n",
    "### Evaluating Recommendation Systems\n",
    "- **Metrics**\n",
    "  - Root Mean Square Error (RMSE)\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - Precision, Recall, and F1-Score for binary recommendations.\n",
    "\n",
    "### Alternating Least Squares (ALS)\n",
    "- **Concept**\n",
    "  - Factorizes the user-item matrix into two lower-dimensional matrices.\n",
    "  - Iteratively minimizes the error between predicted and actual ratings.\n",
    "  - Uses pseudo-inverse to handle non-square matrices.\n",
    "  - Helps fill in missing values in sparse datasets.\n",
    "\n",
    "### Implementation with Surprise Library\n",
    "- **Surprise Library**\n",
    "  - Specialized for building and analyzing recommender systems.\n",
    "  - Handles datasets with user, item, and rating columns.\n",
    "  - Provides various algorithms: KNNBasic, SVD, NMF.\n",
    "\n",
    "- **Example Workflow**\n",
    "  - Load dataset with user, item, and rating.\n",
    "  - Split data using Surprise’s train-test split.\n",
    "  - Instantiate and train models (KNNBasic, SVD, NMF).\n",
    "  - Evaluate model performance using RMSE and MAE.\n",
    "\n",
    "### Practical Considerations\n",
    "- **Bias and Overfitting**\n",
    "  - Beware of biases from users who rate disproportionately.\n",
    "  - Ensure model generalizes well to new, unseen data.\n",
    "\n",
    "- **Combining Techniques**\n",
    "  - Hybrid models can combine content-based and collaborative filtering.\n",
    "  - Use ensemble methods to improve recommendation accuracy.\n",
    "\n",
    "### Summary\n",
    "- Recommendation systems leverage user behavior and item attributes to suggest items.\n",
    "- Balancing content-based and collaborative filtering improves recommendations.\n",
    "- Matrix factorization techniques like ALS are powerful for handling sparse datasets.\n",
    "- Evaluating recommendation accuracy is crucial for effective recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems\n",
    "* Allows predicting the future preference list\n",
    "\n",
    "## Matrix Factorization\n",
    "* Singular Value Decomposition (SVD) and Alternating Least Squares (ALS)\n",
    "\n",
    "## Surprise Library\n",
    "* Used to create recommendation systems and runs really optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Expose People to What They Like\n",
    "* Predicts the future preference of a set of items or user\n",
    "* Taps into the \"long tail\", there's very common items everyone buys but the long tail specific items, like a certain genre of music or special toy are long tail\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/LongTailConcept.png\" alt=\"graph showing products on the x-axis and popularity on the y-axis. a few products are very popular, labeled Head. many other products are not very popular, labeled Long Tail\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal Definition\n",
    "***Recommendation Systems are software agents that elicit the interests and preferences of individual consumers […] and make recommendations accordingly. They have the potential to support and improve the quality of the\n",
    "decisions consumers make while searching for and selecting products online.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applications of Recommendation Systems\n",
    "* Suggest items to a customer\n",
    "* Estimate profit & loss of many competing items and make recommendations to the customer (e.g. buying and selling stocks)\n",
    "* Recommend a product or service based on experience of the custoemr\n",
    "* Show offers appealing to a customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Recommendation Systems\n",
    "* Unpersonalized and Personalized\n",
    "\n",
    "### Unpersonalized\n",
    "* EX: Youtube recommending the most viewed videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized\n",
    "__Given__: \n",
    "The profile of the \"active\" user and possibly some situational context, i.e. user browsing a product or making a purchase etc. \n",
    "\n",
    "__Required__:\n",
    "Creating a set of items, and a score for each recommendable item in that set\n",
    "\n",
    "__Profile__:\n",
    "\n",
    "User profile may contain past purchases, ratings in either implicit or explicit form, demographics and interest scores for item features \n",
    "\n",
    "> There are two ways to gather such data. The first method is to ask for explicit ratings from a user, typically on a concrete rating scale (such as rating a movie from one to five stars). The second is to gather data implicitly as the user is in the domain of the system - that is, to log the actions of a user on the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these techniques make use of different similarity metrics to determine how \"similar\" items are to one another. \n",
    "* [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n",
    "* [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "* [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "* [Jaccard index (useful with binary data)](https://en.wikipedia.org/wiki/Jaccard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Recommenders \n",
    "\n",
    "> __Main Idea__: If you like an item, you will also like \"similar\" items.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/content_based.png\" alt=\"content based filtering. user watches movies, then similar movies are recommended to the user\" width=\"500\">\n",
    "\n",
    "* These systems are based on the characteristics of the items themselves. \"Try other items like this\"\n",
    "* Gives the user a bit more information on why they are seeing the recommendation\n",
    "* Require manual or semi-manual tagging of products\n",
    "* advanced systems can average all items a user liked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering Systems\n",
    "\n",
    "\n",
    "> __Main Idea__: If user A likes items 5, 6, 7, and 8 and user B likes items 5, 6, and 7, then it is highly likely that user B will also like item 8.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/collaborative_filtering.png\" alt=\"collaborative filtering: movies watched by both users indicate that the users are similar, then movies are recommended by one user to another user\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The key idea behind collaborative filtering is that similar users share similar interests and that users tend to like items that are similar to one another.__\n",
    "\n",
    "* Often based off user reviews\n",
    "* Have a cold start problem on how to recommend things to new users that have no activity yet.\n",
    "\n",
    "## Utility Matrix reprsents the associated opinion that a user holds.\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           | 2          |                | 5         |\n",
    "| Lore   | 2         |            | 4              |           |\n",
    "| Mike   |           | 5          | 3              | 2         |\n",
    "| Forest | 5         |            | 1              |           |\n",
    "| Taylor | 1         | 5          |                | 2         |\n",
    "\n",
    "$r_{\\text{Mike},\\text{Little Mermaid}} = 3$.\n",
    "\n",
    "A recommendation system tries to fill in the blanks.  Most of the time these values are largely empty.\n",
    "The matrix above is what  is known as an explicit rating.  Each person has rated what they've seen.  However we can infer or use judgement to determine how to use data for a recommendation system.\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           |  1         |                | 1         |\n",
    "| Lore   | 1         |            | 1              |           |\n",
    "| Mike   |           | 1          | 1              | 1         |\n",
    "| Forest | 1         |            | 1              |           |\n",
    "| Taylor | 1         | 1          |                | 1         |\n",
    "\n",
    "These are __implicit__ ratings because we are assuming that because a person has bought something, they would like to buy other items like it. Of course, this is not necessarily true, but it's better than nothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Create  clusters that have high similarity between the data belonging to one cluster while aiming for minimal similarity between clusters\n",
    "\n",
    "## K-Means Clustering\n",
    "K determines the number of clusters and the algorithm optimizes around that\n",
    "\n",
    "## Hierarchial Agglomerative Clustering\n",
    "You start with $n$ clusters equal the number of data points and at each step you join two clusters.  You stop joining when a certain criterion is reached.\n",
    "\n",
    "## Semi-Supervised Learning\n",
    "Combine both concepts of supervised and unsupervised learning.  Increasingly popular.\n",
    "\n",
    "## Market Segmentation with Clustering\n",
    "Common and useful, we'll practice with a market segmentation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering\n",
    "The most popular and widely used clustering algorithm, and clustering are one of the most popular unsupervised machine learning algorithms. \n",
    "\n",
    "## Goal\n",
    "> **Intra**-class similarity is high\n",
    "\n",
    "> **Inter**-class similarity is low\n",
    "\n",
    "Similarity is determined by distance.  Closer is more similar.\n",
    "* **Agglomerative hierarchical** algorithm starts with n clusters\n",
    "* **Non-heirarchical** chooses k initial clusters\n",
    "\n",
    "Unsupervised and you do not know how many clusters you are looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Hierarchical Clustering with K-Means Clustering\n",
    "\n",
    "### Process\n",
    "1. Select $k$ initial seeds \n",
    "2. Assign each observation to the cluster to which it is \"closest\"\n",
    "3. Loop\n",
    "    - Cluster center is the mean of all points in the cluster, recalculated each iteration.\n",
    "    - Each iteration reassign points to be part of the closest cluster center.\n",
    "    - Stop if there is no reallocation \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/good-centroid-start.gif' alt=\"k-means clustering animation\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set number of clusters at initialization time\n",
    "k_means = KMeans(n_clusters=3) \n",
    "\n",
    "# Run the clustering algorithm\n",
    "k_means.fit(some_df) \n",
    "\n",
    "# Generate cluster index values for each row\n",
    "cluster_assignments = k_means.predict(some_df) \n",
    "\n",
    "# Cluster predictions for each point are also stored in k_means.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Variance Ratio\n",
    "* Accepted metric in wide use is **_Variance Ratio_** aka [**_Calinski Harabasz Score_**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)\n",
    "    * The variance of the points within a cluster to the variance of a point to points in other clusters.\n",
    "    * We want intra-cluster variance to be low suggesting the clusters are tightly knit.\n",
    "    * We want inter-cluster variance to be high suggesting that there is little to no ambiguity about which cluster a point belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Variance Ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code builds on the previous example\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Note that we could also pass in k_means.labels_ instead of cluster_assignments\n",
    "print(calinski_harabasz_score(some_df, cluster_assignments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics\n",
    "* [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
    "* No metric is best, each have diff strengths weaknesses based on given goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal K Value\n",
    "1. Fit different K-means clustering objects for every $k$ we want to try then compare the variance ratio scores of each.\n",
    "2. Visualize results with an **_Elbow Plot_** - plots that we can easily see wehre we hit a point of diminishign returns.  They are used with more than just variance ratios, one example is distortion another clustering metric.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_elbow-method.png' alt=\"Calinski Harabaz scores for different values of k\" width='500'>\n",
    "\n",
    "#### Understanding the Elbow\n",
    "\n",
    "A note on elbow plots: higher scores aren't always better. Higher values of $k$ mean introducing more overall complexity -- we will sometimes see elbow plots that look like this:\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_dim_returns.png' alt=\"plot with the number of clusters on the x-axis and the sum of squared distances to cluster center on the y-axis\" width=\"500\">\n",
    "\n",
    "$k$ = 20 is technically better as a score but $k$ = 4 is better because it balances model complexity with score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5.1: Agglomerative Clustering\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Pipelines simplify the process of applying multiple transformations and fitting models.\n",
    "- ColumnTransformer and FeatureUnion offer flexibility in preprocessing different types of features.\n",
    "- GridSearchCV and RandomizedSearchCV help in tuning hyperparameters efficiently.\n",
    "- SMOTE and ImbPipeline address class imbalance issues effectively.\n",
    "- Bagging, random forests, extra trees, averaging, and stacking are powerful techniques for building robust models.\n",
    "- Hyperparameter tuning and model stacking can significantly improve model performance.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "- **Pop Quiz: Quick Review**\n",
    "  - **Elbow Method:** Used to find the optimal number of clusters in K-means clustering by identifying the \"elbow point\" where adding another cluster does not significantly decrease the variance.\n",
    "  - **K-means Clustering:** An iterative algorithm that partitions data into K clusters by minimizing the distance between data points and the centroid of their assigned cluster.\n",
    "  - **Hyperparameters in K-means:** Number of clusters (K), distance metric.\n",
    "  - **When Elbow Method Fails:** Smooth curve without a clear elbow point, clusters of different shapes, sizes, and densities.\n",
    "\n",
    "### Agglomerative Clustering Overview\n",
    "\n",
    "- **Definition:** A type of hierarchical clustering that builds nested clusters by merging or splitting them successively.\n",
    "- **Approach:** Bottom-up method starting with each point as its own cluster and merging the closest pairs of clusters step by step.\n",
    "- **Comparison with K-means:**\n",
    "  - K-means is flat clustering; agglomerative clustering is hierarchical.\n",
    "  - K-means requires the number of clusters to be specified; agglomerative clustering does not.\n",
    "\n",
    "### Steps in Agglomerative Clustering\n",
    "\n",
    "1. **Calculate Pairwise Distance:** Determine the distance between each pair of data points.\n",
    "2. **Linkage Criteria:** Methods to determine the distance between clusters:\n",
    "   - Single Linkage: Minimum distance between points in two clusters.\n",
    "   - Complete Linkage: Maximum distance between points in two clusters.\n",
    "   - Average Linkage: Average distance between points in two clusters.\n",
    "3. **Merge Closest Clusters:** Form a new cluster by merging the closest pair of clusters.\n",
    "4. **Repeat:** Continue merging until all points are in one cluster or a stopping criterion is met.\n",
    "\n",
    "### Dendrogram\n",
    "\n",
    "- **Definition:** A tree-like diagram that records the sequences of merges or splits.\n",
    "- **Usage:** Helps in determining the number of clusters by visualizing the hierarchical relationship between data points.\n",
    "- **Threshold:** The horizontal cut-off line in the dendrogram to decide the number of clusters.\n",
    "\n",
    "### Implementing Agglomerative Clustering\n",
    "\n",
    "#### Using Scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Calculate the pairwise distance matrix\n",
    "distance_matrix = pdist(data, metric='euclidean')\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering\n",
    "Z = linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Create a dendrogram\n",
    "dendrogram(Z)\n",
    "plt.show()\n",
    "\n",
    "# Form flat clusters\n",
    "clusters = fcluster(Z, t=50, criterion='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model\n",
    "model = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data)\n",
    "\n",
    "# Assign labels to data points\n",
    "labels = model.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Clusters\n",
    "\n",
    "- **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters.\n",
    "- **Cophenetic Correlation Coefficient:** Measures how faithfully the dendrogram represents the dissimilarities among observations.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Can find clusters with arbitrary shapes.\n",
    "  - No need to specify the number of clusters upfront.\n",
    "- **Disadvantages:**\n",
    "  - Computationally intensive, especially with large datasets.\n",
    "  - Sensitive to noise and outliers.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Market segmentation\n",
    "- Social network analysis\n",
    "- Image segmentation\n",
    "- Anomaly detection\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Agglomerative clustering is a powerful tool for hierarchical clustering that helps in identifying nested clusters within data. It offers flexibility in terms of linkage criteria and does not require the number of clusters to be predefined. However, it is computationally intensive and requires careful interpretation of dendrograms and threshold settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Agglomerative Clustering\n",
    "* K-means uses Expectation-Maximization after we tell it to give us $k$ clusters, however it can not have subgroups within subgroups\n",
    "* Agglomerative Clustering to the rescue!  It can have subgroups within subgroups\n",
    "* It starts with $n$ clusters with $n$ = the number of data points then merges until some stopping criterion\n",
    "\n",
    "## Linking Clusters Together\n",
    "\n",
    "* **ward** - merges two cluster on the least variance between them.  Leads to more equally sized clusters\n",
    "* **average** - merges the two clusters that have the smallest average distance between all points\n",
    "* **complete** - merges the two clusters that have the smallest maximum distance between their points\n",
    "\n",
    "Can affect the performance, which to use is based on the data and goals.\n",
    "\n",
    "The following diagram demonstrates the clusters created at each step for a dataset of 16 points. Take a look at the diagram and see if you can figure out what the algorithm is doing at each step as it merges clusters together:\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/hac_iterative.png' alt=\"initialization through step 14 of HAC algorithm\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it takes the closest clusters and merges them into a single cluster.  Below shows as the dots disappear the visualization is repalcing them with the newly calculated center.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/dendrogram_gif.gif' alt=\"animation of clusters shown in x-y space on the left and a dendrogram on the right, showing which clusters correspond to which parts of the dendrogram\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms and Clustergrams\n",
    "* Easily visualize the results at any given step \n",
    "* The image to the right above in the gif is a Dendrogram\n",
    "    * shows the hierarchical realtionship between the various clusters that are computed throughout each step.  \n",
    "* The image below is a Clustergram\n",
    "    * Visualize the same information by drawing lines representing each cluster at the each step\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/new_clustergram.png' alt=\"another view of clusters on the left and dendrogram on the right\" width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases\n",
    "* market segmentation\n",
    "    * things like market segmentation\n",
    "* gain a deeper understanding of a dataset through cluster analysis\n",
    "* photo sorting on smartphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Problems with Clustering Algorithms\n",
    "* No way of verifying the results are correct or not\n",
    "    * Never treat results of a cluster as ground-truth\n",
    "## Advantages and Disadvantages of K-Means Clustering\n",
    "### Advantages\n",
    "* Easy to implement\n",
    "* Usually faster than HAC with reasonably small $k$ and many features\n",
    "* Objects can shift clusters\n",
    "* Tighter clusters than HAC\n",
    "\n",
    "### Disadvantages\n",
    "* Need the right value for $k$\n",
    "* Scaling completely changes the results\n",
    "* Starting points have a strong impact on final results, as seen below.  Bad init is less likely than good init and you can run it multiple times.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/bad-centroid-start.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages of HAC\n",
    "\n",
    "### Advantages\n",
    "* ordered relationship between clusters, which can be useful when visualized\n",
    "* smaller clusters which allows more granular understanding\n",
    "\n",
    "### Disadvantages\n",
    "* Results depend on distance metric used\n",
    "* Objects can be grouped badly early on and no way to move them\n",
    "* We can't check visuals on more than 3 dimensions so it's hard to know when the algorithm was correct\n",
    "* Clustergram below\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_bad-hac.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning and Look-Alike Models\n",
    "* Combining both to solve real world problems\n",
    "\n",
    "## Case 1: Look-Alike Models\n",
    "* Find a similar audience\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_look-alike-model.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identify more customers/market segments that we can plausibly assume are equally valuable due to their similarity with valuable custmoers or market segement we already identified.\n",
    "* Divide into two groups: the ones we know are valuable and everyone else\n",
    "* Uses distance metric of choice to rate similarity of the unknown customers with the ones we have identified\n",
    "* Once we know they are somewhat similar to the valuable group we can spend resources to capture them\n",
    "* Likely see customers that are only somewhat similar to our valuable group\n",
    "* Customer sthat look nothing like our known valuable customer segment\n",
    "* It is a lot like clustering\n",
    "* referred to as prospecting.\n",
    "* choose resources to market to the customers that look like our valuabel customers to increase our top-of-funnel, meaning an increase to the number of potential customers that haven't shown interest in our product or cmpany yet but are likely to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Semi-Supervised Learning\n",
    "* Known as weakly supervised learning too\n",
    "* Generate Pseudo-labels that are possibly correct.\n",
    "    * doesn't use clustering, it uses supervised learning algorithms in an unsupervised way.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_semi-supervised.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. **Train your model on your labeled training data**\n",
    "2. **Use your trained model to generate pseudo-labels for unlabeled data**\n",
    "3. **Combine the pseudo-labels with your actual data**\n",
    "4. **Retrain your model on the new data set**\n",
    "#### Benefits\n",
    "* It is risky\n",
    "* When done well it can increase overall model performance by opening up access to much more data\n",
    "* Saves a ton of money on labeling costs!\n",
    "\n",
    "#### Downsides\n",
    "* When data is really noisy incorrect lables will skew the model\n",
    "* Feedback Loops\n",
    "* More complicated problems tend to work less\n",
    "\n",
    "#### Use a Holdout Set to Test\n",
    "* As usual but even more important in this case, make sure to use ground-truth or non pseudo-code labels to test with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Coefficient\n",
    "\n",
    "The Silhouette Coefficient is a measure used to evaluate the quality of clusters created by a clustering algorithm. It takes into account both the cohesion within clusters and the separation between clusters.\n",
    "\n",
    "## Definition\n",
    "\n",
    "For a given data point $i$, the Silhouette Coefficient $s(i)$ is defined as:\n",
    "\n",
    "$$ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
    "\n",
    "where:\n",
    "- $a(i)$ is the mean distance between $i$ and all other points in the same cluster.\n",
    "- $b(i)$ is the mean distance between $i$ and all points in the nearest cluster (the cluster with the smallest mean distance to $i$).\n",
    "\n",
    "## Interpretation\n",
    "**Higher is better**\n",
    "\n",
    "- $s(i)$ ranges from -1 to 1.\n",
    "  - $s(i) \\approx 1$: The data point is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "  - $s(i) \\approx 0$: The data point is on or very close to the decision boundary between two neighboring clusters.\n",
    "  - $s(i) \\approx -1$: The data point is poorly matched to its own cluster and well-matched to a neighboring cluster.\n",
    "\n",
    "## Overall Silhouette Score\n",
    "\n",
    "The overall Silhouette Score for a clustering is the mean Silhouette Coefficient of all data points:\n",
    "\n",
    "$$ S = \\frac{1}{N} \\sum_{i=1}^{N} s(i) $$\n",
    "\n",
    "where $N$ is the total number of data points.\n",
    "\n",
    "## Usage\n",
    "\n",
    "The Silhouette Coefficient can be used to:\n",
    "- Determine the optimal number of clusters by comparing the average silhouette scores for different numbers of clusters.\n",
    "- Evaluate the quality of clustering algorithms, with higher scores indicating better-defined clusters.\n",
    "\n",
    "## Example\n",
    "\n",
    "To compute the Silhouette Coefficient in Python, you can use the `silhouette_score` function from the `sklearn.metrics` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# X is your data and labels are the cluster labels\n",
    "score = silhouette_score(X, labels)\n",
    "print(f'Silhouette Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Principal Component Analysis in scikit-learn\n",
    "* Reduces dimensions while trying to capture as much info from the dataset as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms dataset along principal axes.  The first axes tries to capture the maximum variance within the data.  From here additional axes are constructed which are orthogonal to the previous axes and continue to account for as much of the remaining variance as possible.\n",
    "\n",
    "Transforms this:\n",
    "\n",
    "<img src=\"images/pca-data1.png\">\n",
    "\n",
    "Into this:\n",
    "\n",
    "<img src=\"images/pca-data2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/inhouse_pca.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99760273e-01, 2.39727247e-04])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99976027, 1.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize the first PCA component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pca-data3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Performing PCA\n",
    "\n",
    "The theory behind PCA rests upon many foundational concepts of linear algebra. After all, PCA is re-encoding a dataset into an alternative basis (the axes). Here are the exact steps:\n",
    "\n",
    "1. Recenter each feature of the dataset by subtracting that feature's mean from the feature vector\n",
    "2. Calculate the covariance matrix for your centered dataset\n",
    "3. Calculate the eigenvectors of the covariance matrix\n",
    "    1. You'll further investigate the concept of eigenvectors in the upcoming lesson\n",
    "4. Project the dataset into the new feature space: Multiply the eigenvectors by the mean-centered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Segmentation with Clustering\n",
    "* one of the most popular use cases for clustering\n",
    "\n",
    "# What is Market Segmentation?\n",
    "* **Cluster Analysis** to segment a customer base into different _market segments_ using the clustering techniques we've learned\n",
    "* Ex: decide marketing budget allocation in order to attract more customers\n",
    "    * Create personalized regression models for each group\n",
    "* Know who your customer is.  Identify sements in the customer data we can look for trends\n",
    "    * Ex decide the station to run commercials on\n",
    "* Find the segments with clustering\n",
    "    * find them based on behavior\n",
    "\n",
    "## Targeting\n",
    "Segmentation is just the first step\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_marketing-strategy.png' width='700'>\n",
    "\n",
    "* Build individualized strategies\n",
    "    * which market segment is most valuable to us? Use resarch and data analysis\n",
    "    * how do we allocate the advertising budget?  determine where to spend money best to reach the group\n",
    "* Figure out how to position our product to make it both desirable and stand out from competitors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Natural Language Tool Kit (NLTK)\n",
    "Popular NLP library in Python\n",
    "\n",
    "## Regular Expressions\n",
    "regex\n",
    "\n",
    "## Feature Engineering for Text Data\n",
    "Text data has a lot of ambiguity and feature engineering for NLP is specific.  \n",
    "* How to remvoe stop words\n",
    "* create frequency distributions\n",
    "* representing histograms\n",
    "* stemming\n",
    "* lemmatization\n",
    "* bigrams which shows how often two words occur together\n",
    "\n",
    "## Context-Free Grammars and Part-of-Speech (POS) Tagging\n",
    "* Context Free Grammar and Part of Speech tagging\n",
    "* POS tagging helps a computer understand how to interpret a sentence\n",
    "* Context free grammars (CFG) defines the rules of how sentences can exist.\n",
    "\n",
    "## Text Classification\n",
    "Will be gone over\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "The study of how computers can interact with humans through natural language\n",
    "\n",
    "* intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*\n",
    "* History\n",
    "    * Used to be rules based with rules borrowed from Linguistics\n",
    "    * Around the 1980s machine learning and AI started to show potential\n",
    "    * Now it is used around the globe every data by data scientists everywhere\n",
    "\n",
    "## NLP and Bayesian Statistics\n",
    "**_Naive Bayesian Classification_** is what keeps spam email at bay.\n",
    "\n",
    "## Working With Text Data\n",
    "Can require more cleaning and preprocessing than many other data types\n",
    "\n",
    "## Creating a Bag of Words\n",
    "* **Corpus**: a large structured text set used for NLP tasks\n",
    "* **Bag of Words**: vectorize data by capturing the unique words in a work, in any order.  A common way to do that is have usage counts of all the unique words.\n",
    "\n",
    "## Basic Cleaning and Tokenization\n",
    "* Often lowercase everything and remove punctuation\n",
    "* Decisions have to be made on how to tokenize and what variations of words to count as the same or different.  Ex. run and runs, Apple and Apple's\n",
    "\n",
    "### Stemming, Lemmatization, and Stop Words\n",
    "* **Stemming** reduces words to their root, in a crude way.  For example runs and running would become run but ponies would become poni.\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-nlp-and-word-vectorization/master/images/new_stemming.png' alt=\"stemming rules and examples\" width=\"400\">\n",
    "* **Lemmatization** uses **morphology** to reduce words to their basic forms called **lemma**\n",
    "|   Word   |  Stem | Lemma |\n",
    "|:--------:|:-----:|:-----:|\n",
    "|  Studies | Studi | Study |\n",
    "| Studying | Study | Study |\n",
    "* **Stop Words** have little to no information.  examples are \"of\" and \"the\".  Stop words are often removed from models to cut down on dimensionality.\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "### Count \n",
    "Count the number of times a word appears in a corpus.\n",
    "\n",
    "| Document | Aardvark | Apple | ... | Zebra |\n",
    "|:--------:|:--------:|:-----:|-----|-------|\n",
    "|     1    |     0    |   3   | ... | 1     |\n",
    "|     2    |     1    |   2   | ... | 0     |\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF) Vectorization\n",
    "Idea that rare words contain more information about a document than words used all the time in all documents.\n",
    "\n",
    "\n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large Term\\ Frequency(t) = \\frac{number\\ of\\ times\\ t\\ appears\\ in\\ a\\ document} {total\\ number\\ of\\ terms\\ in\\ the\\ document} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large IDF(t) = log_e(\\frac{Total\\ Number\\ of\\ Documents}{Number\\ of\\ Documents\\ with\\ t\\ in\\ it})$$\n",
    "\n",
    "The **_TF-IDF_** value is the product of both. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6: Natural Language Processing (NLP)\n",
    "\n",
    "### Introduction to NLP\n",
    "We're going to go into natural language processing (NLP). In this lecture, we will cover text processing, vectorization, and classification. We'll begin with text processing, followed by vectorization, and finally classification.\n",
    "\n",
    "### Required Installations\n",
    "Ensure you have installed the necessary libraries. You can do this using pip:\n",
    "```bash\n",
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to download additional nltk data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of NLP Tasks\n",
    "\n",
    "- **Supervised Learning:** Classify documents (e.g., spam detection).\n",
    "- **Unsupervised Learning:** Group documents into topics without predefined labels.\n",
    "\n",
    "## Representing Text for Models\n",
    "\n",
    "To feed text into models, we need to convert it into numerical form using techniques like Count Vectorizer or TF-IDF Vectorizer.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into individual tokens (words or phrases).\n",
    "\n",
    "- **Word Tokenizer:** Splits text into words.\n",
    "- **Sentence Tokenizer:** Splits text into sentences.\n",
    "\n",
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Steps\n",
    "\n",
    "1. **Tokenization:** Convert sentences to words.\n",
    "2. **Removing Stopwords:** Filter out common words that don't add significant meaning.\n",
    "3. **Lowercasing:** Convert all text to lowercase to ensure uniformity.\n",
    "4. **Removing Punctuation and Numbers:** Eliminate unnecessary symbols and digits.\n",
    "5. **Lemmatization/Stemming:** Reduce words to their base or root form.\n",
    "\n",
    "## Example Code\n",
    "\n",
    "Tokenizing and cleaning text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lowercase and remove stopwords\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "data['processed_text'] = data['text_column'].apply(preprocess_text)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Clusters\n",
    "\n",
    "- **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters.\n",
    "- **Cophenetic Correlation Coefficient:** Measures how faithfully the dendrogram represents the dissimilarities among observations.\n",
    "\n",
    "## Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "- Can find clusters with arbitrary shapes.\n",
    "- No need to specify the number of clusters upfront.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally intensive, especially with large datasets.\n",
    "- Sensitive to noise and outliers.\n",
    "\n",
    "## Applications\n",
    "\n",
    "- Market segmentation\n",
    "- Social network analysis\n",
    "- Image segmentation\n",
    "- Anomaly detection\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Agglomerative clustering is a powerful tool for hierarchical clustering that helps in identifying nested clusters within data. It offers flexibility in terms of linkage criteria and does not require the number of clusters to be predefined. However, it is computationally intensive and requires careful interpretation of dendrograms and threshold settings.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next class, we will cover vectorization and classification. Ensure you are familiar with the concepts discussed today. If you have any questions, please feel free to ask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6.2 NLP Classification\n",
    "\n",
    "## Review of Tokenization and Pre-Processing\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization is the process of splitting text into individual tokens. These tokens can be words, phrases, or even characters. There are different types of tokenization:\n",
    "\n",
    "- **Word Tokenization:** Splits text into words.\n",
    "- **Sentence Tokenization:** Splits text into sentences.\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "Stop words are common words (like \"the\", \"and\", \"is\") that are often removed from text data to reduce noise and focus on the meaningful words.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "- **Stemming:** Reduces words to their base form by removing suffixes. It can produce non-existent words.\n",
    "- **Lemmatization:** Reduces words to their base form considering the part of speech, resulting in valid words.\n",
    "\n",
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Example text preprocessing\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Removing stop words\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Vectorization is the process of converting text into numerical form so it can be fed into machine learning models.\n",
    "\n",
    "### Types of Vectorizers\n",
    "\n",
    "- **Count Vectorizer:** Converts text into a matrix of token counts.\n",
    "- **TF-IDF Vectorizer:** Converts text into a matrix of token counts scaled by inverse document frequency.\n",
    "\n",
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Example text corpus\n",
    "corpus = [\n",
    "'This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Count Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X_count.toarray())\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Clusters\n",
    "\n",
    "- **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters.\n",
    "- **Cophenetic Correlation Coefficient:** Measures how faithfully the dendrogram represents the dissimilarities among observations.\n",
    "\n",
    "## Advantages and Disadvantages of Clustering\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Can find clusters with arbitrary shapes.\n",
    "- No need to specify the number of clusters upfront.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Computationally intensive, especially with large datasets.\n",
    "- Sensitive to noise and outliers.\n",
    "\n",
    "## Applications\n",
    "\n",
    "- Market segmentation\n",
    "- Social network analysis\n",
    "- Image segmentation\n",
    "- Anomaly detection\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Agglomerative clustering is a powerful tool for hierarchical clustering that helps in identifying nested clusters within data. It offers flexibility in terms of linkage criteria and does not require the number of clusters to be predefined. However, it is computationally intensive and requires careful interpretation of dendrograms and threshold settings.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next class, we will cover vectorization and classification. Ensure you are familiar with the concepts discussed today. If you have any questions, please feel free to ask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with NLTK\n",
    "**_Natural Language Tool Kit_**: A popular library for python for Natural Language Processing\n",
    "* Contains sample corpus like presidential speeches and project gutenberg transcripts.\n",
    "* Contains its own Bayesian classifiers for quick testing\n",
    "* Relies heavily on Linguistics but the tools are easy for a non linguist to use, ex making a parse tree\n",
    "<center> <img src='images/new_parse_tree.png'  width=\"750\"> </center>\n",
    "\n",
    "## Working with Text\n",
    "* **Stop Word Removal**\n",
    "* **Filtering and Cleaning**\n",
    "* **Feature Selection and Feature Engineering** - Libraries like Penn Tree Bank, part of speech tags, sentence polarity and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions (regex)\n",
    "* powerful for NLP\n",
    "* word_tokenize() splits a word that contains an apostrephe into 3 tokens.  they're becomes [\"they\",\"'\",\"re\"].  \n",
    "* we can use small regex patterns to capture they're as a word\n",
    "## Patterns\n",
    "* Regex is as good as the patterns we create\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = 'he said that she said \"hello\".'\n",
    "pattern = 'he'\n",
    "p = re.compile(pattern)\n",
    "p.findall(sentence) # Output will be ['he', 'he, 'he']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**anchors** can be used to define word boundaries\n",
    "\n",
    "## Ranges, Groups, Quantifiers\n",
    "* Range: `[A-Za-z0-9]` would match most alphanumeric chars in the English alphabet\n",
    "* Character Class Examples\n",
    "    * `\\d` is `[0-9]`\n",
    "    * `\\w` is any word\n",
    "    * `\\D` anything that isn't a digit\n",
    "    * `\\W` anything that isn't a word\n",
    "* Quantifiers - matches preceding ex. a* or (cat)* \n",
    "    * `*` 0 or more times\n",
    "    * `+` 1 or more times\n",
    "    * `?` 0 or 1 times\n",
    "    * `{n}` match exactly n times, ex `{3}` matches 3 times\n",
    "    * `{n,k}` match between n and k times, ex `{3,5}` matches 3-5 times\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/master/images/regex_cheat_sheet.png' alt=\"regex cheat sheet\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Text Data\n",
    "* Text data has a lot of ambiguity\n",
    "\n",
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(some_text_data)\n",
    "\n",
    "# It is usually a good idea to lowercase all tokens during this step, as well\n",
    "stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import FreqDist\n",
    "freqdist = FreqDist(tokens)\n",
    "\n",
    "# get the 200 most common words \n",
    "most_common = freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Often utilizes wordnet lexical database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('feet') # foot\n",
    "lemmatizer.lemmatize('running') # run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams and Mutual Information Score\n",
    "* Pair adjacent words and treat them as one token, special case of n-grams where n is 2\n",
    "    * n-grams can be created at the character level, and commonly done so\n",
    "    * useful because you can apply a freuency filter.  Common values are at least 5.\n",
    "    * **Pointwise Mutual Information Score** - from information theory a measure of mutual dependence between two words. ex San Franscisco would appear together a lot meaning it has a higher information score.\n",
    "    * NLTK can compute this too\n",
    "* \"the dog played outside\" becomes `('the', 'dog'), ('dog', 'played'), ('played', 'outside')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Free Grammars and POS Tagging\n",
    "\n",
    "> \"Colorless green ideas sleep furiuosly.\"\n",
    "Noam Chomsky.  \n",
    "\n",
    "While correct grammatically and syntactically, it's not meaningful semantically.  There is a \"deep structure\" we recognize as correct regardless of content.  This is what Context-Free Gammar is or CFG, the idea that we don't need any context to determine the grammar is correct.\n",
    "\n",
    "## Five Levels Of Language\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_LevelsOfLanguage-Graph.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CFGs are the Syntax Level**\n",
    "* CFGs are important to computer science due to parsers \n",
    "* Part of Speech (POS) Tags such as run being a noun or a verb.\n",
    "\n",
    "## Parse Trees and Sentence Structure\n",
    "* Sentences in English go Noun Phrase -> Verb Phrase -> Prepositional Phrase but this gets complicated because it can be recursive.  A noun phrase can have multiple noun phrases or even a verb phrase within it which themselves can contain noun and verb phrases.  This leads to ambiguity.\n",
    "\n",
    "> \"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\"\n",
    "\n",
    "The joke is the 2nd, least thought of meaning, is clarified by Marx in the 2nd sentence.  This ambiguity is hard for a computer processor.\n",
    "Parse trees help us understand the difference.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/parse_tree.png'>\n",
    "\n",
    "* Noun phrase: `['I']`\n",
    "* Verb phrase: `['shot', 'an', 'elephant']`\n",
    "* Prepositional phrase: `['in', 'my', 'pajamas']`\n",
    "\n",
    "VS\n",
    "\n",
    "* Noun phrase: `['I']`\n",
    "* Verb phrase: `['shot', 'an', 'elephant', 'in', 'my', 'pajamas']`\n",
    "\n",
    "The 2nd one treats in my pajamas as noun phrase within the verb phrase (so the pajamas belong to the elephant)\n",
    "The 1st one is more typical and treats the prepositional phrase as its own thing\n",
    "\n",
    "Put simply, we know what an elephant is, what pajamas are, and understand that it's highly unlikely that an elephant could fit in pajamas. This helps us determine how we understand that sentence on the fly -- computers don't have this luxury, so they don't know which to choose!\n",
    "\n",
    "## POS Tagging and CFGs\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/cfg.png'>\n",
    "\n",
    "Let's break down this CFG, and see if we can understand it a bit better. \n",
    "\n",
    "* `S -> NP VP` A sentence (S) consists of a Noun Phrase (NP) followed by a Verb Phrase (VP).\n",
    "* `PP -> P NP` A Prepositional Phrase (PP) consists of a Preposition (P) followed by a Noun Phrase (NP)\n",
    "* `NP -> Det N | Det N PP | 'I'` A Noun Phrase (NP) can consist of:\n",
    "    * a Determiner (Det) followed by a Noun (N), or (as denoted by `|`) \n",
    "    * a Determiner (Det) followed by a Noun (N), followed by a Prepositional Phrase (PP), or\n",
    "    * The token `'I'`.\n",
    "* `VP -> V NP | VP PP` A Verb Phrase can consist of:\n",
    "    * a Verb (V) followed by a Noun Phrase (NP) or\n",
    "    * a Verb Phrase (VP) followed by a Prepositional Phrase (PP)\n",
    "* `Det -> 'an' | 'my'` Determiners are the tokens 'an' or 'my'\n",
    "* `N -> 'elephant' | 'pajamas'` Nouns are the tokens 'elephant' or 'pajamas'\n",
    "* `V -> 'shot'` Verbs are the token 'shot'\n",
    "* `P -> 'in'` Prepositions are the token 'in'\n",
    "\n",
    "As we can see, the CFG provides explicit rules as to both:\n",
    "1. How sentences, noun phrases, verb phrases, and prepositional phrases may be structured\n",
    "2. What parts of speech each token belongs to \n",
    "\n",
    "This defines a very small CFG that allows the parser to successfully generate parse trees for the Groucho Marx's sentence. Note that both the parse trees seen above are valid, according to the rules defined in this grammar. Even though this grammar is quite explicit, both of them work.\n",
    "\n",
    "NLTK can get these tokens from things like the **Penn Tree Bank**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data Questions\n",
    "* Do we remove stop words or not?    \n",
    "* Do we stem or lemmatize our text data, or leave the words as is?   \n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?  \n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?  \n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?   \n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ethics\n",
    "## Where Does NLP Data Come From?\n",
    "### Ground Truth in Machine Learning\n",
    "* labels need to exist for each record for supervised learning\n",
    "    * these labels are called \"ground truth\"\n",
    "    * difference between predictions and ground truth is how the quality of a model is measured\n",
    "### Labeling\n",
    "* Traces back to qualitative learning\n",
    "    * social science researchers would code raw data\n",
    "    * determined before the analysis begins (also known as \"a priori\" or \"deductive\" codes), and are sometimes developed during the analysis (also known as \"grounded\" or \"inductive\" codes). Often multiple researchers will apply codes to the same text sample and then use statistical techniques to measure the [inter-rater reliability](https://www.statology.org/inter-rater-reliability/) and ensure that the codes are being applied to the samples in a consistent way\n",
    "\n",
    "### Data Labeling Today: Crowd Workers\n",
    "Data scientists ([and social scientists](https://blogs.lse.ac.uk/impactofsocialsciences/2020/12/15/how-to-conduct-valid-social-science-research-using-mturk-a-checklist/)) instead frequently use crowdsourcing platforms to label text data cheaply, quickly, and at scale.\n",
    "\n",
    "The most popular platform for data labeling is [Amazon Mechanical Turk (MTurk)](https://www.mturk.com/). Requesters can post \"Human Intelligence Tasks\" (HITs) such as data labeling to the MTurk platform and pay workers for completion of each individual task.\n",
    "\n",
    "#### MTurk and Privacy\n",
    "Sending any kind of personally-identifiable information (PII) or other sensitive data to a platform like MTurk risks the exposure of this data. A [Microsoft Research team](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/cscw14-crowdattack.pdf) even found that they could pay MTurk workers to steal user data if it was presented as part of a legitimate task!\n",
    "\n",
    "Some [data manipulation techniques](https://www.cs.cmu.edu/~jbigham/pubs/pdfs/2017/crowdmask.pdf) have been proposed to avoid making certain data identifiable, but the less-risky option is to avoid sending any kind of sensitive data to this kind of platform.\n",
    "\n",
    "#### MTurk and Working Conditions\n",
    "MTurk workers are independent contractors and are therefore usually not legally entitled to certain worker protections. Research has found that some requesters exploit these workers by [paying less than &#36;2 per hour](https://arxiv.org/abs/1712.05796) and create stressful working conditions by [mass rejecting completed tasks](https://blog.turkopticon.net/?p=731) with essentially no recourse for workers.\n",
    "\n",
    "\n",
    "## Implicit Labels Scraped from the Internet\n",
    "\n",
    "Given the logistical and ethical challenges that can arise in paying people to label data, some data scientists have moved towards using data that was never explicitly labeled. As a data scientist in [one study](https://arxiv.org/abs/1812.05239) put it:\n",
    "\n",
    "> There isn’t really a thought process surrounding... _Should [our team] ingest this data in?_ [...] If it is available to us, we ingest it.\n",
    "\n",
    "One example of such a readily-available data source is [Common Crawl](https://commoncrawl.org/). Common Crawl attempts to scrape the entire Internet every month or so, and hosts petabytes of data from each crawl. The scale and ease of access of this dataset has made it very appealing to data scientists.\n",
    "\n",
    "#### Scraped Data and Privacy Concerns\n",
    "Even when posting \"publicly\", website users often have an expectation that this data will not be aggregated and analyzed outside of its original context. For example, the [publication of a dataset](https://www.vox.com/2016/5/12/11666116/70000-okcupid-users-data-release) of 70,000 users scraped from OkCupid was broadly criticized. Social computing research Os Keyes [wrote](https://ironholds.org/scientific-consent/):\n",
    "\n",
    "> this is without a doubt one of the most grossly unprofessional, unethical and reprehensible data releases I have ever seen.\n",
    "\n",
    "(The dataset eventually taken down not because of ethical issues but because OkCupid filed a [DMCA](https://www.copyright.gov/dmca/) complaint).\n",
    "\n",
    "\n",
    "#### Scraped Data and Garbage In, Garbage Out\n",
    "Concerns about data quality are especially relevant when working with data that was scraped from random sources rather than collected with a particular intention in mind.\n",
    "\n",
    "Studies of the Common Crawl corpus in particular have found that it contains a [\"significant amount of undesirable content\"](https://aclanthology.org/2021.acl-short.24.pdf) including hate speech and sexually explicit content, and also that models trained on it [exhibit numerous historical biases](https://www.cs.princeton.edu/~arvindn/publications/language-bias.pdf) related to race and gender.\n",
    "\n",
    "## Bonus Topic: Large Language Models\n",
    "Another kind of model that uses text data but isn't a traditional text classifier is a ***large language model***. At the time of this writing, [GPT-3](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html), [GitHub Copilot](https://github.com/features/copilot), and [BERT](https://huggingface.co/blog/bert-101) are some popular examples of this type of model. Typically a large language model tries to predict the next word or words in a sequence, in a way that can _generate_ text rather than simply labeling it.\n",
    "\n",
    "These models are almost always developed with scraped data, because that is the only way to achieve the necessary scale. GPT-3, for example, was trained on [the Common Crawl corpus in addition to curated sources](https://techcrunch.com/2020/08/07/here-are-a-few-ways-gpt-3-can-go-wrong/). It also has demonstrated bias against [Muslims](https://hai.stanford.edu/news/rooting-out-anti-muslim-bias-popular-language-model-gpt-3), [women](https://aclanthology.org/2021.nuse-1.5), and [disabled people](https://arxiv.org/abs/2206.11993).\n",
    "\n",
    "In a more stunning example, the \"Tay\" chatbot started with a pre-trained dataset then \"learned\" racist and incendiary language from Twitter users, eventually being [pulled from the platform](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist) after less than a day. This shows that social media platforms, despite having some form of moderation, may be worse sources of training data than generic sources like Common Crawl.\n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "* [Qualitative Data Coding 101](https://gradcoach.com/qualitative-data-coding-101/)\n",
    "* [Data and its (dis)contents: A survey of dataset development and use in machine learning research](https://arxiv.org/abs/2012.05345)\n",
    "* [Documenting Data Production Processes: A Participatory Approach for Data Work](https://arxiv.org/abs/2207.04958)\n",
    "* [The trainer, the verifier, the imitator: Three ways in which human platform workers support artificial intelligence](https://journals.sagepub.com/doi/10.1177/2053951720919776)\n",
    "* [Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?](https://arxiv.org/abs/1912.08320)\n",
    "* [The dangers of data scraped from the internet](https://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/)\n",
    "* [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient in Gradient Descent\n",
    "\n",
    "> Here, in finding gradient ascent, our task is not to calculate the gain from a move in either the $x$ or $y$ direction.  Instead, our task is to **find some combination of a change in $x$,$y$ that brings the largest change in output**.  \n",
    "\n",
    "The gradient of the function $f(x,y)$, that is $ \\nabla f(x,y) = 2x + 3y $ is the following: \n",
    "\n",
    "$\\frac{df}{dx}(2x + 3y) = 2 $ and $\\frac{df}{dy}(2x + 3y) = 3 $.\n",
    "\n",
    "So what this tells us is to move in the direction of greatest ascent for the function $f(x,y) = 2x + 3y $, is to move up three and to the right two.  So we would expect our path of greatest ascent to look like the following.\n",
    "\n",
    "This example gives gradient **ascent** $\\nabla $.  So $\\nabla f(x, y) = \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta x} $.  This means that to take the path of greatest ascent, you should move $ \\frac{\\delta f}{\\delta y} $ divided by $ \\frac{\\delta f}{\\delta x} $.  So for example, when $ \\frac{\\delta f}{\\delta y}f(x, y)  = 3 $ , and $ \\frac{\\delta f}{\\delta x}f(x, y)  = 2$, you traveled in line with a slope of 3/2.\n",
    "\n",
    "For gradient descent, that is to find the direction of greatest decrease, you simply reverse the direction of your partial derivatives and move in $ - \\frac{\\delta f}{\\delta y}, - \\frac{\\delta f}{\\delta x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars, Vectors, Matrices, Tensors\n",
    "> **Scalar**: A single number\n",
    "* **Real valued scalars**: Let $S \\in  \\mathbb{R} $  be the salary of an individual\n",
    "* **Natural number scalars**: Let $n \\in \\mathbb{N}$ be the number of floors in a building\n",
    "\n",
    "\n",
    "> **Vector**: An **array** of numbers arranged in some order, as opposed to the single numbered scalar. \n",
    "\n",
    "\\begin{equation}\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "  x_{1} \\\\\n",
    "  x_{2} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{n-1} \\\\\n",
    "  x_{n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Where $x$ is the name of the vector and $(x_1,x_2, \\ldots, x_{n-1}, x_n)$ are the scalar components of the vector.\n",
    "\n",
    "A vector has direction and a magnitude (length).\n",
    "\n",
    "Below is an example of a vector in 3D vector space:  \n",
    "\n",
    "![](https://curriculum-content.s3.amazonaws.com/data-science/images/vec2.png)\n",
    "\n",
    "In python numpy arrays work well for vectors\n",
    "```python \n",
    "# create a vector from list [2,4,6]\n",
    "import numpy as np\n",
    "v = np.array([2, 4, 6])\n",
    "print(v)\n",
    "\n",
    "print (x[1:4])     # second to fourth element. Element 5 is not included\n",
    "print (x[0:-1:2])  # every other element\n",
    "print (x[:])       # print the whole vector\n",
    "print (x[::-1]) # reverse the vector!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Matrix** is a 2-dimensional array of numbers written between square brackets. \n",
    "\n",
    "$$\n",
    "   A=\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   A_{1,1} & A_{1,2} & \\ldots &A_{1,n} \\\\\n",
    "   A_{2,1} & A_{2,2} & \\ldots &A_{2,n} \\\\\n",
    "   \\vdots& \\vdots & \\ddots & \\vdots \\\\\n",
    "   A_{m,1} & A_{m,2} & \\ldots &A_{m,n} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "We usually give matrices uppercase variable names with bold typeface, such as $A$. If a real-valued matrix $A$ has a height of $m$ and a width of $n$ as above, we state this as $A \\in \\mathbb{R}^{m \\times n}$. In machine learning, a vector can be seen as a special case of a matrix.\n",
    "\n",
    "> A vector is a matrix that has only 1 column, so you have an $(m \\times 1)$-matrix. $m$ is the number of rows, and 1 here is the number of columns, so a matrix with just one column is a vector.\n",
    "\n",
    "array of arrays makes a matrix in python\n",
    "```python\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(X)\n",
    "print (X[0, 0]) # element at first row and first column\n",
    "print (X[-1, -1]) # element from the last row and last column \n",
    "print (X[0, :]) # first row and all columns\n",
    "print (X[:, 0]) # all rows and first column \n",
    "print (X[:]) # all rows and all columns\n",
    "```\n",
    "\n",
    "We can also define matlab styles matrices (for those used to matlab definitions) in the following way:\n",
    "```python\n",
    "Y = np.mat([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(Y)\n",
    "```\n",
    "\n",
    "> **Tensor** An array of numbers arranged on a regular grid with a variable number of axes.\n",
    "\n",
    "A vector is a one-dimensional or \"first order tensor\" and a matrix is a two-dimensional or \"second order tensor\".\n",
    "Tensor notation is just like matrix notation, with a capital letter that represents a tensor, and lowercase letters with a subscript representing scalar values within the tensor. Many operations that can be performed with scalars, vectors, and matrices can be reformulated to be performed with tensors as well. The image below shows some of these operations for a  3D tensor. \n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/new_tensors.png\" width=\"700\">\n",
    "\n",
    "As a tool, tensors and tensor algebra are widely used in the fields of physics and engineering, and in data science it is particularly useful when you'll learn about deep learning models. \n",
    "We'll revisit tensors and relevant operations in the deep learning sections and explain how tensors are created, manipulated, and saved using more advanced analytical tools like Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/new_vector.png\" width=\"150\">\n",
    "\n",
    "Neural networks frequently process weights and inputs of different sizes where the dimensions do not meet the requirements of matrix multiplication. Matrix transpose provides a way to \"rotate\" one of the matrices so that the operation complies with multiplication requirements and can continue. There are two steps to transpose a matrix:\n",
    "\n",
    "* Rotate the matrix right 90° clockwise.\n",
    "* Reverse the order of elements in each row (e.g. [a b c] becomes [c b a]).\n",
    "This can be better understood looking at this image :\n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/new_matrix.png\" width=\"350\">\n",
    "\n",
    "Numpy provides the transpose operation by using the `.T` attribute or the `np.transpose()` function with the array that needs to be transposed as shown below:\n",
    "\n",
    "```python\n",
    "# create a transpose of a matrix\n",
    "\n",
    "A = np.array([\n",
    "   [1, 2, 3], \n",
    "   [4, 5, 6],\n",
    "   [7, 8, 9]])\n",
    "\n",
    "A_transposed = A.T\n",
    "A_transposed_2 = np.transpose(A)\n",
    "\n",
    "print(A,'\\n\\n', A_transposed, '\\n\\n', A_transposed_2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "- Scalars = 0D tensors\n",
    "- Vectors = 1D tensors\n",
    "- Matrices = 2D tensors\n",
    "- 3D tensors\n",
    "\n",
    "A tensor is defined by three key attributes:\n",
    "- rank or number of axes\n",
    "- the shape\n",
    "- the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important data manipulations in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrowing matrices (important for images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eg Santa: `(790, 64, 64, 3)` matrix to a `(64*64*3, 790)` matrix!\n",
    "\n",
    "```python \n",
    "img_unrow = img.reshape(790, -1).T  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector with `np.shape() (790,)`\n",
    "\n",
    "```python\n",
    "np.reshape(vector, (1,790)) \n",
    "```\n",
    "\n",
    "```python  \n",
    "tensor[start_idx : end_idx]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "* **Element-wise**: updates each element with the corresponding element from another tensor\n",
    "```python\n",
    "np.array([1, 2, 3, 4]) + np.array([5, 6, 7, 8])\n",
    "#result\n",
    "array([ 6,  8, 10, 12])\n",
    "```\n",
    "\n",
    "* **Broadcasting**: When tensors are different shapes it can broadcast, B is added to every row here for example.\n",
    "```python\n",
    "A += B\n",
    "A:\n",
    " [[ 0  1  2]\n",
    " [ 3  4  5]\n",
    " [ 6  7  8]\n",
    " [ 9 10 11]] \n",
    "\n",
    "B: [1 2 3] \n",
    "\n",
    "Updated A:\n",
    " [[ 1  3  5]\n",
    " [ 4  6  8]\n",
    " [ 7  9 11]\n",
    " [10 12 14]]\n",
    " ```\n",
    "* **Tensor dot**: sum of element products following matrix rules\n",
    "```python\n",
    "# Recall that B is the vector [1, 2, 3]\n",
    "# Taking the dot product of B and itself is equivalent to\n",
    "# 1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
    "print(np.dot(B,B))\n",
    "\n",
    "A:\n",
    " [[ 0  1  2]\n",
    " [ 3  4  5]\n",
    " [ 6  7  8]\n",
    " [ 9 10 11]] \n",
    "\n",
    "B: [1 2 3] \n",
    "\n",
    "array([ 8, 26, 44, 62])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first element is the sum of the first row of $A$ multiplied by $B$ elementwise:  \n",
    "$$ 0*1 + 1*2 + 2*3 = 0 + 2 + 6 = 8 $$ \n",
    "\n",
    "Followed by the sum of the second row of $A$ multiplied by $B$ elementwise:  \n",
    "\n",
    "$$ 3*1 + 4*2 + 5*3 = 3 + 8 + 15 = 26 $$\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dotproduct.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You continue this for every row x column.  Also notice that the width of A has to be the height of B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Sequential Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "#Dense means this layer is fully connected\n",
    "#input_shape is optional and the next layer added is based on the shape of the prior\n",
    "model.add(layers.Dense(units, activation, input_shape))\n",
    "#notice the loss function\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Training Terms\n",
    "\n",
    "**Sample**: A single element of a dataset.\n",
    "  * In a convolutional network, an image is a sample.\n",
    "  * In a speech recognition model, an audio file is a sample.\n",
    "\n",
    "**Batch**: A set of *N* samples processed independently, but in parallel.\n",
    "  * **Training**: Processing a batch results in only one update to the model.\n",
    "  * **Approximation**: Batches better approximate the distribution of input data than single samples. Larger batches provide a more accurate approximation.\n",
    "  * **Inference**: Use the largest batch size your system can manage without memory issues for faster evaluation/prediction.\n",
    "\n",
    "**Epoch**: One full pass over the entire dataset.\n",
    "  * **Phases**: Used to segment training into distinct phases for easier logging and evaluation.\n",
    "  * **Validation**: If using `validation_data` or `validation_split` during training with Keras, evaluations are performed at the end of each epoch.\n",
    "  * **Callbacks**: Functions like adjusting learning rates or saving model checkpoints can be scheduled to run at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Fit\n",
    "`history.history` - history on how the model was trained\n",
    "- `history.history['loss']` loss values per epoch \n",
    "- `history.history['accuracy']` accuracy values per epoch\n",
    "\n",
    "## Predict\n",
    "```python\n",
    "y_hat = model.predict(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Similarly, we can use the `.evaluate()` method in order to compute the loss and other specified metrics for our trained model.\n",
    "\n",
    "For example,   \n",
    "\n",
    "```python\n",
    "model.evaluate(X_train, X_train_labels)\n",
    "``` \n",
    "\n",
    "will return the final loss associated with the model for the training data as well as any other metrics that were specified during compilation.\n",
    "\n",
    "Similarly, \n",
    "\n",
    "```python\n",
    "model.evaluate(X_test, X_test_labels)\n",
    "``` \n",
    "will return the final loss associated with the model for the test data as well as any other specified metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resource\n",
    "    \n",
    "* A full book on Keras by the author of Keras himself:  \n",
    "https://www.manning.com/books/deep-learning-with-python-second-edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Recap\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "The key takeaways from this section include:\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "* Neural networks are powerful models that can be customized and tweaked using various amounts of nodes, layers, etc.\n",
    "* The most basic neural networks are single-layer densely connected neural networks, which have very similar properties as logistic regression models\n",
    "* Compared to more traditional statistics and ML techniques, neural networks perform particularly well when using unstructured data\n",
    "* Apart from densely connected networks, other types of neural networks include convolutional neural networks, recurrent neural networks, and generative adversarial neural networks \n",
    "* When working with image data, it's important to understand how image data is stored when working with them in Python\n",
    "* Logistic regression can be seen as a single-layer neural network with a sigmoid activation function\n",
    "* Neural networks use loss and cost functions to minimize the \"loss\", which is a function that summarizes the difference between the actual outcome (eg. pictures contain Santa or not) and the model prediction (whether the model correctly identifies pictures with Santa)\n",
    "* Backward and forward propagation are used to estimate the so-called \"model weights\"\n",
    "* Adding more layers to neural networks can substantially increase model performance\n",
    "* Several activations can be used in model nodes, you can explore with different types and evaluate how it affects performance\n",
    "\n",
    "### Deep Neural Networks\n",
    "\n",
    "* Deep neural network representations can lighten the burden and automate certain tasks of heavy data preprocessing\n",
    "* Deep representations need exponentially fewer hidden units than shallow networks, to obtain the same performance\n",
    "* Parameter initialization, forward propagation, cost function evaluation, and backward propagation are again the cornerstones of deep networks\n",
    "* Tensors are the building blocks of neural networks and a good understanding of them and how to use them in Python is crucial\n",
    "* Scalars can be seen as 0-D tensors. Vectors can be seen as 1-D tensors, and matrices as 2-D tensors\n",
    "* The usage of tensors reaches beyond matrices: tensors can have N dimensions\n",
    "* Tensors can be created and manipulated using NumPy\n",
    "* Keras makes building neural networks in Python easy, and you learned how to do that in this section\n",
    "* You can use Keras to do some NLP as well, e.g. for tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Neural Network Architecture\n",
    "\n",
    "In this lecture, we'll explore the architecture of neural networks. On Monday, we will start working with TensorFlow to create neural networks.\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "Neural networks mimic the structure of the human brain and are computational graphs connecting inputs to outputs through layers, including hidden layers. Each connection between layers has associated weights and biases, and uses activation functions to learn relationships.\n",
    "\n",
    "### Layers and Complexity\n",
    "\n",
    "- **Input Layer:** Represents features or columns of the dataset.\n",
    "- **Hidden Layers:** Intermediate layers that learn from the input features.\n",
    "- **Output Layer:** Provides the final prediction or classification.\n",
    "\n",
    "Each neuron in a hidden layer represents a unique equation and processes inputs to generate outputs for the next layer.\n",
    "\n",
    "### Example: Cat vs. Dog Classification\n",
    "\n",
    "Inputs (features) are processed through multiple layers to produce an output (e.g., identifying if an image is a cat or a dog).\n",
    "\n",
    "## Understanding Neural Network Training\n",
    "\n",
    "### Epoch and Batch Sizes\n",
    "\n",
    "- **Epoch:** One full pass through the entire dataset.\n",
    "- **Batch Size:** The number of samples processed before the model's internal parameters are updated.\n",
    "\n",
    "Each epoch consists of multiple batches, and after each batch, the model's weights are updated.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns.\n",
    "\n",
    "- **Sigmoid:** Often used in the output layer for binary classification.\n",
    "- **Tanh:** Scaled sigmoid function ranging from -1 to 1.\n",
    "- **ReLU (Rectified Linear Unit):** Outputs the input if it's positive; otherwise, it outputs zero. Most popular due to its simplicity and effectiveness.\n",
    "\n",
    "### Regularization and Learning Rate\n",
    "\n",
    "- **Regularization:** Techniques like L1 and L2 regularization prevent overfitting by adding penalties to the loss function.\n",
    "- **Learning Rate:** Determines the step size during gradient descent optimization.\n",
    "\n",
    "## Building a Neural Network\n",
    "\n",
    "### Weights and Biases\n",
    "\n",
    "- Weights are initialized randomly and adjusted through training.\n",
    "- Bias terms are added to the weighted sum of inputs to create the neuron's output.\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Inputs are passed through the network layer by layer. Each layer applies weights, sums the inputs, adds biases, and uses activation functions to produce outputs for the next layer.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "After forward propagation, the error is calculated using a loss function. Backpropagation adjusts the weights by calculating gradients and updating the weights in the direction that reduces the error.\n",
    "\n",
    "## Example Code: Forward Propagation and Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.random.rand(64)\n",
    "bias = np.random.rand(1)\n",
    "\n",
    "# Input features\n",
    "inputs = np.array([0.5, 0.3, ..., 0.9])  # Example values\n",
    "\n",
    "# Weighted sum\n",
    "z = np.dot(weights, inputs) + bias\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Output after activation\n",
    "output = sigmoid(z)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Image Classification\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Load Image Data:** Flatten images into vectors.\n",
    "2. **Initialize Weights and Biases:** Randomly initialize for each neuron.\n",
    "3. **Forward Propagation:** Calculate weighted sums and apply activation functions.\n",
    "4. **Calculate Error:** Compare predicted outputs with actual labels.\n",
    "5. **Backpropagation:** Adjust weights based on error gradients.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Neural networks are powerful tools for learning complex patterns and making predictions. They involve intricate processes of forward and backward propagation, weight adjustment, and the use of activation functions. While their workings can seem like a black box, understanding their architecture and training process is crucial for effectively using them in machine learning tasks.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next class, we will cover vectorization and classification. Ensure you are familiar with the concepts discussed today. If you have any questions, please feel free to ask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Partial Derivatives\n",
    "Here’s a summary of each partial derivative:\n",
    "\n",
    "1. $$\\frac{\\partial C}{\\partial w_1} = -2 w_5 X_1 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "2. $$\\frac{\\partial C}{\\partial w_2} = -2 w_6 X_1 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "3. $$\\frac{\\partial C}{\\partial w_3} = -2 w_5 X_2 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "4. $$\\frac{\\partial C}{\\partial w_4} = -2 w_6 X_2 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "5. $$\\frac{\\partial C}{\\partial w_5} = -2 (w_1 X_1 + w_3 X_2) \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "6. $$\\frac{\\partial C}{\\partial w_6} = -2 (w_2 X_1 + w_4 X_2) \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "\n",
    "In these equations:\n",
    "- $y$ represents the actual target value.\n",
    "- $X_1$ and $X_2$ are input features.\n",
    "- $w_i$ are the weights of the neural network.\n",
    "- The summation $\\sum$ indicates summing over all training examples.\n",
    "\n",
    "Each equation represents how the cost function $C$ changes with respect to a particular weight $w_i$. These derivatives are used in the backpropagation algorithm to adjust the weights iteratively, reducing the overall cost and improving the model's predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to go the opposite direction of these partial derivatives in order to go down the gradient slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Notation \n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_small_deeper.png' width='700'>\n",
    "\n",
    "For our 2-layer neural network above\n",
    "\n",
    "- $x = a^{[0]}$  as x is what comes out of the input layer\n",
    "- $a^{[1]} = \\begin{bmatrix} a^{[1]}_1  \\\\ a^{[1]}_2 \\\\ a^{[1]}_3  \\\\\\end{bmatrix}$ is the value generated by the hidden layer\n",
    "- $\\hat y =  a^{[2]}$, the output layer will generate a value $a^{[2]}$, which is equal to $\\hat y$ \n",
    "\n",
    "\n",
    "Note that the input layer is not counted or is index 0, so if you say an n-layer network be sure to not count the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "You won't use any of these, you'll use the ones in libraries, but it's a nice show of how they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    if (derivative == True):\n",
    "        return f * (1 - f)\n",
    "    return f\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    f = np.tanh(x)\n",
    "    if (derivative == True):\n",
    "        return (1 - (f ** 2))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = 0\n",
    "    return f\n",
    "\n",
    "def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = leakage\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = x[i]* leakage\n",
    "    return f\n",
    "\n",
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return 1/(1+np.square(x))\n",
    "    return np.arctan(x)\n",
    "\n",
    "z = np.arange(-10, 10, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hyperbolic tangent (tanh) function \n",
    "\n",
    "The hyperbolic tangent (or tanh) function goes between -1 and +1, and is in fact a shifted version of the sigmoid function, with formula $ a=\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$. For intermediate layers, the tanh function generally performs pretty well because, with values between -1 and +1, the means of the activations coming out are closer to zero! \n",
    "\n",
    "tanh is a shifted sigmoid that goes between -1 and 1, recall sigmoid goes between 0 and 1.\n",
    "\n",
    "A disadvantage of both tanh and sigmoid activation functions is that when $z$ gets quite large or small, the derivative of the slopes of these functions become very small, generally 0.0001. This will slow down gradient descent. You can see in the tanh plot that this already starts happening for values of $z > 2$ or $z < 2$. The next few activation functions will try to overcome this issue. \n",
    "<img src=\"images/tanh.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arctan (inverse tangent)\n",
    "\n",
    "The inverse tangent (arctan) function has a lot of the same qualities that tanh has, but the range roughly goes from -1.6 to 1.6, and  the slope is more gentle than the one we saw using the tanh function.\n",
    "\n",
    "<img src=\"images/arctan.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear Unit function (ReLu)\n",
    "\n",
    "This is probably the most popular activation function, along with the tanh! The fact that the activation is exactly 0 when $z <0$  is slightly cumbersome when taking derivatives though. \n",
    "\n",
    "$$a=\\max(0, z)$$\n",
    "\n",
    "<img src=\"images/relu.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The leaky Rectified Linear Unit function \n",
    "\n",
    "The leaky ReLU solves the derivative issue by allowing for the activation to be slightly negative when $z <0$. \n",
    "\n",
    "$$a=\\max(0.001*z ,z)$$\n",
    "\n",
    "<img src=\"images/leaky_relu.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources \n",
    "\n",
    "- [Visualising activation functions in neural networks](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Normalization\n",
    "\n",
    "## Normalized Inputs: Speed Up Training\n",
    "\n",
    "Normalizing inputs (scaling features to a consistent range, e.g., 0 to 1) speeds up training and promotes convergence. Standardization involves subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "### Vanishing or Exploding Gradients\n",
    "\n",
    "Normalization helps mitigate numerical problems in gradient computation, preventing vanishing or exploding gradients. For deep networks, gradients can grow or shrink excessively, making training unstable.\n",
    "\n",
    "Example:\n",
    "- For a deep network with linear activations, the weight products can explode due to many layers. Normalization helps manage this risk.\n",
    "\n",
    "#### Solutions to Gradient Issues\n",
    "\n",
    "- **Initialization**: Small weights can prevent gradient issues. Common practices:\n",
    "  - **Variance Rule**: $ \\text{Var}(w_i) = \\frac{1}{n} $ or $ \\text{Var}(w_i) = \\frac{2}{n} $\n",
    "  - **ReLU Initialization**: $ w^{[l]} = \\text{np.random.randn(shape)} \\times \\sqrt{2/n_{l-1}} $\n",
    "\n",
    "## Optimization Strategies\n",
    "\n",
    "### Gradient Descent with Momentum\n",
    "\n",
    "- **Purpose**: Reduces oscillations and improves convergence.\n",
    "- **How**:\n",
    "  - Calculate moving averages for gradients.\n",
    "  - Update weights with averaged gradients.\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "- **Purpose**: Adapts learning rates for each parameter.\n",
    "- **How**:\n",
    "  - Use exponentially weighted average of squared gradients.\n",
    "  - Adjust learning rate based on average squared gradients.\n",
    "\n",
    "### Adam Optimization\n",
    "\n",
    "- **Purpose**: Combines momentum and RMSprop benefits.\n",
    "- **How**:\n",
    "  - Maintain moving averages for gradients and their squares.\n",
    "  - Apply bias corrections.\n",
    "  - Update weights using corrected averages.\n",
    "\n",
    "### Learning Rate Decay\n",
    "\n",
    "- **Purpose**: Gradually reduce the learning rate over epochs.\n",
    "- **Methods**:\n",
    "  - **Inverse Time Decay**: $ \\alpha = \\frac{1}{1 + \\text{decay rate} \\times \\text{epoch}} \\times \\alpha_0 $\n",
    "  - **Exponential Decay**: $ \\alpha = 0.97^{\\text{epoch}} \\times \\alpha_0 $\n",
    "  - **Manual Decay**\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "- **Important**: Learning rate ($ \\alpha $).\n",
    "- **Next**: Momentum ($ \\beta $), number of hidden units, mini-batch size.\n",
    "- **Less Critical**: Number of layers, learning rate decay.\n",
    "- **Rarely Tuned**: $ \\beta_1 $, $ \\beta_2 $, $ \\epsilon $ (for Adam).\n",
    "\n",
    "Avoid grid search; use iterative testing for hyperparameter tuning.\n",
    "\n",
    "## Additional Resources\n",
    "- [Coursera: Normalizing Inputs](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs)\n",
    "- [Coursera: Gradient Descent with Momentum](https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization\n",
    "\n",
    "## Key Hyperparameters\n",
    "\n",
    "When tuning neural networks, focus on:\n",
    "\n",
    "- **Number of Hidden Units**: Controls model capacity.\n",
    "- **Number of Layers**: Affects model depth.\n",
    "- **Learning Rate ($\\alpha$)**: Determines step size in optimization.\n",
    "- **Activation Function**: Transforms node inputs.\n",
    "\n",
    "Use a **validation set** to balance accuracy and generalization.\n",
    "\n",
    "## Data Splits\n",
    "\n",
    "Divide your data into:\n",
    "\n",
    "- **Training Set**: For training the model.\n",
    "- **Validation Set**: To tune hyperparameters and select the final model.\n",
    "- **Test Set**: To evaluate performance on unseen data.\n",
    "\n",
    "Ensure all sets come from the same distribution (e.g., same image resolution).\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "Balance **bias** (error from too-simple models) and **variance** (error from too-complex models).\n",
    "\n",
    "### The Circles Example\n",
    "\n",
    "Examine the bias-variance trade-off with concentric circles:\n",
    "\n",
    "- **High Bias (Underfitting)**: Model is too simple, missing key patterns.\n",
    "  ![Underfitting](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/underfitting.png)\n",
    "- **Good Fit**: Model accurately captures underlying patterns.\n",
    "  ![Good Fit](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/good.png)\n",
    "- **High Variance (Overfitting)**: Model is too complex, capturing noise.\n",
    "  ![Overfitting](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/overfitting.png)\n",
    "\n",
    "## The Santa Example\n",
    "\n",
    "Performance across different models:\n",
    "\n",
    "|       | High Variance | High Bias | High Variance & Bias | Low Variance and Bias |\n",
    "|-------|---------------|-----------|----------------------|-----------------------|\n",
    "| Train Set Error | 12% | 26% | 26% | 12% |\n",
    "| Validation Set Error | 25% | 28% | 40% | 13% |\n",
    "\n",
    "A model with low variance and bias performs best (87% accuracy).\n",
    "\n",
    "## Bias / Variance Tips\n",
    "\n",
    "| High Bias? (Training Error) | High Variance? (Validation Error) |\n",
    "|------------------------------|-----------------------------------|\n",
    "| Increase network size | Gather more data |\n",
    "| Train longer | Apply regularization |\n",
    "| Try different architectures | Try different architectures |\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Prevents overfitting by penalizing large weights.\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "\n",
    "#### In Logistic Regression\n",
    "\n",
    "For L2 regularization:\n",
    "\n",
    "$$ J(w, b) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m} ||w||_2^2 $$\n",
    "\n",
    "- **L2 Regularization**: Penalizes large weights to simplify the model.\n",
    "- **L1 Regularization**: Adds a term $\\dfrac{\\lambda}{m} ||w||_1$ for sparsity.\n",
    "\n",
    "#### In Neural Networks\n",
    "\n",
    "For L2 regularization across all layers:\n",
    "\n",
    "$$ J(w^{[1]}, b^{[1]}, \\ldots, w^{[L]}, b^{[L]}) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m} \\sum_{l=1}^L ||w^{[l]}||_2^2 $$\n",
    "\n",
    "**Update Rule**:\n",
    "\n",
    "$$ w^{[l]} := w^{[l]} - \\alpha \\left( \\text{[backpropagation]} + \\dfrac{\\lambda}{m} w^{[l]} \\right) $$\n",
    "\n",
    "### Dropout Regularization\n",
    "\n",
    "Dropout randomly ignores nodes during training, reducing overfitting.\n",
    "\n",
    "**Before Dropout**:\n",
    "\n",
    "![Standard Neural Net](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/dropout.png)\n",
    "\n",
    "**After Dropout**:\n",
    "\n",
    "![Neural Net with Dropout](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/dropout.png)\n",
    "\n",
    "In Keras, use the `Dropout` layer:\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(500,)))\n",
    "model.add(layers.Dropout(0.3))  # Dropout applied here\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))  # Dropout applied here\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interpretability - Introduction\n",
    "\n",
    "## What is Interpretability?\n",
    "\n",
    "Interpretability in machine learning refers to how well we can understand the decisions made by our models. There is no single definition or consensus, but it's crucial to align interpretability with the goals of your machine learning project.\n",
    "\n",
    "\n",
    "## Interpretability and Machine Learning Goals\n",
    "\n",
    "### Machine Learning Goals\n",
    "\n",
    "- **Support Human Decisions**: Provides insights to aid human decision-making. For example, Clinical Decision Support (CDS) systems like [Watson Health's Micromedex](https://www.ibm.com/watson-health/solutions/clinical-decision-support) help clinicians by offering advice based on patient data and medical knowledge.\n",
    "\n",
    "    [Watson Health Micromedex](https://www.ibm.com/watson-health/solutions/clinical-decision-support)\n",
    "\n",
    "- **Automate Human Decisions**: Trains models to make decisions independently, such as Natural Language Generation applications used in predictive text and chatbots like ChatGPT.\n",
    "\n",
    "### The Cost and Benefits of Decision Support\n",
    "\n",
    "#### Clinical Decision Support (CDS)\n",
    "\n",
    "CDS systems assist clinicians by providing actionable insights from medical data, which can speed up patient care but also introduce potential for false positives. \n",
    "\n",
    "![Clinical Decision Support](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/clinical-decision-support.png)\n",
    "\n",
    "> **Goal**: Provide reliable recommendations to clinicians. Effectiveness depends on the relevance and accuracy of the information provided.\n",
    "\n",
    "#### Computer Aided Detection (CAD)\n",
    "\n",
    "CAD systems enhance diagnostic imaging, such as detecting breast cancer. They offer a \"second opinion\" but may introduce false positives, impacting patient care and resource allocation.\n",
    "\n",
    "> **Goal**: Improve early detection of conditions like cancer. Balancing sensitivity with false positives is crucial.\n",
    "\n",
    "### How Interpretability Enhances Decision Support\n",
    "\n",
    "Interpretability can improve decision support in several ways:\n",
    "\n",
    "1. **Trust**: Metrics like accuracy and ROC/AUC show how well the model performs and the nature of its errors.\n",
    "\n",
    "2. **Causality**: Helps in understanding relationships between variables, guiding hypotheses and interventions.\n",
    "\n",
    "3. **Transferability**: Assesses whether a model’s performance is consistent in new scenarios. Understanding decision drivers aids in model tuning and debugging.\n",
    "\n",
    "4. **Informativeness**: Provides insights into feature importance, enhancing domain knowledge.\n",
    "\n",
    "5. **Fair and Ethical Decision Making**: Ensures algorithms do not perpetuate societal biases, fostering accountability.\n",
    "\n",
    "## White Box vs. Black Box Models\n",
    "\n",
    "Historically, simpler models like regression and decision trees were more interpretable due to their transparency. However, modern complex models, especially neural networks, are often seen as \"black boxes\" because their decision-making processes are less transparent.\n",
    "\n",
    "- **White Box Models**: Transparent and interpretable, such as regression and decision trees.\n",
    "- **Black Box Models**: Complex and less interpretable, such as neural networks. \n",
    "\n",
    "Despite their complexity, black box models can be analyzed using various interpretability techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interpretability - White-Box Models\n",
    "\n",
    "## Definitions\n",
    "- **White-box Models**: Models that are interpretable with minimal investigation, e.g., regression, decision trees.\n",
    "- **Black-box Models**: Models that are not easily interpretable, e.g., neural networks.\n",
    "- **Intrinsic Interpretability**: Understanding how a model arrived at a prediction directly from the model's structure.\n",
    "- **Post-Hoc Interpretation**: Analyzing a model's predictions after training using additional methods.\n",
    "\n",
    "## Model Selection and Common White-Box Models\n",
    "\n",
    "### Linear Regression\n",
    "Predicts a value based on well-understood variables, e.g., home price based on square footage.\n",
    "\n",
    "### Logistic Regression\n",
    "Classifies data into categories, e.g., determining if a home is a McMansion based on description.\n",
    "\n",
    "### Naive Bayes\n",
    "Analyzes unstructured data for classification, e.g., identifying common features of McMansions from descriptions.\n",
    "\n",
    "### Decision Trees\n",
    "Classifies based on important features, e.g., deciding whether to buy a McMansion based on listed characteristics.\n",
    "\n",
    "## Types of Interpretation\n",
    "\n",
    "### Intrinsic\n",
    "- **Definition**: Directly interpretable due to model simplicity.\n",
    "- **Examples**: Linear regression, logistic regression, simple decision trees.\n",
    "\n",
    "### Post-Hoc\n",
    "- **Definition**: Interpretation methods applied after model training.\n",
    "- **Examples**: Permutation feature importance, visualizations, reading model internals.\n",
    "\n",
    "## Methods of Interpretation\n",
    "\n",
    "### Model-Specific\n",
    "- **Definition**: Methods unique to specific models, e.g., regression weights for regression models.\n",
    "\n",
    "### Model-Agnostic\n",
    "- **Definition**: Methods applicable to any model based on input/output analysis.\n",
    "\n",
    "### Scope of Interpretation\n",
    "- **Local**: Explains individual predictions.\n",
    "- **Global**: Explains overall model behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interpretability - Black Box Models\n",
    "\n",
    "## Common Black-Box Models\n",
    "\n",
    "### Gradient Boosted Trees (GBDT)\n",
    "- **Definition**: An ensemble method combining multiple decision trees for improved accuracy.\n",
    "- **How It Works**: Boosting enhances weak models incrementally, using gradient descent to minimize errors.\n",
    "- **Applications**:\n",
    "  - **Fraud Detection**: Identifies fraudulent transactions.\n",
    "  - **Medical Outcomes**: Predicts disease likelihood or treatment effectiveness.\n",
    "  - **Recommender Systems**: Suggests products or content based on user preferences.\n",
    "  - **Computer Vision**: Recognizes objects in images and videos.\n",
    "  - **Customer Churn**: Predicts which customers may leave.\n",
    "\n",
    "### Neural Networks\n",
    "- **Definition**: Mimics biological neurons to process data through layers (input, hidden, output).\n",
    "- **How They Work**: Nodes activate based on weights and thresholds, passing data through layers.\n",
    "- **Applications**:\n",
    "  - **Medical Imaging**: Analyzes X-rays, CT scans, and MRIs.\n",
    "  - **Drug Research**: Predicts drug effectiveness and side effects.\n",
    "  - **Patient Outcomes**: Predicts disease progression and survival rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases for Neural Networks\n",
    "\n",
    "Neural networks have significant applications in the medical field, including:\n",
    "\n",
    "- **Medical Imaging**: Analyzing X-rays, CT scans, and MRIs to identify features or abnormalities, aiding in the diagnosis of diseases such as cancer, heart disease, and neurological disorders.\n",
    "\n",
    "- **Drug Research and Development**: Analyzing data from chemical compounds to predict the effectiveness and side effects of new drugs, helping pharmaceutical companies identify promising drug candidates faster.\n",
    "\n",
    "- **Patient Outcomes**: Predicting risks such as readmission, disease progression, and survival rates using a range of patient data (genetic, demographic, clinical), assisting doctors in making informed decisions about patient care.\n",
    "\n",
    "Neural networks improve medical diagnosis and treatment accuracy by efficiently analyzing and learning from large data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to clipboard_image.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageGrab\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "def save_clipboard_image(file_path):\n",
    "    try:\n",
    "        # Grab the image from the clipboard\n",
    "        image = ImageGrab.grabclipboard()\n",
    "        if isinstance(image, Image.Image):\n",
    "            # Save the image to the specified file path\n",
    "            image.save(file_path)\n",
    "            print(f\"Image saved to {file_path}\")\n",
    "            return file_path\n",
    "        else:\n",
    "            print(\"No image found in clipboard.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save image from clipboard\n",
    "file_path = save_clipboard_image(\"clipboard_image.png\")\n",
    "\n",
    "# Display the saved image if it exists\n",
    "if file_path:\n",
    "    IPImage(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
