{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is an effective classification and regression algorithm that uses nearby points to generate a prediction.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Choosing a Point**\n",
    "2. **Finding the K-nearest Points**\n",
    "   - K is a predefined user constant such as 1, 3, 5, or 11.\n",
    "3. **Predicting a Label for the Current Point**\n",
    "   - **Classification:** Take the most common class of the K neighbors.\n",
    "   - **Regression:** Take the average target metric of the K neighbors.\n",
    "   - Both can use weighted averages based on the distance of the neighbors.\n",
    "4. **No Traditional Training Phase**\n",
    "   - KNN does not involve model fitting; it stores the training data and calculates distances during prediction.\n",
    "5. **Efficiency**\n",
    "   - Efficient on small to mid-sized data but not suitable for large datasets.\n",
    "\n",
    "### Assumptions of Distance-Based Classifiers\n",
    "- Distance helps quantify similarity.\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "#### Manhattan Distance\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/manhattan_fs.png\" alt=\"Manhattan Distance\" width=300>\n",
    "\n",
    "$$ \\large d(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "manhattan_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the absolute difference and add it\n",
    "    manhattan_distance += abs(A[i] - B[i])\n",
    "\n",
    "manhattan_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/euclidean_fs.png\" alt=\"Euclidean Distance\" width=300>\n",
    "\n",
    "$$ \\large d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Locations of two points A and B\n",
    "A = (1, 7, 12)\n",
    "B = (-1, 0, -5)\n",
    "\n",
    "euclidean_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the difference, square, and add it\n",
    "    euclidean_distance += (A[i] - B[i]) ** 2\n",
    "\n",
    "# Square root of the final result\n",
    "euclidean_distance = sqrt(euclidean_distance)\n",
    "\n",
    "euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minkowski Distance\n",
    "Both Manhattan and Euclidean distances are special cases of Minkowski distance.\n",
    "\n",
    "$$\\large d(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$$\n",
    "\n",
    "### Hamming Distance\n",
    "Hamming distance can even be used to compare strings.\n",
    "\n",
    "### Level Up: Distance Metrics\n",
    "> The \"closeness\" of data points → proxy for similarity\n",
    "\n",
    "<img src=\"./images/distance.png\" alt=\"Distance\" width=\"800\">\n",
    "\n",
    "### How Adjusting K Works\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/fit_fs.png\" alt=\"Fit\" width=600>\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/best_k_fs.png\" alt=\"Best K\" width=600>\n",
    "\n",
    "### Computational Complexity\n",
    "- KNN isn't the best choice for extremely large datasets or high-dimensional models due to its exponential time complexity.\n",
    "\n",
    "### Best Value for K\n",
    "- Determined through testing on a dataset and cross-validation.\n",
    "\n",
    "### Practical Application of KNN\n",
    "\n",
    "- **Pick K** for low bias and low variance.\n",
    "- **Fitting** doesn't train; it just stores the locations in the feature space.\n",
    "- **Hyper tuning** the number of neighbors.\n",
    "- **Low K** leads to overfitting; **High K** leads to underfitting.\n",
    "- **Scale the features**!\n",
    "- Use **K-folds, GridSearchCV**, etc., and standardize after splitting.\n",
    "- **Cross-validation** helps find the best score.\n",
    "- **Lower K** that predicts better is usually better.\n",
    "- **Weighted averages:** multiply support by.\n",
    "- **Hidden dimensions:** latent space.\n",
    "- Predicting about generalizing well.\n",
    "- **Lazy algorithm:** works well with smaller datasets.\n",
    "  - Over 100K samples, it starts to be too big.\n",
    "  - More columns matter too.\n",
    "- Alternative to OHE? Encode one column with all the values.\n",
    "- More features = more dimensions = more sparsity.\n",
    "  - Makes it harder to train or predict and can overfit.\n",
    "  - Volume scales exponentially.\n",
    "  - Affects all algorithms.\n",
    "  - More columns can capture variance but can be overdone.\n",
    "- **Feature spaces:**\n",
    "  - Cosine used for recommendations.\n",
    "  - Hamming used in NLP for the distance between words.\n",
    "\n",
    "#### Unscaled\n",
    "<img src=\"images/nonnormal.png\" alt=\"Unscaled\" width=600>\n",
    "\n",
    "#### Scaled\n",
    "<img src=\"images/normalized.png\" alt=\"Scaled\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_transform = LabelEncoder()\n",
    "iris_df['Species'] = target_transform.fit_transform(iris_df['Species'])\n",
    "\n",
    "# Label Encoder - takes categorical data like dog, cat, fish, etc., and turns them into numerical values like 0, 1, 2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = iris_df[['SepalWidthCm', 'PetalWidthCm']]\n",
    "y = iris_df['Species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.15, random_state=42)\n",
    "fold_index = KFold(n_splits=5).split(X_train)\n",
    "\n",
    "next(fold_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- **Introduction to KNN:** A distance-based supervised learning algorithm, not to be confused with K-means clustering.\n",
    "- **Key Concepts:** Distance metrics (Euclidean, Manhattan, Minkowski), hyperparameter tuning, labeled data.\n",
    "- **Practical Steps:** Choosing K, using distance metrics, algorithm steps, no training phase.\n",
    "- **Tuning K:** Low K = overfitting, high K = underfitting, optimal K via cross-validation.\n",
    "- **Considerations:** Scaling features, avoiding ties, computational complexity, dimensionality reduction.\n",
    "- **Applications:** Small to mid-sized datasets, not ideal for large or high-dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Concepts\n",
    "\n",
    "- **Lazy Algorithm:** KNN does not involve explicit training.\n",
    "- **Public Opinion Analogy:** Majority vote influences the outcome.\n",
    "- **Cosine Similarity:** Used in recommendation systems.\n",
    "- **Hamming Distance:** Used in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Clusters: Lloyd's vs Fair Lloyds\n",
    "\n",
    "- **Lloyd's Algorithm**: Traditional K-means clustering.\n",
    "- **Fair Lloyds**: Modifies clustering to ensure cost fairness across demographic groups. Slight increase in CPU usage.\n",
    "\n",
    "## GridSearchCV\n",
    "\n",
    "- **Purpose**: Combines cross-validation and hyperparameter tuning.\n",
    "- **Caution**: Can be exhaustive and time-consuming if parameters are not chosen wisely.\n",
    "\n",
    "## Pickle\n",
    "\n",
    "- **Function**: Serializes and deserializes Python objects to/from files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines\n",
    "\n",
    "- **Avoid Data Leakage**: Ensures that preprocessing steps are applied consistently across training and test datasets.\n",
    "- **Workflow Automation**: Streamlines preprocessing, model fitting, and evaluation.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('mms', MinMaxScaler()),\n",
    "    ('tree', DecisionTreeClassifier(random_state=123))\n",
    "])\n",
    "\n",
    "# Create the grid parameter\n",
    "grid = [{'tree__max_depth': [None, 2, 6, 10], \n",
    "         'tree__min_samples_split': [5, 10]}]\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "\n",
    "# Fit using grid search\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the test score\n",
    "gridsearch.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "- **Hyperparameters**: Both parametric and non-parametric models have tunable hyperparameters.\n",
    "- **Pipeline Benefits**:\n",
    "  - Simplifies K-Fold cross-validation.\n",
    "  - Ensures consistent preprocessing.\n",
    "  - Supports parallel processing.\n",
    "- **Pipeline Construction**:\n",
    "  - Accepts a list of tuples (label, transformer/estimator).\n",
    "  - Use `pipeline.fit` and `pipeline.transform` for training and transforming data.\n",
    "- **GridSearchCV**:\n",
    "  - Use `pipelinename__hyperparameter` to specify parameters.\n",
    "  - Access the best model with `.best_estimator_`.\n",
    "  - Refit on the entire training set for final predictions.\n",
    "\n",
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- **Function**: Makes predictions by evaluating the K closest data points.\n",
    "- **Classification**: Majority class of the nearest neighbors.\n",
    "- **Regression**: Average of the target values of the nearest neighbors.\n",
    "- **Impact of Scaling**: Essential to normalize distances for fair comparisons.\n",
    "- **Model Characteristics**:\n",
    "  - **Nonparametric**: Does not assume a fixed form for the function.\n",
    "  - **Lazy Learning**: No explicit training phase; stores data for distance computation.\n",
    "\n",
    "## Scaling Techniques\n",
    "\n",
    "- **Standard Scaler**: Centers data around the mean with unit variance.\n",
    "- **Normalizer**: Scales individual samples to unit norm; useful when the direction is more important than the magnitude.\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "- **Purpose**: Integrates preprocessing and model training into a single workflow.\n",
    "\n",
    "### Example: Basic Pipeline\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "### Column Transformer\n",
    "\n",
    "- **Purpose**: Apply different preprocessing steps to different subsets of features.\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "numeric_features = ['age', 'hours_per_week']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['occupation', 'sex']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "- **GridSearchCV**: Exhaustively searches through a specified parameter grid to find the best combination.\n",
    "- **RandomizedSearchCV**: Randomly samples a specified number of parameter settings from the grid, useful for large datasets or high-dimensional parameter spaces.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "## SMOTE and Pipeline\n",
    "\n",
    "- **Purpose**: Handle class imbalance by oversampling the minority class using synthetic data.\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', smote),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "## Feature Union\n",
    "\n",
    "- **Purpose**: Apply multiple transformations to the same features and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "def sin_transform(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def cos_transform(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "feature_union = FeatureUnion([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('sin', FunctionTransformer(sin_transform)),\n",
    "    ('cos', FunctionTransformer(cos_transform))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', feature_union),\n",
    "    ('classifier', Ridge())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- **Pipelines** simplify the process of applying multiple transformations and fitting models, ensuring consistency and reducing code duplication.\n",
    "- **ColumnTransformer** and **FeatureUnion** offer flexibility in preprocessing different types of features.\n",
    "- **GridSearchCV** and **RandomizedSearchCV** help in tuning hyperparameters efficiently.\n",
    "- **SMOTE** and **ImbPipeline** address class imbalance issues effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "- **Ensemble Model**: Uses more than one model to make a prediction, often aggregating results. Common in supervised learning.\n",
    "- **Resilience to Variance**: Aggregating multiple models can cancel out over and underestimates, achieving a smoothing effect.\n",
    "\n",
    "## Bootstrap Aggregation (Bagging)\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png' alt=\"flowchart of input sample being split into several bootstrap samples, then building several decision trees, then aggregation\" width=\"600\">\n",
    "\n",
    "- **Bagging**: Combines bootstrap resampling and aggregation.\n",
    "- **Bootstrap Resampling**: Estimates distribution by sampling with replacement from the dataset.\n",
    "- **Aggregation**: Combines the bootstrap samples.\n",
    "\n",
    "### Process:\n",
    "1. Sample dataset with replacement.\n",
    "2. Train a classifier on the sample.\n",
    "3. Repeat for multiple classifiers.\n",
    "4. Aggregate predictions from all classifiers.\n",
    "\n",
    "Decision Trees are commonly used due to their sensitivity to variance.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "- **Random Forest**: An ensemble of decision trees with added randomization.\n",
    "- **Process**:\n",
    "  1. Bag 2/3 of the data.\n",
    "  2. Randomly select features for each node.\n",
    "  3. Train the tree.\n",
    "  4. Use out-of-bag data for testing and error estimation.\n",
    "\n",
    "- **Benefits**:\n",
    "  - Handles large datasets efficiently.\n",
    "  - Reduces overfitting.\n",
    "  - Does not require data standardization.\n",
    "  - Resilient to overfitting by averaging multiple trees.\n",
    "- **Drawbacks**:\n",
    "  - Computationally intensive.\n",
    "  - Large memory footprint.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "## Extra Trees\n",
    "\n",
    "- Adds extra randomization in feature selection within each node.\n",
    "- Useful for further reducing overfitting.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt', bootstrap=True, random_state=42)\n",
    "et_clf.fit(X_train, y_train)\n",
    "y_pred = et_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "## Combining Models\n",
    "\n",
    "### Averaging and Weighted Averaging\n",
    "\n",
    "- **Averaging**: Combines multiple model predictions by averaging.\n",
    "- **Weighted Averaging**: Assigns different weights to model predictions based on performance.\n",
    "\n",
    "### Stacking\n",
    "\n",
    "- Uses outputs of base models as inputs for a final estimator.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stack_clf.fit(X_train, y_train)\n",
    "y_pred = stack_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "- Essential for improving model performance.\n",
    "- Common tools: `GridSearchCV`, `RandomizedSearchCV`.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 9]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Boosting\n",
    "\n",
    "## Adaptive Boosting (AdaBoost)\n",
    "\n",
    "- Uses weak learners and focuses on difficult-to-classify points by increasing their weights.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "- Sequentially fits models to correct residuals of previous models.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "- Advanced implementation of gradient boosting.\n",
    "- Handles missing values and allows custom loss functions.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "## Gradient Boosting and Weak Learners\n",
    "\n",
    "### Weak Learners\n",
    "A model that is only good at predicting slightly better than random chance\n",
    "\n",
    "1. Train a single weak learner  \n",
    "2. Figure out which examples the weak learner got wrong  \n",
    "3. Build another weak learner that focuses on the areas the first weak learner got wrong  \n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued  \n",
    "\n",
    "## Boosting vs Random Forest\n",
    "\n",
    "Very similar to random forests: ensembles of high variance models that aggregate to make a prediction. Both often use tree models, boosting can use other models though.\n",
    "\n",
    "|Boosting|Random Forest|\n",
    "|--------|-------------|\n",
    "|Iterate|Parallel|\n",
    "|Corrects on Prior Trees|Trees don't know of each other|\n",
    "|Ensemble of Weak Learners|Ensemble of Strong Learners|\n",
    "|Very Resistant To Overfitting|Resistant to Overfitting|\n",
    "|Weighted Votes|Simple Votes|\n",
    "|Weight on Trees That Solve Harder Problems|All Even Weights|\n",
    "|Aggregate Solves Easy Problems|No Interaction Like this|\n",
    "\n",
    "## Gradient Boosted Trees\n",
    "\n",
    "- Makes use of Gradient Descent\n",
    "- Uses weak learners\n",
    "- This is where it diverges from AdaBoost: It calculated the residuals next to see how far it is off\n",
    "- Residuals are combined with a loss function\n",
    "- Loss function is differentiable\n",
    "- Loss function is inflated more where the model is more wrong, thus it will be pushed towards making a model focusing on these harder problems\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_gradient-boosting.png' width=\"600\">\n",
    "\n",
    "* How does gradient boosting work for a classification problem? How do we even make sense of the notion of a gradient in that context? The short answer is that we appeal to the probabilities associated with the predictions for the various classes. See more on this topic [here](https://sefiks.com/2018/10/29/a-step-by-step-gradient-boosting-example-for-classification/). <br/> \n",
    "* Why is this called \"_gradient_ boosting\"? Because using a model's residuals to build a new model is using information about the derivative (or gradient) of that model's loss function to make improvements. See more on this topic [here](https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/).\n",
    "\n",
    "\n",
    "### Learning Rate\n",
    "$\\gamma$ -- this is the greek letter, **_gamma_** which is for learning rate\n",
    "\n",
    "Remember that too high of a learning rate is good to quickly train but won't find the best setting, and can lead to bouncing.\n",
    "\n",
    "A small learning rate will take longer to train and can get stuck in local minimums easier but will find a better value.\n",
    "\n",
    "### Algorithm\n",
    "Use mean squared error (MSE) and want to minimize that <-- done by gradient descent\n",
    "\n",
    "Use the residuals (pattern in the residuals) to create an even better model\n",
    "\n",
    "1. Fit a model to the data, $F_1(x) = y$\n",
    "2. Fit a model to the residuals, $h_1(x) = y - F_1(x)$\n",
    "3. Create a new model, $F_2(x) = F_1(x) + h_1(x)$\n",
    "4. Repeat\n",
    "\n",
    "#### Example of Iterative Steps\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "> Parts adapted from https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb\n",
    "\n",
    "## XGBoost - Extreme Gradient Boosting\n",
    "\n",
    "- Handles missing values for you\n",
    "- Runs on multiple cpu cores in parallel\n",
    "- Distributes training across computer clusters\n",
    "- Go-to competition Algorithm\n",
    "- Always use multiple algorithms but it's a top dog right now\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Ensemble Techniques**: Bagging, random forests, extra trees, averaging, and stacking enhance model robustness.\n",
    "- **Hyperparameter Tuning**: Maximizes model potential.\n",
    "- **Boosting Methods**: AdaBoost and Gradient Boosting focus on difficult cases for improved performance.\n",
    "- **Stacking**: Combines strengths of different models.\n",
    "- **Random Forest**: Efficient for large datasets and provides strong performance with interpretability.\n",
    "\n",
    "## References\n",
    "- [Random forests paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n",
    "- [Random forests website](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Pipelines simplify the process of applying multiple transformations and fitting models.\n",
    "- ColumnTransformer and FeatureUnion offer flexibility in preprocessing different types of features.\n",
    "- GridSearchCV and RandomizedSearchCV help in tuning hyperparameters efficiently.\n",
    "- SMOTE and ImbPipeline address class imbalance issues effectively.\n",
    "- Bagging, random forests, extra trees, averaging, and stacking are powerful techniques for building robust models.\n",
    "- Hyperparameter tuning and model stacking can significantly improve model performance.\n",
    "\n",
    "### Introduction\n",
    "- **Boosting Techniques Overview**\n",
    "  - Adaptive Boosting (AdaBoost)\n",
    "  - Gradient Boosting (GB)\n",
    "  - Difference: AdaBoost uses weights on misclassified data points; GB uses residuals to improve model accuracy.\n",
    "\n",
    "### Key Concepts in Recommendation Systems\n",
    "- **Cold Start Problem**: Difficulty in recommending items to new users without historical data.\n",
    "- **Implicit vs. Explicit Data**\n",
    "  - **Implicit Data**: User behaviors (e.g., click history, time spent on page).\n",
    "  - **Explicit Data**: Direct feedback (e.g., ratings).\n",
    "\n",
    "### Types of Filtering\n",
    "- **Content-Based Filtering**\n",
    "  - Recommends items similar to those the user liked in the past.\n",
    "  - Based on item attributes and user preferences.\n",
    "  - **Example**: Pandora recommends music with similar properties.\n",
    "\n",
    "- **Collaborative Filtering**\n",
    "  - Recommends items based on the preferences of similar users.\n",
    "  - **User-User Collaborative Filtering**: Finds users similar to the target user and recommends items they liked.\n",
    "  - **Item-Item Collaborative Filtering**: Recommends items that are similar to items the user liked.\n",
    "  - **Example**: Netflix recommends shows watched by users with similar viewing histories.\n",
    "\n",
    "### Memory-Based vs. Model-Based Systems\n",
    "- **Memory-Based Systems**\n",
    "  - Use the entire user-item dataset.\n",
    "  - Compute similarity scores between users/items.\n",
    "  - Metrics: Cosine Similarity, Pearson Correlation.\n",
    "\n",
    "- **Model-Based Systems**\n",
    "  - Use machine learning models to make recommendations.\n",
    "  - Example Techniques: Matrix Factorization, Alternating Least Squares (ALS).\n",
    "\n",
    "### Evaluating Recommendation Systems\n",
    "- **Metrics**\n",
    "  - Root Mean Square Error (RMSE)\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - Precision, Recall, and F1-Score for binary recommendations.\n",
    "\n",
    "### Alternating Least Squares (ALS)\n",
    "- **Concept**\n",
    "  - Factorizes the user-item matrix into two lower-dimensional matrices.\n",
    "  - Iteratively minimizes the error between predicted and actual ratings.\n",
    "  - Uses pseudo-inverse to handle non-square matrices.\n",
    "  - Helps fill in missing values in sparse datasets.\n",
    "\n",
    "### Implementation with Surprise Library\n",
    "- **Surprise Library**\n",
    "  - Specialized for building and analyzing recommender systems.\n",
    "  - Handles datasets with user, item, and rating columns.\n",
    "  - Provides various algorithms: KNNBasic, SVD, NMF.\n",
    "\n",
    "- **Example Workflow**\n",
    "  - Load dataset with user, item, and rating.\n",
    "  - Split data using Surprise’s train-test split.\n",
    "  - Instantiate and train models (KNNBasic, SVD, NMF).\n",
    "  - Evaluate model performance using RMSE and MAE.\n",
    "\n",
    "### Practical Considerations\n",
    "- **Bias and Overfitting**\n",
    "  - Beware of biases from users who rate disproportionately.\n",
    "  - Ensure model generalizes well to new, unseen data.\n",
    "\n",
    "- **Combining Techniques**\n",
    "  - Hybrid models can combine content-based and collaborative filtering.\n",
    "  - Use ensemble methods to improve recommendation accuracy.\n",
    "\n",
    "### Summary\n",
    "- Recommendation systems leverage user behavior and item attributes to suggest items.\n",
    "- Balancing content-based and collaborative filtering improves recommendations.\n",
    "- Matrix factorization techniques like ALS are powerful for handling sparse datasets.\n",
    "- Evaluating recommendation accuracy is crucial for effective recommendations.\n",
    "\n",
    "## Goal: Expose People to What They Like\n",
    "* Predicts the future preference of a set of items or user\n",
    "* Taps into the \"long tail\", there's very common items everyone buys but the long tail specific items, like a certain genre of music or special toy are long tail\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/LongTailConcept.png\" alt=\"graph showing products on the x-axis and popularity on the y-axis. a few products are very popular, labeled Head. many other products are not very popular, labeled Long Tail\" width=\"500\">\n",
    "\n",
    "## Formal Definition\n",
    "***Recommendation Systems are software agents that elicit the interests and preferences of individual consumers […] and make recommendations accordingly. They have the potential to support and improve the quality of the decisions consumers make while searching for and selecting products online.***\n",
    "\n",
    "## Applications of Recommendation Systems\n",
    "* Suggest items to a customer\n",
    "* Estimate profit & loss of many competing items and make recommendations to the customer (e.g. buying and selling stocks)\n",
    "* Recommend a product or service based on experience of the customer\n",
    "* Show offers appealing to a customer\n",
    "\n",
    "## Types of Recommendation Systems\n",
    "* Unpersonalized and Personalized\n",
    "\n",
    "### Unpersonalized\n",
    "* EX: Youtube recommending the most viewed videos.\n",
    "\n",
    "### Personalized\n",
    "__Given__: \n",
    "The profile of the \"active\" user and possibly some situational context, i.e. user browsing a product or making a purchase etc. \n",
    "\n",
    "__Required__:\n",
    "Creating a set of items, and a score for each recommendable item in that set\n",
    "\n",
    "__Profile__:\n",
    "\n",
    "User profile may contain past purchases, ratings in either implicit or explicit form, demographics and interest scores for item features \n",
    "\n",
    "> There are two ways to gather such data. The first method is to ask for explicit ratings from a user, typically on a concrete rating scale (such as rating a movie from one to five stars). The second is to gather data implicitly as the user is in the domain of the system - that is, to log the actions of a user on the site.\n",
    "Each of these techniques make use of different similarity metrics to determine how \"similar\" items are to one another. \n",
    "* [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n",
    "* [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "* [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "* [Jaccard index (useful with binary data)](https://en.wikipedia.org/wiki/Jaccard_index)\n",
    "\n",
    "### Content-Based Recommenders \n",
    "\n",
    "> __Main Idea__: If you like an item, you will also like \"similar\" items.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/content_based.png\" alt=\"content based filtering. user watches movies, then similar movies are recommended to the user\" width=\"500\">\n",
    "\n",
    "* These systems are based on the characteristics of the items themselves. \"Try other items like this\"\n",
    "* Gives the user a bit more information on why they are seeing the recommendation\n",
    "* Require manual or semi-manual tagging of products\n",
    "* Advanced systems can average all items a user liked\n",
    "\n",
    "### Collaborative Filtering Systems\n",
    "\n",
    "> __Main Idea__: If user A likes items 5, 6, 7, and 8 and user B likes items 5, 6, and 7, then it is highly likely that user B will also like item 8.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/collaborative_filtering.png\" alt=\"collaborative filtering: movies watched by both users indicate that the users are similar, then movies are recommended by one user to another user\" width=\"450\">\n",
    "\n",
    "__The key idea behind collaborative filtering is that similar users share similar interests and that users tend to like items that are similar to one another.__\n",
    "\n",
    "* Often based off user reviews\n",
    "* Have a cold start problem on how to recommend things to new users that have no activity yet.\n",
    "\n",
    "## Utility Matrix\n",
    "Represents the associated opinion that a user holds.\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           | 2          |                | 5         |\n",
    "| Lore   | 2         |            | 4              |           |\n",
    "| Mike   |           | 5          | 3              | 2         |\n",
    "| Forest | 5         |            | 1              |           |\n",
    "| Taylor | 1         | 5          |                | 2         |\n",
    "\n",
    "$r_{\\text{Mike},\\text{Little Mermaid}} = 3$.\n",
    "\n",
    "\n",
    "A recommendation system tries to fill in the blanks.  Most of the time these values are largely empty.\n",
    "The matrix above is what is known as an explicit rating.  Each person has rated what they've seen.  However we can infer or use judgement to determine how to use data for a recommendation system.\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           |  1         |                | 1         |\n",
    "| Lore   | 1         |            | 1              |           |\n",
    "| Mike   |           | 1          | 1              | 1         |\n",
    "| Forest | 1         |            | 1              |           |\n",
    "| Taylor | 1         | 1          |                | 1         |\n",
    "\n",
    "These are __implicit__ ratings because we are assuming that because a person has bought something, they would like to buy other items like it. Of course, this is not necessarily true, but it's better than nothing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Create clusters that have high similarity between the data belonging to one cluster while aiming for minimal similarity between clusters\n",
    "\n",
    "## K-Means Clustering\n",
    "K determines the number of clusters and the algorithm optimizes around that\n",
    "\n",
    "## Hierarchial Agglomerative Clustering\n",
    "You start with $n$ clusters equal the number of data points and at each step you join two clusters. You stop joining when a certain criterion is reached.\n",
    "\n",
    "## Semi-Supervised Learning\n",
    "Combine both concepts of supervised and unsupervised learning. Increasingly popular.\n",
    "\n",
    "## Market Segmentation with Clustering\n",
    "Common and useful, we'll practice with a market segmentation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# K-means Clustering\n",
    "The most popular and widely used clustering algorithm, and clustering are one of the most popular unsupervised machine learning algorithms.\n",
    "\n",
    "## Goal\n",
    "> **Intra**-class similarity is high\n",
    "\n",
    "> **Inter**-class similarity is low\n",
    "\n",
    "Similarity is determined by distance. Closer is more similar.\n",
    "* **Agglomerative hierarchical** algorithm starts with n clusters\n",
    "* **Non-hierarchical** chooses k initial clusters\n",
    "\n",
    "Unsupervised and you do not know how many clusters you are looking for\n",
    "\n",
    "## Non-Hierarchical Clustering with K-Means Clustering\n",
    "\n",
    "### Process\n",
    "1. Select $k$ initial seeds\n",
    "2. Assign each observation to the cluster to which it is \"closest\"\n",
    "3. Loop\n",
    "    - Cluster center is the mean of all points in the cluster, recalculated each iteration.\n",
    "    - Each iteration reassign points to be part of the closest cluster center.\n",
    "    - Stop if there is no reallocation\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/good-centroid-start.gif' alt=\"k-means clustering animation\" >\n",
    "\n",
    "### Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set number of clusters at initialization time\n",
    "k_means = KMeans(n_clusters=3) \n",
    "\n",
    "# Run the clustering algorithm\n",
    "k_means.fit(some_df) \n",
    "\n",
    "# Generate cluster index values for each row\n",
    "cluster_assignments = k_means.predict(some_df) \n",
    "\n",
    "# Cluster predictions for each point are also stored in k_means.labels_\n",
    "```\n",
    "\n",
    "### Evaluation with Variance Ratio\n",
    "* Accepted metric in wide use is **_Variance Ratio_** aka [**_Calinski Harabasz Score_**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)\n",
    "    * The variance of the points within a cluster to the variance of a point to points in other clusters.\n",
    "    * We want intra-cluster variance to be low suggesting the clusters are tightly knit.\n",
    "    * We want inter-cluster variance to be high suggesting that there is little to no ambiguity about which cluster a point belongs to.\n",
    "\n",
    "#### Calculating Variance Ratio\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Note that we could also pass in k_means.labels_ instead of cluster_assignments\n",
    "print(calinski_harabasz_score(some_df, cluster_assignments))\n",
    "```\n",
    "### Other Metrics\n",
    "* [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
    "* No metric is best, each have different strengths and weaknesses based on given goals.\n",
    "\n",
    "### Optimal K Value\n",
    "1. Fit different K-means clustering objects for every $k$ we want to try then compare the variance ratio scores of each.\n",
    "2. Visualize results with an **_Elbow Plot_** - plots that we can easily see where we hit a point of diminishing returns. They are used with more than just variance ratios, one example is distortion another clustering metric.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_elbow-method.png' alt=\"Calinski Harabaz scores for different values of k\" width='500'>\n",
    "\n",
    "#### Understanding the Elbow\n",
    "\n",
    "A note on elbow plots: higher scores aren't always better. Higher values of $k$ mean introducing more overall complexity -- we will sometimes see elbow plots that look like this:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_dim_returns.png' alt=\"plot with the number of clusters on the x-axis and the sum of squared distances to cluster center on the y-axis\" width=\"500\">\n",
    "\n",
    "$k$ = 20 is technically better as a score but $k$ = 4 is better because it balances model complexity with score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Pipelines simplify the process of applying multiple transformations and fitting models.\n",
    "- ColumnTransformer and FeatureUnion offer flexibility in preprocessing different types of features.\n",
    "- GridSearchCV and RandomizedSearchCV help in tuning hyperparameters efficiently.\n",
    "- SMOTE and ImbPipeline address class imbalance issues effectively.\n",
    "- Bagging, random forests, extra trees, averaging, and stacking are powerful techniques for building robust models.\n",
    "- Hyperparameter tuning and model stacking can significantly improve model performance.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "- **Pop Quiz: Quick Review**\n",
    "  - **Elbow Method:** Identifies optimal number of clusters in K-means by finding the \"elbow point\" where adding another cluster doesn't significantly decrease the variance.\n",
    "  - **K-means Clustering:** Partitions data into K clusters by minimizing the distance between data points and the centroid of their assigned cluster.\n",
    "  - **Hyperparameters in K-means:** Number of clusters (K), distance metric.\n",
    "  - **When Elbow Method Fails:** Smooth curve without a clear elbow point, clusters of different shapes, sizes, and densities.\n",
    "\n",
    "### Agglomerative Clustering Overview\n",
    "\n",
    "- **Definition:** Hierarchical clustering that builds nested clusters by merging or splitting them successively.\n",
    "- **Approach:** Bottom-up method starting with each point as its own cluster, merging the closest pairs step by step.\n",
    "- **Comparison with K-means:**\n",
    "  - K-means is flat clustering; agglomerative clustering is hierarchical.\n",
    "  - K-means requires specifying the number of clusters; agglomerative clustering does not.\n",
    "\n",
    "### Steps in Agglomerative Clustering\n",
    "\n",
    "1. **Calculate Pairwise Distance:** Determine the distance between each pair of data points.\n",
    "2. **Linkage Criteria:** Methods to determine the distance between clusters:\n",
    "   - Single Linkage: Minimum distance between points in two clusters.\n",
    "   - Complete Linkage: Maximum distance between points in two clusters.\n",
    "   - Average Linkage: Average distance between points in two clusters.\n",
    "3. **Merge Closest Clusters:** Form a new cluster by merging the closest pair of clusters.\n",
    "4. **Repeat:** Continue merging until all points are in one cluster or a stopping criterion is met.\n",
    "\n",
    "### Dendrogram\n",
    "\n",
    "- **Definition:** A tree-like diagram that records the sequences of merges or splits.\n",
    "- **Usage:** Helps in determining the number of clusters by visualizing the hierarchical relationship between data points.\n",
    "- **Threshold:** Horizontal cut-off line in the dendrogram to decide the number of clusters.\n",
    "\n",
    "### Implementing Agglomerative Clustering\n",
    "\n",
    "#### Using Scipy\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Calculate the pairwise distance matrix\n",
    "distance_matrix = pdist(data, metric='euclidean')\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering\n",
    "Z = linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Create a dendrogram\n",
    "dendrogram(Z)\n",
    "plt.show()\n",
    "\n",
    "# Form flat clusters\n",
    "clusters = fcluster(Z, t=50, criterion='distance')\n",
    "```\n",
    "\n",
    "#### Using SKLearn\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model\n",
    "model = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data)\n",
    "\n",
    "# Assign labels to data points\n",
    "labels = model.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='rainbow')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Evaluating Clusters\n",
    "\n",
    "- **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters.\n",
    "- **Cophenetic Correlation Coefficient:** Measures how faithfully the dendrogram represents the dissimilarities among observations.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Can find clusters with arbitrary shapes.\n",
    "  - No need to specify the number of clusters upfront.\n",
    "- **Disadvantages:**\n",
    "  - Computationally intensive, especially with large datasets.\n",
    "  - Sensitive to noise and outliers.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Market segmentation\n",
    "- Social network analysis\n",
    "- Image segmentation\n",
    "- Anomaly detection\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Agglomerative clustering is a powerful tool for hierarchical clustering that helps in identifying nested clusters within data. It offers flexibility in terms of linkage criteria and does not require the number of clusters to be predefined. However, it is computationally intensive and requires careful interpretation of dendrograms and threshold settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hierarchical Agglomerative Clustering\n",
    "\n",
    "* K-means uses Expectation-Maximization after we tell it to give us $k$ clusters, however it can not have subgroups within subgroups\n",
    "* Agglomerative Clustering to the rescue! It can have subgroups within subgroups\n",
    "* It starts with $n$ clusters with $n$ = the number of data points then merges until some stopping criterion\n",
    "\n",
    "## Linking Clusters Together\n",
    "\n",
    "* **ward** - merges two cluster on the least variance between them. Leads to more equally sized clusters\n",
    "* **average** - merges the two clusters that have the smallest average distance between all points\n",
    "* **complete** - merges the two clusters that have the smallest maximum distance between their points\n",
    "\n",
    "Can affect the performance, which to use is based on the data and goals.\n",
    "\n",
    "The following diagram demonstrates the clusters created at each step for a dataset of 16 points. Take a look at the diagram and see if you can figure out what the algorithm is doing at each step as it merges clusters together:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/hac_iterative.png' alt=\"initialization through step 14 of HAC algorithm\" width='600'>\n",
    "\n",
    "As you can see it takes the closest clusters and merges them into a single cluster. Below shows as the dots disappear the visualization is replacing them with the newly calculated center.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/dendrogram_gif.gif' alt=\"animation of clusters shown in x-y space on the left and a dendrogram on the right, showing which clusters correspond to which parts of the dendrogram\" width='600'>\n",
    "\n",
    "### Dendrograms and Clustergrams\n",
    "\n",
    "* Easily visualize the results at any given step \n",
    "* The image to the right above in the gif is a Dendrogram\n",
    "  * shows the hierarchical relationship between the various clusters that are computed throughout each step.\n",
    "* The image below is a Clustergram\n",
    "  * Visualize the same information by drawing lines representing each cluster at the each step\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/new_clustergram.png' alt=\"another view of clusters on the left and dendrogram on the right\" width='600'>\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "* Market segmentation\n",
    "* Gain a deeper understanding of a dataset through cluster analysis\n",
    "* Photo sorting on smartphones\n",
    "\n",
    "## Common Problems with Clustering Algorithms\n",
    "\n",
    "* No way of verifying the results are correct or not\n",
    "  * Never treat results of a cluster as ground-truth\n",
    "\n",
    "## Advantages and Disadvantages of K-Means Clustering\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Easy to implement\n",
    "* Usually faster than HAC with reasonably small $k$ and many features\n",
    "* Objects can shift clusters\n",
    "* Tighter clusters than HAC\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Need the right value for $k$\n",
    "* Scaling completely changes the results\n",
    "* Starting points have a strong impact on final results, as seen below. Bad init is less likely than good init and you can run it multiple times.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/bad-centroid-start.gif' alt=\"bad centroid start\" width='600'>\n",
    "\n",
    "## Advantages & Disadvantages of HAC\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Ordered relationship between clusters, which can be useful when visualized\n",
    "* Smaller clusters which allows more granular understanding\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Results depend on distance metric used\n",
    "* Objects can be grouped badly early on and no way to move them\n",
    "* We can't check visuals on more than 3 dimensions so it's hard to know when the algorithm was correct\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_bad-hac.png' alt=\"bad hac\" width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Customer and Market Segmentation\n",
    "\n",
    "- Identify more customers/market segments that are similar to valuable customers or market segments already identified.\n",
    "- Divide into two groups: known valuable customers and everyone else.\n",
    "- Use a distance metric of choice to rate similarity of unknown customers to known valuable customers.\n",
    "- Spend resources to capture customers that are similar to the valuable group.\n",
    "- Likely see customers that are only somewhat similar to the valuable group.\n",
    "- Identify customers that look nothing like known valuable customers.\n",
    "- This process is similar to clustering and is referred to as prospecting.\n",
    "- Allocate resources to market to customers resembling valuable customers to increase the top-of-funnel, meaning more potential customers that haven't shown interest in the product or company yet but are likely to.\n",
    "\n",
    "## Use Case 2: Semi-Supervised Learning\n",
    "\n",
    "- Also known as weakly supervised learning.\n",
    "- Generate pseudo-labels that are possibly correct.\n",
    "    - Doesn't use clustering; it uses supervised learning algorithms in an unsupervised way.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_semi-supervised.png' width='600'>\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Train your model on your labeled training data.**\n",
    "2. **Use your trained model to generate pseudo-labels for unlabeled data.**\n",
    "3. **Combine the pseudo-labels with your actual data.**\n",
    "4. **Retrain your model on the new dataset.**\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- It is risky.\n",
    "- When done well, it can increase overall model performance by opening up access to much more data.\n",
    "- Saves a ton of money on labeling costs.\n",
    "\n",
    "#### Downsides\n",
    "\n",
    "- When data is really noisy, incorrect labels will skew the model.\n",
    "- Feedback loops.\n",
    "- More complicated problems tend to work less.\n",
    "\n",
    "#### Use a Holdout Set to Test\n",
    "\n",
    "- As usual but even more important in this case, make sure to use ground-truth or non-pseudo-code labels to test with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Coefficient\n",
    "\n",
    "The Silhouette Coefficient is a measure used to evaluate the quality of clusters created by a clustering algorithm. It takes into account both the cohesion within clusters and the separation between clusters.\n",
    "\n",
    "## Definition\n",
    "\n",
    "For a given data point $i$, the Silhouette Coefficient $s(i)$ is defined as:\n",
    "\n",
    "$$ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
    "\n",
    "where:\n",
    "- $a(i)$ is the mean distance between $i$ and all other points in the same cluster.\n",
    "- $b(i)$ is the mean distance between $i$ and all points in the nearest cluster (the cluster with the smallest mean distance to $i$).\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "**Higher is better**\n",
    "\n",
    "- $s(i)$ ranges from -1 to 1.\n",
    "  - $s(i) \\approx 1$: The data point is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
    "  - $s(i) \\approx 0$: The data point is on or very close to the decision boundary between two neighboring clusters.\n",
    "  - $s(i) \\approx -1$: The data point is poorly matched to its own cluster and well-matched to a neighboring cluster.\n",
    "\n",
    "## Overall Silhouette Score\n",
    "\n",
    "The overall Silhouette Score for a clustering is the mean Silhouette Coefficient of all data points:\n",
    "\n",
    "$$ S = \\frac{1}{N} \\sum_{i=1}^{N} s(i) $$\n",
    "\n",
    "where $N$ is the total number of data points.\n",
    "\n",
    "## Usage\n",
    "\n",
    "The Silhouette Coefficient can be used to:\n",
    "- Determine the optimal number of clusters by comparing the average silhouette scores for different numbers of clusters.\n",
    "- Evaluate the quality of clustering algorithms, with higher scores indicating better-defined clusters.\n",
    "\n",
    "## Example\n",
    "\n",
    "To compute the Silhouette Coefficient in Python, you can use the `silhouette_score` function from the `sklearn.metrics` module:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# X is your data and labels are the cluster labels\n",
    "score = silhouette_score(X, labels)\n",
    "print(f'Silhouette Score: {score}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Principal Component Analysis in scikit-learn\n",
    "\n",
    "* Reduces dimensions while trying to capture as much info from the dataset as possible\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "transformed = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "Transforms dataset along principal axes. The first axes tries to capture the maximum variance within the data. From here, additional axes are constructed which are orthogonal to the previous axes and continue to account for as much of the remaining variance as possible.\n",
    "\n",
    "Transforms this:\n",
    "\n",
    "<img src=\"images/pca-data1.png\" width=\"300\">\n",
    "\n",
    "Into this:\n",
    "\n",
    "<img src=\"images/pca-data2.png\" width=\"300\">\n",
    "<img src=\"images/inhouse_pca.png\" width=\"600\">\n",
    "\n",
    "```python\n",
    "pca.explained_variance_ratio_\n",
    "```\n",
    "\n",
    "Results are cumulative:\n",
    "\n",
    "```python\n",
    "np.cumsum(pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "Below we visualize the first PCA component:\n",
    "\n",
    "<img src=\"images/pca-data3.png\" width=\"300\">\n",
    "\n",
    "## Steps for Performing PCA\n",
    "\n",
    "The theory behind PCA rests upon many foundational concepts of linear algebra. After all, PCA is re-encoding a dataset into an alternative basis (the axes). Here are the exact steps:\n",
    "\n",
    "1. Recenter each feature of the dataset by subtracting that feature's mean from the feature vector.\n",
    "2. Calculate the covariance matrix for your centered dataset.\n",
    "3. Calculate the eigenvectors of the covariance matrix.\n",
    "    1. You'll further investigate the concept of eigenvectors in the upcoming lesson.\n",
    "4. Project the dataset into the new feature space: Multiply the eigenvectors by the mean-centered features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Segmentation with Clustering\n",
    "* one of the most popular use cases for clustering\n",
    "\n",
    "## What is Market Segmentation?\n",
    "* **Cluster Analysis** to segment a customer base into different _market segments_ using the clustering techniques we've learned\n",
    "* Ex: decide marketing budget allocation in order to attract more customers\n",
    "    * Create personalized regression models for each group\n",
    "* Know who your customer is.  Identify sements in the customer data we can look for trends\n",
    "    * Ex decide the station to run commercials on\n",
    "* Find the segments with clustering\n",
    "    * find them based on behavior\n",
    "\n",
    "## Targeting\n",
    "Segmentation is just the first step\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_marketing-strategy.png' width='700'>\n",
    "\n",
    "* Build individualized strategies\n",
    "    * which market segment is most valuable to us? Use resarch and data analysis\n",
    "    * how do we allocate the advertising budget?  determine where to spend money best to reach the group\n",
    "* Figure out how to position our product to make it both desirable and stand out from competitors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing (NLP)\n",
    "\n",
    "NLP is the study of how computers interact with humans through natural language, intersecting *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*.\n",
    "\n",
    "## History\n",
    "\n",
    "- Initially rules-based with rules borrowed from Linguistics.\n",
    "- In the 1980s, machine learning and AI showed potential.\n",
    "- Now used globally by data scientists.\n",
    "\n",
    "## Popular Libraries\n",
    "\n",
    "- **Natural Language Tool Kit (NLTK):** Popular NLP library in Python.\n",
    "\n",
    "## Regular Expressions\n",
    "\n",
    "- Used for text pattern matching and manipulation (regex).\n",
    "\n",
    "## Feature Engineering for Text Data\n",
    "\n",
    "Text data requires specific feature engineering due to its ambiguity.\n",
    "\n",
    "- Remove stop words.\n",
    "- Create frequency distributions.\n",
    "- Represent histograms.\n",
    "- Apply stemming and lemmatization.\n",
    "- Identify bigrams (pairs of words that occur together).\n",
    "\n",
    "### Stemming, Lemmatization, and Stop Words\n",
    "\n",
    "- **Stemming:** Reduces words to their root form crudely (e.g., runs, running → run; ponies → poni).\n",
    "  <img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-nlp-and-word-vectorization/master/images/new_stemming.png' alt=\"stemming rules and examples\" width=\"400\">\n",
    "- **Lemmatization:** Uses morphology to reduce words to their basic forms called **lemma**.\n",
    "  |   Word   |  Stem | Lemma |\n",
    "  |:--------:|:-----:|:-----:|\n",
    "  |  Studies | Studi | Study |\n",
    "  | Studying | Study | Study |\n",
    "- **Stop Words:** Common words with little information (e.g., \"of\", \"the\"). Often removed to reduce dimensionality.\n",
    "\n",
    "## Context-Free Grammars and Part-of-Speech (POS) Tagging\n",
    "\n",
    "- **Context-Free Grammar (CFG):** Defines rules for sentence structure.\n",
    "- **POS Tagging:** Helps interpret sentence structure by tagging parts of speech.\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "- **Supervised Learning:** Classify documents (e.g., spam detection).\n",
    "- **Unsupervised Learning:** Group documents into topics without predefined labels.\n",
    "\n",
    "## Working With Text Data\n",
    "\n",
    "Text data often requires more cleaning and preprocessing than other data types.\n",
    "\n",
    "### Basic Cleaning and Tokenization\n",
    "\n",
    "- **Tokenization:** Splits text into tokens (words or phrases).\n",
    "- **Cleaning:** Often includes lowercasing text, removing punctuation, and filtering out common stop words.\n",
    "\n",
    "## Creating a Bag of Words\n",
    "\n",
    "- **Corpus:** A large structured text set used for NLP tasks.\n",
    "- **Bag of Words:** Vectorizes text data by counting word occurrences.\n",
    "  | Document | Aardvark | Apple | ... | Zebra |\n",
    "  |:--------:|:--------:|:-----:|-----|-------|\n",
    "  |     1    |     0    |   3   | ... | 1     |\n",
    "  |     2    |     1    |   2   | ... | 0     |\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF) Vectorization\n",
    "\n",
    "- Emphasizes rare words which contain more information.\n",
    "- **Term Frequency (TF):**\n",
    "  $$\\large Term\\ Frequency(t) = \\frac{number\\ of\\ times\\ t\\ appears\\ in\\ a\\ document}{total\\ number\\ of\\ terms\\ in\\ the\\ document} $$ \n",
    "- **Inverse Document Frequency (IDF):**\n",
    "  $$\\large IDF(t) = log_e(\\frac{Total\\ Number\\ of\\ Documents}{Number\\ of\\ Documents\\ with\\ t\\ in\\ it})$$\n",
    "- **TF-IDF:** Product of TF and IDF values.\n",
    "\n",
    "## Example: Tokenizing and Cleaning Text\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "data['processed_text'] = data['text_column'].apply(preprocess_text)\n",
    "print(data.head())\n",
    "```\n",
    "\n",
    "## NLP and Bayesian Statistics\n",
    "\n",
    "- **Naive Bayesian Classification:** Widely used for spam detection.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- NLP involves various tasks like text processing, vectorization, and classification.\n",
    "- Ensuring proper text cleaning and preprocessing is crucial.\n",
    "- Using models like Count Vectorizer and TF-IDF Vectorizer helps represent text numerically for machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Classification\n",
    "\n",
    "## Review of Tokenization and Pre-Processing\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization is the process of splitting text into individual tokens. These tokens can be words, phrases, or even characters. There are different types of tokenization:\n",
    "\n",
    "- **Word Tokenization:** Splits text into words.\n",
    "- **Sentence Tokenization:** Splits text into sentences.\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "Stop words are common words (like \"the\", \"and\", \"is\") that are often removed from text data to reduce noise and focus on the meaningful words.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "- **Stemming:** Reduces words to their base form by removing suffixes. It can produce non-existent words.\n",
    "- **Lemmatization:** Reduces words to their base form considering the part of speech, resulting in valid words.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Example text preprocessing\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Removing stop words\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(lemmatized_tokens)\n",
    "```\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "Vectorization is the process of converting text into numerical form so it can be fed into machine learning models.\n",
    "\n",
    "### Types of Vectorizers\n",
    "\n",
    "- **Count Vectorizer:** Converts text into a matrix of token counts.\n",
    "- **TF-IDF Vectorizer:** Converts text into a matrix of token counts scaled by inverse document frequency.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Example text corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Count Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X_count.toarray())\n",
    "print(X_tfidf.toarray())\n",
    "```\n",
    "\n",
    "## Evaluating Clusters\n",
    "\n",
    "- **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters.\n",
    "- **Cophenetic Correlation Coefficient:** Measures how faithfully the dendrogram represents the dissimilarities among observations.\n",
    "\n",
    "## Advantages and Disadvantages of Clustering\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Can find clusters with arbitrary shapes.\n",
    "- No need to specify the number of clusters upfront.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Computationally intensive, especially with large datasets.\n",
    "- Sensitive to noise and outliers.\n",
    "\n",
    "## Applications\n",
    "\n",
    "- Market segmentation\n",
    "- Social network analysis\n",
    "- Image segmentation\n",
    "- Anomaly detection\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Agglomerative clustering is a powerful tool for hierarchical clustering that helps in identifying nested clusters within data. It offers flexibility in terms of linkage criteria and does not require the number of clusters to be predefined. However, it is computationally intensive and requires careful interpretation of dendrograms and threshold settings.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next class, we will cover vectorization and classification. Ensure you are familiar with the concepts discussed today. If you have any questions, please feel free to ask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with NLTK\n",
    "\n",
    "**Natural Language Tool Kit (NLTK):** A popular library for Python for Natural Language Processing.\n",
    "* Contains sample corpus like presidential speeches and Project Gutenberg transcripts.\n",
    "* Contains its own Bayesian classifiers for quick testing.\n",
    "* Relies heavily on linguistics but the tools are easy for non-linguists to use, e.g., making a parse tree.\n",
    "<center> <img src='images/new_parse_tree.png' width=\"750\"> </center>\n",
    "\n",
    "## Working with Text\n",
    "\n",
    "- **Stop Word Removal**\n",
    "- **Filtering and Cleaning**\n",
    "- **Feature Selection and Feature Engineering:** Libraries like Penn Tree Bank, part of speech tags, sentence polarity, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions (regex)\n",
    "\n",
    "Regular expressions (regex) are powerful tools for Natural Language Processing (NLP). They allow for flexible pattern matching and text manipulation.\n",
    "\n",
    "## Handling Apostrophes\n",
    "\n",
    "Using `word_tokenize()` splits a word that contains an apostrophe into multiple tokens. For example, \"they're\" becomes [\"they\", \"'\", \"re\"]. We can use regex patterns to capture \"they're\" as a single word.\n",
    "\n",
    "## Patterns\n",
    "\n",
    "Regex is as effective as the patterns we create.\n",
    "\n",
    "```python\n",
    "import re\n",
    "sentence = 'he said that she said \"hello\".'\n",
    "pattern = 'he'\n",
    "p = re.compile(pattern)\n",
    "print(p.findall(sentence)) # Output will be ['he', 'he, 'he']\n",
    "```\n",
    "\n",
    "**Anchors** can be used to define word boundaries.\n",
    "\n",
    "## Ranges, Groups, Quantifiers\n",
    "\n",
    "### Ranges\n",
    "\n",
    "- `[A-Za-z0-9]` matches most alphanumeric characters in the English alphabet.\n",
    "\n",
    "### Character Class Examples\n",
    "\n",
    "- `\\d` matches any digit (equivalent to `[0-9]`).\n",
    "- `\\w` matches any word character.\n",
    "- `\\D` matches any non-digit character.\n",
    "- `\\W` matches any non-word character.\n",
    "\n",
    "### Quantifiers\n",
    "\n",
    "Quantifiers match the preceding element a specific number of times.\n",
    "\n",
    "- `*` matches 0 or more times.\n",
    "- `+` matches 1 or more times.\n",
    "- `?` matches 0 or 1 times.\n",
    "- `{n}` matches exactly n times (e.g., `{3}` matches exactly 3 times).\n",
    "- `{n,k}` matches between n and k times (e.g., `{3,5}` matches between 3 and 5 times).\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/master/images/regex_cheat_sheet.png' alt=\"regex cheat sheet\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Feature Engineering Text Data\n",
    "\n",
    "Text data has a lot of ambiguity and requires specific techniques for feature engineering.\n",
    "\n",
    "## Remove Stop Words\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(some_text_data)\n",
    "\n",
    "# It is usually a good idea to lowercase all tokens during this step, as well\n",
    "stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "```\n",
    "\n",
    "## Frequency Distributions\n",
    "\n",
    "```python\n",
    "from nltk import FreqDist\n",
    "\n",
    "freqdist = FreqDist(tokens)\n",
    "\n",
    "# Get the 200 most common words \n",
    "most_common = freqdist.most_common(200)\n",
    "```\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "Often utilizes the WordNet lexical database.\n",
    "\n",
    "```python\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet'))  # Output: foot\n",
    "print(lemmatizer.lemmatize('running'))  # Output: run\n",
    "```\n",
    "\n",
    "## Bigrams and Mutual Information Score\n",
    "\n",
    "- Pair adjacent words and treat them as one token, a special case of n-grams where n is 2.\n",
    "- n-grams can be created at the character level and commonly done so.\n",
    "- Useful because you can apply a frequency filter; common values are at least 5.\n",
    "- **Pointwise Mutual Information Score**: A measure of mutual dependence between two words from information theory. For example, \"San Francisco\" would appear together a lot, meaning it has a higher information score.\n",
    "- NLTK can compute this too.\n",
    "\n",
    "```python\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_freq_filter(5)\n",
    "print(finder.nbest(bigram_measures.pmi, 10))\n",
    "```\n",
    "\n",
    "## Context-Free Grammars and POS Tagging\n",
    "\n",
    "> \"Colorless green ideas sleep furiously.\" - Noam Chomsky\n",
    "\n",
    "While correct grammatically and syntactically, it's not meaningful semantically. There is a \"deep structure\" we recognize as correct regardless of content. This is what Context-Free Grammar (CFG) is, the idea that we don't need any context to determine the grammar is correct.\n",
    "\n",
    "### Five Levels of Language\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_LevelsOfLanguage-Graph.png' alt=\"Levels of Language\" width=800>\n",
    "\n",
    "**CFGs are the Syntax Level**\n",
    "\n",
    "- CFGs are important to computer science due to parsers.\n",
    "- Part of Speech (POS) Tags such as \"run\" being a noun or a verb.\n",
    "\n",
    "### Parse Trees and Sentence Structure\n",
    "\n",
    "Sentences in English follow structures like Noun Phrase -> Verb Phrase -> Prepositional Phrase, but this gets complicated due to recursion and ambiguity.\n",
    "\n",
    "> \"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\" - Groucho Marx\n",
    "\n",
    "The ambiguity in this sentence is hard for a computer processor. Parse trees help us understand the difference.\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/parse_tree.png' alt=\"Parse Tree\">\n",
    "\n",
    "* Noun phrase: `['I']`\n",
    "* Verb phrase: `['shot', 'an', 'elephant']`\n",
    "* Prepositional phrase: `['in', 'my', 'pajamas']`\n",
    "\n",
    "VS\n",
    "\n",
    "* Noun phrase: `['I']`\n",
    "* Verb phrase: `['shot', 'an', 'elephant', 'in', 'my', 'pajamas']`\n",
    "\n",
    "The second one treats \"in my pajamas\" as a noun phrase within the verb phrase (so the pajamas belong to the elephant). The first one is more typical and treats the prepositional phrase as its own entity.\n",
    "\n",
    "### POS Tagging and CFGs\n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/cfg.png' alt=\"CFG\">\n",
    "\n",
    "Breaking down the CFG:\n",
    "\n",
    "* `S -> NP VP`: A sentence (S) consists of a Noun Phrase (NP) followed by a Verb Phrase (VP).\n",
    "* `PP -> P NP`: A Prepositional Phrase (PP) consists of a Preposition (P) followed by a Noun Phrase (NP).\n",
    "* `NP -> Det N | Det N PP | 'I'`: A Noun Phrase (NP) can consist of:\n",
    "  * a Determiner (Det) followed by a Noun (N), or \n",
    "  * a Determiner (Det) followed by a Noun (N) and a Prepositional Phrase (PP), or\n",
    "  * The token 'I'.\n",
    "* `VP -> V NP | VP PP`: A Verb Phrase can consist of:\n",
    "  * a Verb (V) followed by a Noun Phrase (NP), or\n",
    "  * a Verb Phrase (VP) followed by a Prepositional Phrase (PP).\n",
    "* `Det -> 'an' | 'my'`: Determiners are 'an' or 'my'.\n",
    "* `N -> 'elephant' | 'pajamas'`: Nouns are 'elephant' or 'pajamas'.\n",
    "* `V -> 'shot'`: Verbs are 'shot'.\n",
    "* `P -> 'in'`: Prepositions are 'in'.\n",
    "\n",
    "The CFG provides explicit rules for structuring sentences, noun phrases, verb phrases, and prepositional phrases, as well as the parts of speech each token belongs to.\n",
    "\n",
    "## Working with Text Data Questions\n",
    "\n",
    "- Do we remove stop words or not?\n",
    "- Do we stem or lemmatize our text data, or leave the words as is?\n",
    "- Is basic tokenization enough, or do we need to support special edge cases through the use of regex?\n",
    "- Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\n",
    "- Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\n",
    "- What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ethics\n",
    "## Where Does NLP Data Come From?\n",
    "### Ground Truth in Machine Learning\n",
    "* labels need to exist for each record for supervised learning\n",
    "    * these labels are called \"ground truth\"\n",
    "    * difference between predictions and ground truth is how the quality of a model is measured\n",
    "### Labeling\n",
    "* Traces back to qualitative learning\n",
    "    * social science researchers would code raw data\n",
    "    * determined before the analysis begins (also known as \"a priori\" or \"deductive\" codes), and are sometimes developed during the analysis (also known as \"grounded\" or \"inductive\" codes). Often multiple researchers will apply codes to the same text sample and then use statistical techniques to measure the [inter-rater reliability](https://www.statology.org/inter-rater-reliability/) and ensure that the codes are being applied to the samples in a consistent way\n",
    "\n",
    "### Data Labeling Today: Crowd Workers\n",
    "Data scientists ([and social scientists](https://blogs.lse.ac.uk/impactofsocialsciences/2020/12/15/how-to-conduct-valid-social-science-research-using-mturk-a-checklist/)) instead frequently use crowdsourcing platforms to label text data cheaply, quickly, and at scale.\n",
    "\n",
    "The most popular platform for data labeling is [Amazon Mechanical Turk (MTurk)](https://www.mturk.com/). Requesters can post \"Human Intelligence Tasks\" (HITs) such as data labeling to the MTurk platform and pay workers for completion of each individual task.\n",
    "\n",
    "#### MTurk and Privacy\n",
    "Sending any kind of personally-identifiable information (PII) or other sensitive data to a platform like MTurk risks the exposure of this data. A [Microsoft Research team](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/cscw14-crowdattack.pdf) even found that they could pay MTurk workers to steal user data if it was presented as part of a legitimate task!\n",
    "\n",
    "Some [data manipulation techniques](https://www.cs.cmu.edu/~jbigham/pubs/pdfs/2017/crowdmask.pdf) have been proposed to avoid making certain data identifiable, but the less-risky option is to avoid sending any kind of sensitive data to this kind of platform.\n",
    "\n",
    "#### MTurk and Working Conditions\n",
    "MTurk workers are independent contractors and are therefore usually not legally entitled to certain worker protections. Research has found that some requesters exploit these workers by [paying less than &#36;2 per hour](https://arxiv.org/abs/1712.05796) and create stressful working conditions by [mass rejecting completed tasks](https://blog.turkopticon.net/?p=731) with essentially no recourse for workers.\n",
    "\n",
    "\n",
    "## Implicit Labels Scraped from the Internet\n",
    "\n",
    "Given the logistical and ethical challenges that can arise in paying people to label data, some data scientists have moved towards using data that was never explicitly labeled. As a data scientist in [one study](https://arxiv.org/abs/1812.05239) put it:\n",
    "\n",
    "> There isn’t really a thought process surrounding... _Should [our team] ingest this data in?_ [...] If it is available to us, we ingest it.\n",
    "\n",
    "One example of such a readily-available data source is [Common Crawl](https://commoncrawl.org/). Common Crawl attempts to scrape the entire Internet every month or so, and hosts petabytes of data from each crawl. The scale and ease of access of this dataset has made it very appealing to data scientists.\n",
    "\n",
    "#### Scraped Data and Privacy Concerns\n",
    "Even when posting \"publicly\", website users often have an expectation that this data will not be aggregated and analyzed outside of its original context. For example, the [publication of a dataset](https://www.vox.com/2016/5/12/11666116/70000-okcupid-users-data-release) of 70,000 users scraped from OkCupid was broadly criticized. Social computing research Os Keyes [wrote](https://ironholds.org/scientific-consent/):\n",
    "\n",
    "> this is without a doubt one of the most grossly unprofessional, unethical and reprehensible data releases I have ever seen.\n",
    "\n",
    "(The dataset eventually taken down not because of ethical issues but because OkCupid filed a [DMCA](https://www.copyright.gov/dmca/) complaint).\n",
    "\n",
    "\n",
    "#### Scraped Data and Garbage In, Garbage Out\n",
    "Concerns about data quality are especially relevant when working with data that was scraped from random sources rather than collected with a particular intention in mind.\n",
    "\n",
    "Studies of the Common Crawl corpus in particular have found that it contains a [\"significant amount of undesirable content\"](https://aclanthology.org/2021.acl-short.24.pdf) including hate speech and sexually explicit content, and also that models trained on it [exhibit numerous historical biases](https://www.cs.princeton.edu/~arvindn/publications/language-bias.pdf) related to race and gender.\n",
    "\n",
    "## Bonus Topic: Large Language Models\n",
    "Another kind of model that uses text data but isn't a traditional text classifier is a ***large language model***. At the time of this writing, [GPT-3](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html), [GitHub Copilot](https://github.com/features/copilot), and [BERT](https://huggingface.co/blog/bert-101) are some popular examples of this type of model. Typically a large language model tries to predict the next word or words in a sequence, in a way that can _generate_ text rather than simply labeling it.\n",
    "\n",
    "These models are almost always developed with scraped data, because that is the only way to achieve the necessary scale. GPT-3, for example, was trained on [the Common Crawl corpus in addition to curated sources](https://techcrunch.com/2020/08/07/here-are-a-few-ways-gpt-3-can-go-wrong/). It also has demonstrated bias against [Muslims](https://hai.stanford.edu/news/rooting-out-anti-muslim-bias-popular-language-model-gpt-3), [women](https://aclanthology.org/2021.nuse-1.5), and [disabled people](https://arxiv.org/abs/2206.11993).\n",
    "\n",
    "In a more stunning example, the \"Tay\" chatbot started with a pre-trained dataset then \"learned\" racist and incendiary language from Twitter users, eventually being [pulled from the platform](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist) after less than a day. This shows that social media platforms, despite having some form of moderation, may be worse sources of training data than generic sources like Common Crawl.\n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "* [Qualitative Data Coding 101](https://gradcoach.com/qualitative-data-coding-101/)\n",
    "* [Data and its (dis)contents: A survey of dataset development and use in machine learning research](https://arxiv.org/abs/2012.05345)\n",
    "* [Documenting Data Production Processes: A Participatory Approach for Data Work](https://arxiv.org/abs/2207.04958)\n",
    "* [The trainer, the verifier, the imitator: Three ways in which human platform workers support artificial intelligence](https://journals.sagepub.com/doi/10.1177/2053951720919776)\n",
    "* [Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?](https://arxiv.org/abs/1912.08320)\n",
    "* [The dangers of data scraped from the internet](https://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/)\n",
    "* [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars, Vectors, Matrices, Tensors\n",
    "> **Scalar**: A single number\n",
    "* **Real valued scalars**: Let $S \\in  \\mathbb{R} $  be the salary of an individual\n",
    "* **Natural number scalars**: Let $n \\in \\mathbb{N}$ be the number of floors in a building\n",
    "\n",
    "\n",
    "> **Vector**: An **array** of numbers arranged in some order, as opposed to the single numbered scalar. \n",
    "\n",
    "\\begin{equation}\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "  x_{1} \\\\\n",
    "  x_{2} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{n-1} \\\\\n",
    "  x_{n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Where $x$ is the name of the vector and $(x_1,x_2, \\ldots, x_{n-1}, x_n)$ are the scalar components of the vector.\n",
    "\n",
    "A vector has direction and a magnitude (length).\n",
    "\n",
    "Below is an example of a vector in 3D vector space:  \n",
    "\n",
    "![](https://curriculum-content.s3.amazonaws.com/data-science/images/vec2.png)\n",
    "\n",
    "In python numpy arrays work well for vectors\n",
    "```python \n",
    "# create a vector from list [2,4,6]\n",
    "import numpy as np\n",
    "v = np.array([2, 4, 6])\n",
    "print(v)\n",
    "\n",
    "print (x[1:4])     # second to fourth element. Element 5 is not included\n",
    "print (x[0:-1:2])  # every other element\n",
    "print (x[:])       # print the whole vector\n",
    "print (x[::-1]) # reverse the vector!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Matrix** is a 2-dimensional array of numbers written between square brackets. \n",
    "\n",
    "$$\n",
    "   A=\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   A_{1,1} & A_{1,2} & \\ldots &A_{1,n} \\\\\n",
    "   A_{2,1} & A_{2,2} & \\ldots &A_{2,n} \\\\\n",
    "   \\vdots& \\vdots & \\ddots & \\vdots \\\\\n",
    "   A_{m,1} & A_{m,2} & \\ldots &A_{m,n} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "We usually give matrices uppercase variable names with bold typeface, such as $A$. If a real-valued matrix $A$ has a height of $m$ and a width of $n$ as above, we state this as $A \\in \\mathbb{R}^{m \\times n}$. In machine learning, a vector can be seen as a special case of a matrix.\n",
    "\n",
    "> A vector is a matrix that has only 1 column, so you have an $(m \\times 1)$-matrix. $m$ is the number of rows, and 1 here is the number of columns, so a matrix with just one column is a vector.\n",
    "\n",
    "array of arrays makes a matrix in python\n",
    "```python\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(X)\n",
    "print (X[0, 0]) # element at first row and first column\n",
    "print (X[-1, -1]) # element from the last row and last column \n",
    "print (X[0, :]) # first row and all columns\n",
    "print (X[:, 0]) # all rows and first column \n",
    "print (X[:]) # all rows and all columns\n",
    "```\n",
    "\n",
    "We can also define matlab styles matrices (for those used to matlab definitions) in the following way:\n",
    "```python\n",
    "Y = np.mat([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(Y)\n",
    "```\n",
    "\n",
    "> **Tensor** An array of numbers arranged on a regular grid with a variable number of axes.\n",
    "\n",
    "A vector is a one-dimensional or \"first order tensor\" and a matrix is a two-dimensional or \"second order tensor\".\n",
    "Tensor notation is just like matrix notation, with a capital letter that represents a tensor, and lowercase letters with a subscript representing scalar values within the tensor. Many operations that can be performed with scalars, vectors, and matrices can be reformulated to be performed with tensors as well. The image below shows some of these operations for a  3D tensor. \n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/new_tensors.png\" width=\"700\">\n",
    "\n",
    "As a tool, tensors and tensor algebra are widely used in the fields of physics and engineering, and in data science it is particularly useful when you'll learn about deep learning models. \n",
    "We'll revisit tensors and relevant operations in the deep learning sections and explain how tensors are created, manipulated, and saved using more advanced analytical tools like Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/new_vector.png\" width=\"150\">\n",
    "\n",
    "Neural networks frequently process weights and inputs of different sizes where the dimensions do not meet the requirements of matrix multiplication. Matrix transpose provides a way to \"rotate\" one of the matrices so that the operation complies with multiplication requirements and can continue. There are two steps to transpose a matrix:\n",
    "\n",
    "* Rotate the matrix right 90° clockwise.\n",
    "* Reverse the order of elements in each row (e.g. [a b c] becomes [c b a]).\n",
    "This can be better understood looking at this image :\n",
    "\n",
    "<img src=\"https://curriculum-content.s3.amazonaws.com/data-science/images/new_matrix.png\" width=\"350\">\n",
    "\n",
    "Numpy provides the transpose operation by using the `.T` attribute or the `np.transpose()` function with the array that needs to be transposed as shown below:\n",
    "\n",
    "```python\n",
    "# create a transpose of a matrix\n",
    "\n",
    "A = np.array([\n",
    "   [1, 2, 3], \n",
    "   [4, 5, 6],\n",
    "   [7, 8, 9]])\n",
    "\n",
    "A_transposed = A.T\n",
    "A_transposed_2 = np.transpose(A)\n",
    "\n",
    "print(A,'\\n\\n', A_transposed, '\\n\\n', A_transposed_2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras and TensorFlow for Neural Networks\n",
    "\n",
    "## Tensors in Keras\n",
    "\n",
    "- **Scalars**: 0D tensors\n",
    "- **Vectors**: 1D tensors\n",
    "- **Matrices**: 2D tensors\n",
    "- **3D tensors**\n",
    "\n",
    "A tensor is defined by three key attributes:\n",
    "- **Rank**: Number of axes\n",
    "- **Shape**: Dimensions of the tensor\n",
    "- **Data Type**: Type of data contained\n",
    "\n",
    "## Data Manipulations in NumPy\n",
    "\n",
    "### Unrowing Matrices (important for images)\n",
    "Example: Converting a `(790, 64, 64, 3)` matrix to a `(64*64*3, 790)` matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_unrow = img.reshape(790, -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the Rank\n",
    "Convert a vector with shape `(790,)` to `(1,790)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(vector, (1,790))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[start_idx : end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "**Element-wise**: Updates each element with the corresponding element from another tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2, 3, 4]) + np.array([5, 6, 7, 8])\n",
    "# result: array([ 6,  8, 10, 12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcasting**: Allows a smaller tensor to be added to a larger tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "A += B\n",
    "print(A)\n",
    "# Updated A:\n",
    "# [[ 1  3  5]\n",
    "#  [ 4  6  8]\n",
    "#  [ 7  9 11]\n",
    "#  [10 12 14]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor dot**: Sum of element products following matrix rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = [1, 2, 3]\n",
    "print(np.dot(B, B))\n",
    "# Result: 14\n",
    "\n",
    "A = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(np.dot(A, B))\n",
    "# Result: array([ 8, 26, 44, 62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the first element is the sum of the first row of $A$ multiplied by $B$ elementwise:  \n",
    "$$ 0*1 + 1*2 + 2*3 = 0 + 2 + 6 = 8 $$ \n",
    "\n",
    "Followed by the sum of the second row of $A$ multiplied by $B$ elementwise:  \n",
    "$$ 3*1 + 4*2 + 5*3 = 3 + 8 + 15 = 26 $$\n",
    "\n",
    "and so on.\n",
    "\n",
    "## Keras Sequential Model Example\n",
    "\n",
    "<img src = \"images/sequentialvsfunctional.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "# Dense means this layer is fully connected\n",
    "# input_shape is optional and the next layer added is based on the shape of the prior\n",
    "model.add(layers.Dense(units, activation, input_shape))\n",
    "\n",
    "# notice the loss function\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional API:\n",
    "\n",
    "<center><img src = \"images/skipconnection.png\" ></center>\n",
    "\n",
    "Skip connections, branching, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Training Terms\n",
    "\n",
    "**Sample**: A single element of a dataset.\n",
    "  * In a convolutional network, an image is a sample.\n",
    "  * In a speech recognition model, an audio file is a sample.\n",
    "\n",
    "**Batch**: A set of *N* samples processed independently, but in parallel.\n",
    "  * **Training**: Processing a batch results in only one update to the model.\n",
    "  * **Approximation**: Batches better approximate the distribution of input data than single samples. Larger batches provide a more accurate approximation.\n",
    "  * **Inference**: Use the largest batch size your system can manage without memory issues for faster evaluation/prediction.\n",
    "\n",
    "**Epoch**: One full pass over the entire dataset.\n",
    "  * **Phases**: Used to segment training into distinct phases for easier logging and evaluation.\n",
    "  * **Validation**: If using `validation_data` or `validation_split` during training with Keras, evaluations are performed at the end of each epoch.\n",
    "  * **Callbacks**: Functions like adjusting learning rates or saving model checkpoints can be scheduled to run at the end of each epoch.\n",
    "\n",
    "## Batch Sizes\n",
    "batch_size = # of points you train on in given forward/back-prop loop\n",
    "typical batch sizes: 1, 32, 64, 128, 256, 512\n",
    "Batch size:\n",
    "\n",
    "- Stochastic Gradient Descent: batch_size = 1\n",
    "- Minibatch Gradient Descent: batch_size = $n_{batch} < N_{train}$\n",
    "- Batch Gradient Descent: full training set fed in on each forward/back-prop loop\n",
    "The intuition behind SGD, mini-batch:\n",
    "\n",
    "SGD: single point $i$\n",
    "$$ \\textbf{w}_{k+1} = \\textbf{w}_k - \\alpha \\nabla_{\\textbf{w}}L(y_i, \\hat{y}_i) $$\n",
    "\n",
    "- Using a single point may not be good enough to sample loss function and its gradient. Noisy descent. \n",
    "- Can go wrong direction.\n",
    "- Effectively looping one point at a time. Computationally inefficient.\n",
    "Mini-batch gives you average of gradient over a sample of the training set: often good enough!\n",
    "\n",
    "$$ \\textbf{w}_{k+1} = \\textbf{w}_k - \\frac{\\alpha}{n_{batch}} \\sum_{i=1}^{n_{batch}} \\nabla_{\\textbf{w}}L(y_i, \\hat{y}_i) $$\n",
    "\n",
    "- Substantially smaller amount of data fed through: still make progress.\n",
    "Batch gradient descent: all training data.\n",
    "    \n",
    "- Will always lower objective on training.\n",
    "- But depending on dataset:\n",
    "- manipulating huge matrices just to make one gradient descent step.\n",
    "<img src = \"images/gradient_descent.png\" width=600>\n",
    "\n",
    "#### Epochs\n",
    "- Number of times to cycle through **entire** training data.\n",
    "- Implies number of forward/backprop cycles is: $$ \\frac{N_{train}}{n_{batch}} \\times \\text{epochs} $$\n",
    "Can train through all epochs defined or:\n",
    "\n",
    "**Earlystopping callback on training**\n",
    "- Monitor training loss and set improvement threshold (min_delta)\n",
    "- Waiting certain number of epochs if no improvements (patience)\n",
    "- Terminate training\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "trainCallback = EarlyStopping(monitor='loss', min_delta = 1e-6, patience = 5)\n",
    "**Validation set**\n",
    "- can feed an external validation set \n",
    "- hold out a fraction of training set for validation\n",
    "- evaluates on train and validation at each epoch\n",
    "\n",
    "## After Fit\n",
    "\n",
    "`history.history` - history on how the model was trained\n",
    "- `history.history['loss']`: Loss values per epoch\n",
    "- `history.history['accuracy']`: Accuracy values per epoch\n",
    "\n",
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Use the `.evaluate()` method to compute the loss and other specified metrics for our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train, X_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will return the final loss associated with the model for the test data as well as any other specified metrics.\n",
    "\n",
    "## Additional Resource\n",
    "    \n",
    "* A full book on Keras by the author of Keras himself:  \n",
    "  [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "The key takeaways from this section include:\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "* Neural networks are powerful models that can be customized and tweaked using various amounts of nodes, layers, etc.\n",
    "* The most basic neural networks are single-layer densely connected neural networks, which have very similar properties as logistic regression models\n",
    "* Compared to more traditional statistics and ML techniques, neural networks perform particularly well when using unstructured data\n",
    "* Apart from densely connected networks, other types of neural networks include convolutional neural networks, recurrent neural networks, and generative adversarial neural networks \n",
    "* When working with image data, it's important to understand how image data is stored when working with them in Python\n",
    "* Logistic regression can be seen as a single-layer neural network with a sigmoid activation function\n",
    "* Neural networks use loss and cost functions to minimize the \"loss\", which is a function that summarizes the difference between the actual outcome (e.g., pictures contain Santa or not) and the model prediction (whether the model correctly identifies pictures with Santa)\n",
    "* Backward and forward propagation are used to estimate the so-called \"model weights\"\n",
    "* Adding more layers to neural networks can substantially increase model performance\n",
    "* Several activations can be used in model nodes, you can explore with different types and evaluate how it affects performance\n",
    "\n",
    "### Deep Neural Networks\n",
    "\n",
    "* Deep neural network representations can lighten the burden and automate certain tasks of heavy data preprocessing\n",
    "* Deep representations need exponentially fewer hidden units than shallow networks, to obtain the same performance\n",
    "* Parameter initialization, forward propagation, cost function evaluation, and backward propagation are again the cornerstones of deep networks\n",
    "* Tensors are the building blocks of neural networks and a good understanding of them and how to use them in Python is crucial\n",
    "* Scalars can be seen as 0-D tensors. Vectors can be seen as 1-D tensors, and matrices as 2-D tensors\n",
    "* The usage of tensors reaches beyond matrices: tensors can have N dimensions\n",
    "* Tensors can be created and manipulated using NumPy\n",
    "* Keras makes building neural networks in Python easy, and you learned how to do that in this section\n",
    "* You can use Keras to do some NLP as well, e.g. for tokenization \n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "In this lecture, we'll explore the architecture of neural networks. On Monday, we will start working with TensorFlow to create neural networks.\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "Neural networks mimic the structure of the human brain and are computational graphs connecting inputs to outputs through layers, including hidden layers. Each connection between layers has associated weights and biases, and uses activation functions to learn relationships.\n",
    "\n",
    "### Layers and Complexity\n",
    "\n",
    "- **Input Layer:** Represents features or columns of the dataset.\n",
    "- **Hidden Layers:** Intermediate layers that learn from the input features.\n",
    "- **Output Layer:** Provides the final prediction or classification.\n",
    "\n",
    "Each neuron in a hidden layer represents a unique equation and processes inputs to generate outputs for the next layer.\n",
    "\n",
    "### Example: Cat vs. Dog Classification\n",
    "\n",
    "Inputs (features) are processed through multiple layers to produce an output (e.g., identifying if an image is a cat or a dog).\n",
    "\n",
    "## Understanding Neural Network Training\n",
    "\n",
    "### Epoch and Batch Sizes\n",
    "\n",
    "- **Epoch:** One full pass through the entire dataset.\n",
    "- **Batch Size:** The number of samples processed before the model's internal parameters are updated.\n",
    "\n",
    "Each epoch consists of multiple batches, and after each batch, the model's weights are updated.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns.\n",
    "\n",
    "- **Sigmoid:** Often used in the output layer for binary classification.\n",
    "- **Tanh:** Scaled sigmoid function ranging from -1 to 1.\n",
    "- **ReLU (Rectified Linear Unit):** Outputs the input if it's positive; otherwise, it outputs zero. Most popular due to its simplicity and effectiveness.\n",
    "\n",
    "### Regularization and Learning Rate\n",
    "\n",
    "- **Regularization:** Techniques like L1 and L2 regularization prevent overfitting by adding penalties to the loss function.\n",
    "- **Learning Rate:** Determines the step size during gradient descent optimization.\n",
    "\n",
    "## Building a Neural Network\n",
    "\n",
    "### Weights and Biases\n",
    "\n",
    "- Weights are initialized randomly and adjusted through training.\n",
    "- Bias terms are added to the weighted sum of inputs to create the neuron's output.\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Inputs are passed through the network layer by layer. Each layer applies weights, sums the inputs, adds biases, and uses activation functions to produce outputs for the next layer.\n",
    "\n",
    "Single neuron network (regression):\n",
    "\n",
    "<center><img src = \"images/costfunction_singleexample.png\" width = 2000></center>\n",
    "\n",
    "Forward propagate data through single neuron:\n",
    "<center><img src = \"images/single-unit.png \" width = 900></center>\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "After forward propagation, the error is calculated using a loss function. Backpropagation adjusts the weights by calculating gradients and updating the weights in the direction that reduces the error.\n",
    "\n",
    "<img src = \"images/backprop.gif\" width = 600>\n",
    "\n",
    "## Example Code: Forward Propagation and Activation Functions\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.random.rand(64)\n",
    "bias = np.random.rand(1)\n",
    "\n",
    "# Input features\n",
    "inputs = np.array([0.5, 0.3, ..., 0.9])  # Example values\n",
    "\n",
    "# Weighted sum\n",
    "z = np.dot(weights, inputs) + bias\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Output after activation\n",
    "output = sigmoid(z)\n",
    "print(\"Output:\", output)\n",
    "```\n",
    "\n",
    "## Practical Example: Image Classification\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Load Image Data:** Flatten images into vectors.\n",
    "2. **Initialize Weights and Biases:** Randomly initialize for each neuron.\n",
    "3. **Forward Propagation:** Calculate weighted sums and apply activation functions.\n",
    "4. **Calculate Error:** Compare predicted outputs with actual labels.\n",
    "5. **Backpropagation:** Adjust weights based on error gradients.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Neural networks are powerful tools for learning complex patterns and making predictions. They involve intricate processes of forward and backward propagation, weight adjustment, and the use of activation functions. While their workings can seem like a black box, understanding their architecture and training process is crucial for effectively using them in machine learning tasks.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next class, we will cover vectorization and classification. Ensure you are familiar with the concepts discussed today. If you have any questions, please feel free to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Partial Derivatives\n",
    "\n",
    "Here’s a summary of each partial derivative:\n",
    "\n",
    "1. $$\\frac{\\partial C}{\\partial w_1} = -2 w_5 X_1 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "2. $$\\frac{\\partial C}{\\partial w_2} = -2 w_6 X_1 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "3. $$\\frac{\\partial C}{\\partial w_3} = -2 w_5 X_2 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "4. $$\\frac{\\partial C}{\\partial w_4} = -2 w_6 X_2 \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "5. $$\\frac{\\partial C}{\\partial w_5} = -2 (w_1 X_1 + w_3 X_2) \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "6. $$\\frac{\\partial C}{\\partial w_6} = -2 (w_2 X_1 + w_4 X_2) \\sum \\left( y - \\left[ X_1 (w_1 w_5 + w_2 w_6) + X_2 (w_3 w_5 + w_4 w_6) \\right] \\right)$$\n",
    "\n",
    "In these equations:\n",
    "- $y$ represents the actual target value.\n",
    "- $X_1$ and $X_2$ are input features.\n",
    "- $w_i$ are the weights of the neural network.\n",
    "- The summation $\\sum$ indicates summing over all training examples.\n",
    "\n",
    "Each equation represents how the cost function $C$ changes with respect to a particular weight $w_i$. These derivatives are used in the backpropagation algorithm to adjust the weights iteratively, reducing the overall cost and improving the model's predictions.\n",
    "\n",
    "We want to go the opposite direction of these partial derivatives in order to go down the gradient slope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Notation \n",
    "\n",
    "<img src='https://curriculum-content.s3.amazonaws.com/data-science/images/new_small_deeper.png' width='700'>\n",
    "\n",
    "For our 2-layer neural network above:\n",
    "\n",
    "- $x = a^{[0]}$  as x is what comes out of the input layer\n",
    "- $a^{[1]} = \\begin{bmatrix} a^{[1]}_1 \\\\ a^{[1]}_2 \\\\ a^{[1]}_3 \\end{bmatrix}$ is the value generated by the hidden layer\n",
    "- $\\hat y = a^{[2]}$, the output layer will generate a value $a^{[2]}$, which is equal to $\\hat y$ \n",
    "\n",
    "Note that the input layer is not counted or is index 0, so if you say an n-layer network be sure to not count the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Activation Functions\n",
    "You won't use any of these, you'll use the ones in libraries, but it's a nice show of how they work.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    if (derivative == True):\n",
    "        return f * (1 - f)\n",
    "    return f\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    f = np.tanh(x)\n",
    "    if (derivative == True):\n",
    "        return (1 - (f ** 2))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = 0\n",
    "    return f\n",
    "\n",
    "def leaky_relu(x, leakage=0.05, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = leakage\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = x[i] * leakage\n",
    "    return f\n",
    "\n",
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return 1/(1+np.square(x))\n",
    "    return np.arctan(x)\n",
    "\n",
    "z = np.arange(-10, 10, 0.2)\n",
    "```\n",
    "\n",
    "## The hyperbolic tangent (tanh) function \n",
    "\n",
    "The hyperbolic tangent (or tanh) function goes between -1 and +1, and is in fact a shifted version of the sigmoid function, with formula $ a=\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$. For intermediate layers, the tanh function generally performs pretty well because, with values between -1 and +1, the means of the activations coming out are closer to zero! \n",
    "\n",
    "Tanh is a shifted sigmoid that goes between -1 and 1, recall sigmoid goes between 0 and 1.\n",
    "\n",
    "A disadvantage of both tanh and sigmoid activation functions is that when $z$ gets quite large or small, the derivative of the slopes of these functions become very small, generally 0.0001. This will slow down gradient descent. You can see in the tanh plot that this already starts happening for values of $z > 2$ or $z < 2$. The next few activation functions will try to overcome this issue. \n",
    "\n",
    "<img src=\"images/tanh.png\">\n",
    "\n",
    "## arctan (inverse tangent)\n",
    "\n",
    "The inverse tangent (arctan) function has a lot of the same qualities that tanh has, but the range roughly goes from -1.6 to 1.6, and the slope is more gentle than the one we saw using the tanh function.\n",
    "\n",
    "<img src=\"images/arctan.png\">\n",
    "\n",
    "## The Rectified Linear Unit function (ReLu)\n",
    "\n",
    "This is probably the most popular activation function, along with the tanh! The fact that the activation is exactly 0 when $z <0$  is slightly cumbersome when taking derivatives though. \n",
    "\n",
    "$$a=\\max(0, z)$$\n",
    "\n",
    "<img src=\"images/relu.png\">\n",
    "\n",
    "## The leaky Rectified Linear Unit function \n",
    "\n",
    "The leaky ReLU solves the derivative issue by allowing for the activation to be slightly negative when $z <0$. \n",
    "\n",
    "$$a=\\max(0.001*z ,z)$$\n",
    "\n",
    "<img src=\"images/leaky_relu.png\">\n",
    "\n",
    "## Softmax\n",
    "\n",
    "<img src = \"images/softmax.png\" width = 600>\n",
    "\n",
    "$$ \\large \\sigma(\\textbf{z})_i = \\frac{e^{z_i}}{\\sum_{j}e^{z_j}} $$\n",
    "Appropriate activation in the output layer for **multi-class** classification problems. \n",
    "\n",
    "- Outputting the probabilities of belonging to each class.\n",
    "There are other activation functions; [see here](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a). \n",
    "Our nodes will be taking in input from multiple sources. Let's add the entire training set as our input. \n",
    "\n",
    "## Additional Resources \n",
    "\n",
    "- [Visualising activation functions in neural networks](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Normalization\n",
    "\n",
    "## Normalized Inputs: Speed Up Training\n",
    "\n",
    "Normalizing inputs (scaling features to a consistent range, e.g., 0 to 1) speeds up training and promotes convergence. Standardization involves subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "<img src = \"images/normalize_nn.png\" >\n",
    "\n",
    "### Vanishing or Exploding Gradients\n",
    "\n",
    "Normalization helps mitigate numerical problems in gradient computation, preventing vanishing or exploding gradients. For deep networks, gradients can grow or shrink excessively, making training unstable.\n",
    "\n",
    "Example:\n",
    "- For a deep network with linear activations, the weight products can explode due to many layers. Normalization helps manage this risk.\n",
    "\n",
    "#### Solutions to Gradient Issues\n",
    "\n",
    "- **Initialization**: Small weights can prevent gradient issues. Common practices:\n",
    "  - **Variance Rule**: $ \\text{Var}(w_i) = \\frac{1}{n} $ or $ \\text{Var}(w_i) = \\frac{2}{n} $\n",
    "  - **ReLU Initialization**: $ w^{[l]} = \\text{np.random.randn(shape)} \\times \\sqrt{2/n_{l-1}} $\n",
    "\n",
    "## Optimization Strategies\n",
    "\n",
    "### Gradient Descent with Momentum\n",
    "\n",
    "- **Purpose**: Reduces oscillations and improves convergence.\n",
    "- **How**:\n",
    "  - Calculate moving averages for gradients.\n",
    "  - Update weights with averaged gradients.\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "- **Purpose**: Adapts learning rates for each parameter.\n",
    "- **How**:\n",
    "  - Use exponentially weighted average of squared gradients.\n",
    "  - Adjust learning rate based on average squared gradients.\n",
    "\n",
    "### Adam Optimization\n",
    "\n",
    "- **Purpose**: Combines momentum and RMSprop benefits.\n",
    "- **How**:\n",
    "  - Maintain moving averages for gradients and their squares.\n",
    "  - Apply bias corrections.\n",
    "  - Update weights using corrected averages.\n",
    "\n",
    "### Learning Rate Decay\n",
    "\n",
    "- **Purpose**: Gradually reduce the learning rate over epochs.\n",
    "- **Methods**:\n",
    "  - **Inverse Time Decay**: $ \\alpha = \\frac{1}{1 + \\text{decay rate} \\times \\text{epoch}} \\times \\alpha_0 $\n",
    "  - **Exponential Decay**: $ \\alpha = 0.97^{\\text{epoch}} \\times \\alpha_0 $\n",
    "  - **Manual Decay**\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "- **Important**: Learning rate ($ \\alpha $).\n",
    "- **Next**: Momentum ($ \\beta $), number of hidden units, mini-batch size.\n",
    "- **Less Critical**: Number of layers, learning rate decay.\n",
    "- **Rarely Tuned**: $ \\beta_1 $, $ \\beta_2 $, $ \\epsilon $ (for Adam).\n",
    "\n",
    "Avoid grid search; use iterative testing for hyperparameter tuning.\n",
    "\n",
    "## Additional Resources\n",
    "- [Coursera: Normalizing Inputs](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs)\n",
    "- [Coursera: Gradient Descent with Momentum](https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization\n",
    "\n",
    "## Key Hyperparameters\n",
    "\n",
    "When tuning neural networks, focus on:\n",
    "\n",
    "- **Number of Hidden Units**: Controls the model's capacity.\n",
    "- **Number of Layers**: Affects the model's depth.\n",
    "- **Learning Rate ($\\alpha$)**: Determines step size in optimization.\n",
    "- **Activation Function**: Transforms node inputs.\n",
    "\n",
    "Use a **validation set** to balance accuracy and generalization.\n",
    "\n",
    "## Data Splits\n",
    "\n",
    "Divide your data into:\n",
    "\n",
    "- **Training Set**: For training the model.\n",
    "- **Validation Set**: To tune hyperparameters and select the final model.\n",
    "- **Test Set**: To evaluate performance on unseen data.\n",
    "\n",
    "Ensure all sets come from the same distribution (e.g., same image resolution).\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "Balance **bias** (error from too-simple models) and **variance** (error from too-complex models).\n",
    "\n",
    "### The Circles Example\n",
    "\n",
    "Examine the bias-variance trade-off with concentric circles:\n",
    "\n",
    "- **High Bias (Underfitting)**: Model is too simple, missing key patterns.\n",
    "  ![Underfitting](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/underfitting.png)\n",
    "- **Good Fit**: Model accurately captures underlying patterns.\n",
    "  ![Good Fit](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/good.png)\n",
    "- **High Variance (Overfitting)**: Model is too complex, capturing noise.\n",
    "  ![Overfitting](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/overfitting.png)\n",
    "\n",
    "## The Santa Example\n",
    "\n",
    "Performance across different models:\n",
    "\n",
    "|       | High Variance | High Bias | High Variance & Bias | Low Variance and Bias |\n",
    "|-------|---------------|-----------|----------------------|-----------------------|\n",
    "| Train Set Error | 12% | 26% | 26% | 12% |\n",
    "| Validation Set Error | 25% | 28% | 40% | 13% |\n",
    "\n",
    "A model with low variance and bias performs best (87% accuracy).\n",
    "\n",
    "## Bias / Variance Tips\n",
    "\n",
    "| High Bias? (Training Error) | High Variance? (Validation Error) |\n",
    "|------------------------------|-----------------------------------|\n",
    "| Increase network size | Gather more data |\n",
    "| Train longer | Apply regularization |\n",
    "| Try different architectures | Try different architectures |\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Prevents overfitting by penalizing large weights.\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "\n",
    "#### In Logistic Regression\n",
    "\n",
    "For L2 regularization:\n",
    "\n",
    "$$ J(w, b) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m} ||w||_2^2 $$\n",
    "\n",
    "- **L2 Regularization**: Penalizes large weights to simplify the model.\n",
    "- **L1 Regularization**: Adds a term $\\dfrac{\\lambda}{m} ||w||_1$ for sparsity.\n",
    "\n",
    "#### In Neural Networks\n",
    "\n",
    "For L2 regularization across all layers:\n",
    "\n",
    "$$ J(w^{[1]}, b^{[1]}, \\ldots, w^{[L]}, b^{[L]}) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\dfrac{\\lambda}{2m} \\sum_{l=1}^L ||w^{[l]}||_2^2 $$\n",
    "\n",
    "**Update Rule**:\n",
    "\n",
    "$$ w^{[l]} := w^{[l]} - \\alpha \\left( \\text{[backpropagation]} + \\dfrac{\\lambda}{m} w^{[l]} \\right) $$\n",
    "\n",
    "### Dropout Regularization\n",
    "\n",
    "Dropout randomly ignores nodes during training, reducing overfitting.\n",
    "\n",
    "**Before Dropout**:\n",
    "\n",
    "![Standard Neural Net](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/dropout.png)\n",
    "\n",
    "**After Dropout**:\n",
    "\n",
    "![Neural Net with Dropout](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/dropout.png)\n",
    "\n",
    "In Keras, use the `Dropout` layer:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', input_shape=(500,)))\n",
    "model.add(Dropout(0.3))  # Dropout applied here\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout applied here\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "## Steps for Using Keras\n",
    "\n",
    "1. Load Data\n",
    "2. Define Keras Model\n",
    "3. Compile Keras Model\n",
    "4. Fit Keras Model\n",
    "5. Evaluate Keras Model\n",
    "6. Make Predictions\n",
    "\n",
    "## Regularization Details\n",
    "\n",
    "### L2 Regularization: \n",
    "\n",
    "Add penalty on weight matrices using Frobenius norm.\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{m} \\frac{1}{m} L(y_i, \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "J' = \\sum_{i=1}^{m} \\frac{1}{m} L(y_i, \\hat{y}_i) + \\frac{\\lambda}{2} \\sum_{l=1}^{L} \\|W[l]\\|_F^2\n",
    "$$\n",
    "\n",
    "where \\(F\\) denotes the Frobenius norm.\n",
    "\n",
    "### Frobenius norm is the square root of the sum of element-wise squares:\n",
    "\n",
    "$$\n",
    "\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n",
    "$$\n",
    "\n",
    "### Modified objective function:\n",
    "\n",
    "$$\n",
    "J' = \\sum_{i=1}^{m} \\frac{1}{m} L(y_i, \\hat{y}_i) + \\frac{\\lambda}{2} \\sum_{l=1}^{L} \\|W[l]\\|_F^2\n",
    "$$\n",
    "\n",
    "### Regularization serves to penalize any particular weight from being too large:\n",
    "\n",
    "- Smoothens/redistributes weights for each weight matrix\n",
    "- Aids in generalization\n",
    "\n",
    "### L2 regularization: often known as weight decay\n",
    "\n",
    "**Perspective: gradient descent**\n",
    "\n",
    "$$\n",
    "J' = \\sum_{i=1}^{m} \\frac{1}{m} L(y_i, \\hat{y}_i) + \\frac{\\lambda}{2} \\sum_{l=1}^{L} \\|W[l]\\|_F^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "W[l] \\rightarrow W[l] - \\alpha \\frac{\\partial J'}{\\partial W[l]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{m} \\frac{1}{m} L(y_i, \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "W[l] \\rightarrow (W[l] - \\alpha \\frac{\\lambda}{2m} W[l]) - \\alpha \\frac{\\partial J}{\\partial W[l]}\n",
    "$$\n",
    "\n",
    "At each iteration, regularization tries to reduce the magnitude of the weight matrix.\n",
    "\n",
    "### Implementing L2 Regularization in Keras\n",
    "\n",
    "Creates keras regularizer object:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "L2 regularizer object takes in $\\lambda$ parameter as argument:\n",
    "\n",
    "reg = l2(3e-3)  # 1e-5 to .1\n",
    "```\n",
    "Implementing regularization to weights in Keras:\n",
    "* Dense(kernel_regularizer = __)\n",
    "* Takes in string ('l2') or regularizer object\n",
    "\n",
    "Less common but also used:\n",
    "* Can apply regularization to bias terms\n",
    "* Dense(bias_regularizer = __)\n",
    "\n",
    "```python\n",
    "# load in the data\n",
    "from tensorflow.keras import datasets, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train/255  # normalization\n",
    "x_test = x_test/255\n",
    "\n",
    "# Keras requires multi-class labels to be one-hot encoded\n",
    "y_tr_one_hot = to_categorical(y_train)\n",
    "y_tt_one_hot = to_categorical(y_test)\n",
    "\n",
    "model_images = Sequential()\n",
    "model_images.add(Flatten(input_shape=(28, 28)))  # flattens each 28x28 image into a vector\n",
    "model_images.add(Dense(32, activation='relu', kernel_regularizer=reg))  # take in regularizer\n",
    "model_images.add(Dense(8, activation='relu', kernel_regularizer=reg))\n",
    "model_images.add(Dense\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interpretability - Introduction\n",
    "\n",
    "## What is Interpretability?\n",
    "\n",
    "Interpretability in machine learning refers to how well we can understand the decisions made by our models. There is no single definition or consensus, but it's crucial to align interpretability with the goals of your machine learning project.\n",
    "\n",
    "\n",
    "## Interpretability and Machine Learning Goals\n",
    "\n",
    "### Machine Learning Goals\n",
    "\n",
    "- **Support Human Decisions**: Provides insights to aid human decision-making. For example, Clinical Decision Support (CDS) systems like [Watson Health's Micromedex](https://www.ibm.com/watson-health/solutions/clinical-decision-support) help clinicians by offering advice based on patient data and medical knowledge.\n",
    "\n",
    "    [Watson Health Micromedex](https://www.ibm.com/watson-health/solutions/clinical-decision-support)\n",
    "\n",
    "- **Automate Human Decisions**: Trains models to make decisions independently, such as Natural Language Generation applications used in predictive text and chatbots like ChatGPT.\n",
    "\n",
    "### The Cost and Benefits of Decision Support\n",
    "\n",
    "#### Clinical Decision Support (CDS)\n",
    "\n",
    "CDS systems assist clinicians by providing actionable insights from medical data, which can speed up patient care but also introduce potential for false positives. \n",
    "\n",
    "![Clinical Decision Support](https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/clinical-decision-support.png)\n",
    "\n",
    "> **Goal**: Provide reliable recommendations to clinicians. Effectiveness depends on the relevance and accuracy of the information provided.\n",
    "\n",
    "#### Computer Aided Detection (CAD)\n",
    "\n",
    "CAD systems enhance diagnostic imaging, such as detecting breast cancer. They offer a \"second opinion\" but may introduce false positives, impacting patient care and resource allocation.\n",
    "\n",
    "> **Goal**: Improve early detection of conditions like cancer. Balancing sensitivity with false positives is crucial.\n",
    "\n",
    "### How Interpretability Enhances Decision Support\n",
    "\n",
    "Interpretability can improve decision support in several ways:\n",
    "\n",
    "1. **Trust**: Metrics like accuracy and ROC/AUC show how well the model performs and the nature of its errors.\n",
    "\n",
    "2. **Causality**: Helps in understanding relationships between variables, guiding hypotheses and interventions.\n",
    "\n",
    "3. **Transferability**: Assesses whether a model’s performance is consistent in new scenarios. Understanding decision drivers aids in model tuning and debugging.\n",
    "\n",
    "4. **Informativeness**: Provides insights into feature importance, enhancing domain knowledge.\n",
    "\n",
    "5. **Fair and Ethical Decision Making**: Ensures algorithms do not perpetuate societal biases, fostering accountability.\n",
    "\n",
    "## White Box vs. Black Box Models\n",
    "\n",
    "Historically, simpler models like regression and decision trees were more interpretable due to their transparency. However, modern complex models, especially neural networks, are often seen as \"black boxes\" because their decision-making processes are less transparent.\n",
    "\n",
    "- **White Box Models**: Transparent and interpretable, such as regression and decision trees.\n",
    "- **Black Box Models**: Complex and less interpretable, such as neural networks. \n",
    "\n",
    "Despite their complexity, black box models can be analyzed using various interpretability techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interpretability - White-Box Models\n",
    "\n",
    "## Definitions\n",
    "- **White-box Models**: Models that are interpretable with minimal investigation, e.g., regression, decision trees.\n",
    "- **Black-box Models**: Models that are not easily interpretable, e.g., neural networks.\n",
    "- **Intrinsic Interpretability**: Understanding how a model arrived at a prediction directly from the model's structure.\n",
    "- **Post-Hoc Interpretation**: Analyzing a model's predictions after training using additional methods.\n",
    "\n",
    "## Model Selection and Common White-Box Models\n",
    "\n",
    "### Linear Regression\n",
    "Predicts a value based on well-understood variables, e.g., home price based on square footage.\n",
    "\n",
    "### Logistic Regression\n",
    "Classifies data into categories, e.g., determining if a home is a McMansion based on description.\n",
    "\n",
    "### Naive Bayes\n",
    "Analyzes unstructured data for classification, e.g., identifying common features of McMansions from descriptions.\n",
    "\n",
    "### Decision Trees\n",
    "Classifies based on important features, e.g., deciding whether to buy a McMansion based on listed characteristics.\n",
    "\n",
    "## Types of Interpretation\n",
    "\n",
    "### Intrinsic\n",
    "- **Definition**: Directly interpretable due to model simplicity.\n",
    "- **Examples**: Linear regression, logistic regression, simple decision trees.\n",
    "\n",
    "### Post-Hoc\n",
    "- **Definition**: Interpretation methods applied after model training.\n",
    "- **Examples**: Permutation feature importance, visualizations, reading model internals.\n",
    "\n",
    "## Methods of Interpretation\n",
    "\n",
    "### Model-Specific\n",
    "- **Definition**: Methods unique to specific models, e.g., regression weights for regression models.\n",
    "\n",
    "### Model-Agnostic\n",
    "- **Definition**: Methods applicable to any model based on input/output analysis.\n",
    "\n",
    "### Scope of Interpretation\n",
    "- **Local**: Explains individual predictions.\n",
    "- **Global**: Explains overall model behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interpretability - Black Box Models\n",
    "\n",
    "## Common Black-Box Models\n",
    "\n",
    "### Gradient Boosted Trees (GBDT)\n",
    "- **Definition**: An ensemble method combining multiple decision trees for improved accuracy.\n",
    "- **How It Works**: Boosting enhances weak models incrementally, using gradient descent to minimize errors.\n",
    "- **Applications**:\n",
    "  - **Fraud Detection**: Identifies fraudulent transactions.\n",
    "  - **Medical Outcomes**: Predicts disease likelihood or treatment effectiveness.\n",
    "  - **Recommender Systems**: Suggests products or content based on user preferences.\n",
    "  - **Computer Vision**: Recognizes objects in images and videos.\n",
    "  - **Customer Churn**: Predicts which customers may leave.\n",
    "\n",
    "### Neural Networks\n",
    "- **Definition**: Mimics biological neurons to process data through layers (input, hidden, output).\n",
    "- **How They Work**: Nodes activate based on weights and thresholds, passing data through layers.\n",
    "- **Applications**:\n",
    "  - **Medical Imaging**: Analyzes X-rays, CT scans, and MRIs.\n",
    "  - **Drug Research**: Predicts drug effectiveness and side effects.\n",
    "  - **Patient Outcomes**: Predicts disease progression and survival rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases for Neural Networks\n",
    "\n",
    "Neural networks have significant applications in the medical field, including:\n",
    "\n",
    "- **Medical Imaging**: Analyzing X-rays, CT scans, and MRIs to identify features or abnormalities, aiding in the diagnosis of diseases such as cancer, heart disease, and neurological disorders.\n",
    "\n",
    "- **Drug Research and Development**: Analyzing data from chemical compounds to predict the effectiveness and side effects of new drugs, helping pharmaceutical companies identify promising drug candidates faster.\n",
    "\n",
    "- **Patient Outcomes**: Predicting risks such as readmission, disease progression, and survival rates using a range of patient data (genetic, demographic, clinical), assisting doctors in making informed decisions about patient care.\n",
    "\n",
    "Neural networks improve medical diagnosis and treatment accuracy by efficiently analyzing and learning from large data sets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
